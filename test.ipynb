{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文档向量化测试\n",
    "# count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"Many existing knowledge bases(KBs), including Freebase, Yago, and NELL, rely on a ﬁxed ontology, given as an input to the system, which deﬁnes the data to be cataloged in the KB, i.e., a hierarchy of categories and relations between them. The system then extracts facts that match the predeﬁned ontology. We propose an unsupervised model that jointly learns a latent ontological structure of an input corpus, and identiﬁes facts from the corpus that match the learned structure. Our approach combines mixed membership stochastic block models and topic models to infer a structure by jointly modeling text, a latent concept hierarchy, and latent semantic relationships among the entities mentioned in the text. As a case study, we apply the model to a corpus of Web documents from the software domain,and evaluate the accuracy of the various components of the learned ontology. \"\n",
    "b = [\"hello world. what the fuck.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [\" \".join(preprocess(a))]\n",
    "vector = CountVectorizer(ngram_range=(1, 2), vocabulary=[\"knowledge base\"], stop_words='english')\n",
    "vector.build_analyzer()\n",
    "x = vector.fit_transform(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = CountVectorizer(ngram_range=(1, 2), vocabulary=['fuck', 'hello', 'world', \"hello world\"], stop_words='english')\n",
    "estimator.build_analyzer()\n",
    "res = estimator.fit_transform(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.toarray()\n",
    "# estimator.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 斯坦福nlp工具测试\n",
    "# stanfordcorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenize: ['Guangdong', 'University', 'of', 'Foreign', 'Studies', 'is', 'located', 'in', 'Guangzhou', '.']\n",
      "Part of Speech: [('Guangdong', 'NNP'), ('University', 'NNP'), ('of', 'IN'), ('Foreign', 'NNP'), ('Studies', 'NNPS'), ('is', 'VBZ'), ('located', 'JJ'), ('in', 'IN'), ('Guangzhou', 'NNP'), ('.', '.')]\n",
      "Named Entities: [('Guangdong', 'ORGANIZATION'), ('University', 'ORGANIZATION'), ('of', 'ORGANIZATION'), ('Foreign', 'ORGANIZATION'), ('Studies', 'ORGANIZATION'), ('is', 'O'), ('located', 'O'), ('in', 'O'), ('Guangzhou', 'CITY'), ('.', 'O')]\n",
      "Constituency Parsing: (ROOT\n",
      "  (S\n",
      "    (NP\n",
      "      (NP (NNP Guangdong) (NNP University))\n",
      "      (PP (IN of)\n",
      "        (NP (NNP Foreign) (NNPS Studies))))\n",
      "    (VP (VBZ is)\n",
      "      (ADJP (JJ located)\n",
      "        (PP (IN in)\n",
      "          (NP (NNP Guangzhou)))))\n",
      "    (. .)))\n",
      "Dependency Parsing: [('ROOT', 0, 7), ('compound', 2, 1), ('nsubjpass', 7, 2), ('case', 5, 3), ('compound', 5, 4), ('nmod', 2, 5), ('auxpass', 7, 6), ('case', 9, 8), ('nmod', 7, 9), ('punct', 7, 10)]\n"
     ]
    }
   ],
   "source": [
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "nlp = StanfordCoreNLP('http://localhost', port=9000)\n",
    "sentence = 'Guangdong University of Foreign Studies is located in Guangzhou.'\n",
    "print('Tokenize:', nlp.word_tokenize(sentence))\n",
    "print('Part of Speech:', nlp.pos_tag(sentence))\n",
    "print('Named Entities:', nlp.ner(sentence))\n",
    "print('Constituency Parsing:', nlp.parse(sentence))\n",
    "print('Dependency Parsing:', nlp.dependency_parse(sentence))\n",
    "\n",
    "#nlp.close() # Do not forget to close! The backend server will consume a lot memery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新闻数据集测试\n",
    "# 20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newsgroups\n",
    "size = 500\n",
    "tmp = newsgroups.get_news_data(size)\n",
    "newsdata = []\n",
    "for cate in tmp:\n",
    "    newsdata.extend(cate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess as pp\n",
    "import configs\n",
    "import persister\n",
    "texts = [' '.join(pp.preprocess_abstract(a)) for a in tmp]\n",
    "persister.save_json(configs.NEWSDATA, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import persister\n",
    "import configs\n",
    "texts = persister.load_json(configs.NEWSDATA)\n",
    "newssent = persister.load_json(configs.NEWSSENT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'regard fractal commpression seen fractal compressed movie fairly impressive first one wa gray scale movie casablanca wa mb minute fps video wa little grainy bad second one saw wa minute bit color fps measured mb consider fractal movie practical thing explore unlike many format end losing resolution know kind software hardware wa used creating movie saw guy showed said took minute per frame generate said playback wa frame per second else could put minute one floppy disk'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 英文分句\n",
    "# sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import sent_tokenize\n",
    "newssent = []\n",
    "for news in newsdata:\n",
    "    newssent.append(pp.split2sent(news))\n",
    "persister.save_json(configs.NEWSSENT, newssent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Price is about $28, call to or visit in offices   in Menlo Park, in Reston, Virginia (800-USA-MAPS).'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newssent[160][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('ROOT', 0, 1), ('punct', 1, 2), ('nmod:npmod', 4, 3), ('dep', 1, 4), ('punct', 4, 5), ('nummod', 10, 6), ('compound', 10, 7), ('compound', 10, 8), ('compound', 10, 9), ('dep', 4, 10), ('punct', 10, 11), ('compound', 13, 12), ('dep', 10, 13), ('punct', 15, 14), ('dep', 13, 15), ('nummod', 15, 16), ('punct', 15, 17), ('nummod', 15, 18), ('amod', 20, 19), ('dep', 15, 20), ('punct', 15, 21), ('nummod', 23, 22), ('dep', 15, 23), ('compound', 25, 24), ('nummod', 23, 25), ('punct', 15, 26)]\n"
     ]
    }
   ],
   "source": [
    "print(nlp.dependency_parse(newssent[160][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(ROOT\\r\\n  (S\\r\\n    (SBAR (IN In)\\r\\n      (S\\r\\n        (VP (VBZ regards)\\r\\n          (PP (TO to)\\r\\n            (NP (JJ fractal) (NN commpression))))))\\r\\n    (, ,)\\r\\n    (NP (PRP I))\\r\\n    (VP (VBP have)\\r\\n      (VP (VBN seen)\\r\\n        (S\\r\\n          (NP (CD 2))\\r\\n          (ADJP (JJ fractal))\\r\\n          (S\\r\\n            (VP (VBN compressed)\\r\\n              (S (`` ``)\\r\\n                (NP (NNS movies))\\r\\n                ('' '')))))))\\r\\n    (. .)))\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.parse(newssent[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos = nlp.pos_tag(newssent[160][4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fuck m e'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = \"fuck\\nm  e\"\n",
    "re.sub(\"\\s+\", \" \", s)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
