{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import preprocess as pp\n",
    "import configs\n",
    "import persister\n",
    "import relation\n",
    "import lda\n",
    "\n",
    "import re\n",
    "import json\n",
    "\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tree import Tree\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文档向量化测试\n",
    "# count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"Many existing knowledge bases(KBs), including Freebase, Yago, and NELL, rely on a ﬁxed ontology, given as an input to the system, which deﬁnes the data to be cataloged in the KB, i.e., a hierarchy of categories and relations between them. The system then extracts facts that match the predeﬁned ontology. We propose an unsupervised model that jointly learns a latent ontological structure of an input corpus, and identiﬁes facts from the corpus that match the learned structure. Our approach combines mixed membership stochastic block models and topic models to infer a structure by jointly modeling text, a latent concept hierarchy, and latent semantic relationships among the entities mentioned in the text. As a case study, we apply the model to a corpus of Web documents from the software domain,and evaluate the accuracy of the various components of the learned ontology. \"\n",
    "b = [\"hello world. what the fuck.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [\" \".join(preprocess(a))]\n",
    "vector = CountVectorizer(ngram_range=(1, 2), vocabulary=[\"knowledge base\"], stop_words='english')\n",
    "vector.build_analyzer()\n",
    "x = vector.fit_transform(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = CountVectorizer(ngram_range=(1, 2), vocabulary=['fuck', 'hello', 'world', \"hello world\"], stop_words='english')\n",
    "estimator.build_analyzer()\n",
    "res = estimator.fit_transform(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.toarray()\n",
    "# estimator.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 斯坦福nlp工具测试\n",
    "# stanfordcorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP('/mnt/d/stanford-corenlp-full-2018-02-27', port=9000)\n",
    "sentence = 'Brink is taking part in the final exercise.'\n",
    "print('Tokenize:', nlp.word_tokenize(sentence))\n",
    "print('Part of Speech:', nlp.pos_tag(sentence))\n",
    "print('Named Entities:', nlp.ner(sentence))\n",
    "print('Constituency Parsing:', nlp.parse(sentence))\n",
    "print('Dependency Parsing:', nlp.dependency_parse(sentence))\n",
    "\n",
    "nlp.close() # Do not forget to close! The backend server will consume a lot memery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 官方corenlp api\n",
    "# stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stanza.server import CoreNLPClient\n",
    "with CoreNLPClient(properties=\"./corenlp_server.props\", timeout=30000, memory='5G') as client:\n",
    "    res = relation.corenlp_annotate(client, pp.format_news(newsdata[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新闻数据集测试\n",
    "# 20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newsgroups\n",
    "size = 500\n",
    "tmp = newsgroups.get_news_data(size)\n",
    "newsdata = []\n",
    "for cate in tmp:\n",
    "    newsdata.extend(cate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [' '.join(pp.preprocess_abstract(a))\n",
    "persister.save_json(configs.NEWSDATA, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理成句子\n",
    "# nltk.sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newssent = []\n",
    "for news in newsdata:\n",
    "    newssent.append(pp.split2sent(news))\n",
    "persister.save_json(configs.NEWSSENT, newssent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newssenttoken = []\n",
    "for news in newssent:\n",
    "    tokenized_lemmatized_news = []\n",
    "    for sent in news:\n",
    "        tokenized_lemmatized_news.append(relation.lemmatize_sent_words(sent))\n",
    "    newssenttoken.append(tokenized_lemmatized_news)\n",
    "persister.save_json(configs.NEWSSENTTOKEN, newssenttoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprossed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawnews = persister.load_json(configs.RAWNEWS)\n",
    "newsparse = persister.read_parse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lda res\n",
    "news_input = persister.read_input(configs.NEWSINPUT)\n",
    "terms, doc_topic, topic_word = persister.read_lda(configs.NEWSLDA)\n",
    "top_terms, top_docs = lda.get_topics(topic_word, terms, doc_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gensim compute coherence\n",
    "texts = [word_tokenize(corp) for corp in news_input]\n",
    "dictionary = Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "m = CoherenceModel(topics=[[t[0] for t in topic] for topic in top_terms], \n",
    "                   corpus=corpus, dictionary=dictionary, coherence='u_mass')\n",
    "m.get_coherence_per_topic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use tmtool select model by coherence\n",
    "tf, vector_model = lda.extract_feature(news_input)\n",
    "m, c_lst = lda.select_model_by_coherence(tf, np.array(vector_model.get_feature_names()), news_input, \"c_v\", range(2,8,2), max_iter=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# column names\n",
    "topicnames = [\"Topic\" + str(i) for i in range(len(topic_word))]\n",
    "\n",
    "# index names\n",
    "docnames = [\"Doc\" + str(i) for i in range(len(doc_topic))]\n",
    "\n",
    "# Make the pandas dataframe\n",
    "df_document_topic = pd.DataFrame(np.round(doc_topic, 2), columns=topicnames, index=docnames)\n",
    "\n",
    "# Get dominant topic for each document\n",
    "dominant_topic = np.argmax(df_document_topic.values, axis=1)\n",
    "df_document_topic['dominant_topic'] = dominant_topic\n",
    "\n",
    "# Styling\n",
    "def color_green(val):\n",
    "    color = 'green' if val > .1 else 'black'\n",
    "    return 'color: {col}'.format(col=color)\n",
    "\n",
    "def make_bold(val):\n",
    "    weight = 700 if val > .1 else 400\n",
    "    return 'font-weight: {weight}'.format(weight=weight)\n",
    "\n",
    "# Apply Style\n",
    "df_document_topics = df_document_topic[10:20].style.applymap(color_green).applymap(make_bold)\n",
    "df_document_topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(lda.get_dominant_topic(doc_topic, topic_word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_terms_vis, top_docs_vis = lda.pd_topics_vis(top_terms,top_docs)\n",
    "top_terms_vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = lda.generate_lda_parameter(2,8,2,[20])\n",
    "tf, _ = lda.extract_feature(news_input)\n",
    "model = lda.gridsearchCV(param, tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.best_estimator_.perplexity(tf))\n",
    "pd.DataFrame(model.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = list(range(2,8,2))\n",
    "score = 'mean_test_score'\n",
    "log_likelyhoods_5 = [round(s) for s in model.cv_results_[score][:len(n_topics)]] \n",
    "log_likelyhoods_7 = [round(s) for s in model.cv_results_[score][len(n_topics):2*len(n_topics)]]\n",
    "log_likelyhoods_9 = [round(s) for s in model.cv_results_[score][2*len(n_topics):3*len(n_topics)]]\n",
    "\n",
    "# Show graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "print(log_likelyhoods_5)\n",
    "print(log_likelyhoods_7)\n",
    "plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
    "plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
    "plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
    "plt.title(\"Choosing Optimal LDA Model\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Log Likelyhood Scores\")\n",
    "plt.legend(title='Learning decay', loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf, _ = lda.extract_feature(news_input)\n",
    "m,perps = lda.select_model_by_perplexity(tf, range(2,8,1), learning_decay=0.8, max_iter=30)\n",
    "perps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "failed_idxs = []\n",
    "for idx, i in enumerate(newsparse):\n",
    "    if type(i) == str:\n",
    "        failed_idxs.append(idx)\n",
    "print(len(set(failed_idxs) & set(map(lambda x:x[0], reduce(lambda x,y:x+y, top_docs)))))\n",
    "print(len(set(map(lambda x:x[0], reduce(lambda x,y:x+y, top_docs)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_news = 1195\n",
    "idx_sent = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos_tag(word_tokenize(newssent[idx_news][43])))\n",
    "print(pp.WNL.lemmatize(\"has\", wordnet.VERB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(newsparse[idx_news][\"sentences\"]))\n",
    "print(newsparse[idx_news][\"sentences\"][idx_sent].keys())\n",
    "examparse = newsparse[idx_news][\"sentences\"][idx_sent]\n",
    "print(examparse[\"enhancedPlusPlusDependencies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = Tree.fromstring(examparse[\"parse\"])\n",
    "tt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4 = [t[0] for t in top_terms[3]]\n",
    "t4_values = [t[1] for t in top_terms[3]]\n",
    "print(t4)\n",
    "print(t4_values)\n",
    "idxs, importance, count = relation.extract_important_sents(newssenttoken[idx_news], t4, t4_values)\n",
    "print(idxs, importance[40], count[40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation.extract_word_relation_from_sent(i, newsparse[1200][\"sentences\"][sent_idx][\"enhancedDependencies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in top_terms[3]:\n",
    "    for news in top_docs[3]:\n",
    "        time.sleep(2)\n",
    "        news_idx = news[0]\n",
    "        for sent_idx,sent in enumerate(newssenttoken[news_idx]):\n",
    "            if type(newsparse[news_idx]) is str:\n",
    "                print(news_idx, \"parse\", \"err\")\n",
    "                break\n",
    "            topic_word_idxs = relation.get_word_idx(term[0], sent)\n",
    "            for i in topic_word_idxs:\n",
    "                print(sent_idx, i, news_idx)\n",
    "                rs = relation.extract_word_relation_from_sent(i, newsparse[news_idx][\"sentences\"][sent_idx][\"enhancedDependencies\"])\n",
    "                for r in rs:\n",
    "                    print(relation.convert_relation2str(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tree.fromstring(newsparse[idx_news][\"sentences\"][5][\"parse\"])\n",
    "t.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = persister.load_json(configs.RAWABSTRACT)\n",
    "abslda_input = persister.read_input(configs.ABSTRACTINPUT)\n",
    "terms, doc_topic, topic_word = persister.read_lda(configs.ABSTRACTLDA)\n",
    "absparse = persister.read_parse(configs.ABSTRACTPARSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param = lda.generate_lda_parameter(5,30,5,10)\n",
    "vec, _ = lda.extract_feature(abslda_input)\n",
    "model = lda.gridsearchCV(param, vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_topics = list(range(5,30,5))\n",
    "score = 'mean_test_score'\n",
    "# score = 'std_test_score'\n",
    "log_likelyhoods_5 = [round(s) for s in model.cv_results_[score][:len(n_topics)]] \n",
    "log_likelyhoods_7 = [round(s) for s in model.cv_results_[score][len(n_topics):2*len(n_topics)]]\n",
    "log_likelyhoods_9 = [round(s) for s in model.cv_results_[score][2*len(n_topics):3*len(n_topics)]]\n",
    "\n",
    "# Show graph\n",
    "plt.figure(figsize=(12, 8))\n",
    "print(log_likelyhoods_5)\n",
    "print(log_likelyhoods_7)\n",
    "plt.plot(n_topics, log_likelyhoods_5, label='0.5')\n",
    "plt.plot(n_topics, log_likelyhoods_7, label='0.7')\n",
    "plt.plot(n_topics, log_likelyhoods_9, label='0.9')\n",
    "plt.title(\"Choosing Optimal LDA Model\")\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Log Likelyhood Scores\")\n",
    "plt.legend(title='Learning decay', loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyLDAvis\n",
    "pyLDAvis.enable_notebook(True)\n",
    "panel = pyLDAvis.prepare(topic_word, doc_topic, [len(s) for s in [word_tokenize(corp) for corp in news_input]], vector_model.get_feature_names(), np.array(sum(tf).todense())[0])\n",
    "pyLDAvis.show(panel)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lda362",
   "language": "python",
   "name": "lda362"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
