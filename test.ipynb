{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inited stanford CoreNLP client, dont forget to close it!\n"
     ]
    }
   ],
   "source": [
    "import preprocess as pp\n",
    "import configs\n",
    "import persister\n",
    "import relation\n",
    "import lda\n",
    "import json\n",
    "\n",
    "from stanfordcorenlp import StanfordCoreNLP\n",
    "from nltk import sent_tokenize, word_tokenize, pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tree import Tree\n",
    "from imp import reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文档向量化测试\n",
    "# count vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = \"Many existing knowledge bases(KBs), including Freebase, Yago, and NELL, rely on a ﬁxed ontology, given as an input to the system, which deﬁnes the data to be cataloged in the KB, i.e., a hierarchy of categories and relations between them. The system then extracts facts that match the predeﬁned ontology. We propose an unsupervised model that jointly learns a latent ontological structure of an input corpus, and identiﬁes facts from the corpus that match the learned structure. Our approach combines mixed membership stochastic block models and topic models to infer a structure by jointly modeling text, a latent concept hierarchy, and latent semantic relationships among the entities mentioned in the text. As a case study, we apply the model to a corpus of Web documents from the software domain,and evaluate the accuracy of the various components of the learned ontology. \"\n",
    "b = [\"hello world. what the fuck.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = [\" \".join(preprocess(a))]\n",
    "vector = CountVectorizer(ngram_range=(1, 2), vocabulary=[\"knowledge base\"], stop_words='english')\n",
    "vector.build_analyzer()\n",
    "x = vector.fit_transform(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = CountVectorizer(ngram_range=(1, 2), vocabulary=['fuck', 'hello', 'world', \"hello world\"], stop_words='english')\n",
    "estimator.build_analyzer()\n",
    "res = estimator.fit_transform(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res.toarray()\n",
    "# estimator.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 斯坦福nlp工具测试\n",
    "# stanfordcorenlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP('/mnt/d/stanford-corenlp-full-2018-02-27', port=9000)\n",
    "sentence = 'Brink is taking part in the final exercise.'\n",
    "print('Tokenize:', nlp.word_tokenize(sentence))\n",
    "print('Part of Speech:', nlp.pos_tag(sentence))\n",
    "print('Named Entities:', nlp.ner(sentence))\n",
    "print('Constituency Parsing:', nlp.parse(sentence))\n",
    "print('Dependency Parsing:', nlp.dependency_parse(sentence))\n",
    "\n",
    "nlp.close() # Do not forget to close! The backend server will consume a lot memery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 官方corenlp api\n",
    "# stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新闻数据集测试\n",
    "# 20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import newsgroups\n",
    "size = 500\n",
    "tmp = newsgroups.get_news_data(size)\n",
    "newsdata = []\n",
    "for cate in tmp:\n",
    "    newsdata.extend(cate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [' '.join(pp.preprocess_abstract(a)) for a in tmp]\n",
    "persister.save_json(configs.NEWSDATA, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理成句子\n",
    "# nltk.sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newssent = []\n",
    "for news in newsdata:\n",
    "    newssent.append(pp.split2sent(news))\n",
    "persister.save_json(configs.NEWSSENT, newssent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newssenttoken = []\n",
    "for news in newssent:\n",
    "    tokenized_lemmatized_news = []\n",
    "    for sent in news:\n",
    "        tokenized_lemmatized_news.append(relation.lemmatize_sent_words(sent))\n",
    "    newssenttoken.append(tokenized_lemmatized_news)\n",
    "persister.save_json(configs.NEWSSENTTOKEN, newssenttoken)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load preprossed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = persister.load_json(configs.NEWSDATA)\n",
    "newssent = persister.load_json(configs.NEWSSENT)\n",
    "newssenttoken = persister.load_json(configs.NEWSSENTTOKEN)\n",
    "newsparse = []\n",
    "with open(configs.NEWSPARSE+\".json\", encoding=\"utf8\") as f:\n",
    "    for l in f:\n",
    "        newsparse.append(json.loads(l))\n",
    "# newsparse = [json.loads(n) for n in newsparse]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(newssent[1][0])\n",
    "email_reg = \"[0-9a-zA-Z_]{0,19}@(?:[0-9a-zA-Z]{1,13}[.])+\\w+\"\n",
    "reg = re.compile(email_reg)\n",
    "g = reg.findall(newssent[1][0])\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load lda res\n",
    "terms, doc_topic, topic_word = persister.read_lda(configs.NEWSLDA)\n",
    "lda.print_topics(topic_word, terms, doc_topic)\n",
    "top_terms, top_docs = lda.get_topics(topic_word, terms, doc_topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "46\n",
      "My father and I built a \"Veep\" (Volkswagen powered Jeep CJ-2A) when I was in high school, so I consider myself fairly good with tools, electronics, and cars.\n",
      "['My', 'father', 'and', 'I', 'built', 'a', '\"', 'Veep', '\"', '(', 'Volkswagen', 'powered', 'Jeep', 'CJ-2A', ')', 'when', 'I', 'wa', 'in', 'high', 'school', ',', 'so', 'I', 'consider', 'myself', 'fairly', 'good', 'with', 'tool', ',', 'electronics', ',', 'and', 'car', '.']\n",
      "howdy little new newsgroup would like tap knowledge expertise available subject market cruise control background recently broke ankle road bicycling accident place five screw yuk two week returning texas home school byu provo utah imagine trying drive nearly mile broken right ankle epitome good time car doe cruise control would pedalling ha ha messed ankle question general opinion market cruise control unit realize cheap cc cruise control say pep boy going good factory professionally installed unit thing uderstand probably expect much way accuracy look sort thing anything got ta better trying drive hosed ankle jeep cherokee speed standard l engine kettering sp ignition know distributor cap rotor set electronic maybe could guessed trying give information completly found cc unit buck seems use vehicle vacuum system instead electric servor motor good bad buy cc vacuum hose tap ha two speed sensor one magnetic one get signal negative side distributor kinda like tach pick understand use either one best manual say read store today magnetic axle set accurate harder install really big difference ha sensor brake pedal like cc doe sensor clutch pedal paying real close attention might push clutch cruise trying get speed would wind engine kinda high got wit turned thing pretty coordinated bother girlfriend car would bother ok installation also call attachment steady brake signal switched brake signal think get switched brake signal correct side brake light blade fuse right sure get steady brake signal matter exactly idea manufaturer want get think figure thing like hook negative side tach type sensing gizmo cabin control unit ground miscellaneous business need little help worth money safety risk device particularly good market cc professionally installed cc signifacantly better worth cabbage unit saw sorry manufacturer model number pep boy sufficient simple need get thing installed properly specifically question father built veep volkswagen powered jeep cj wa high school consider fairly good tool electronics car installation scare want certain get thing installed correctly cherokee wee bit complicated veep appreciate time reading post would appreciate expertise opinion anybody ha subject would like share wisdom please email get group often check mail time thanks help anyone may\n"
     ]
    }
   ],
   "source": [
    "idx_news = 1195\n",
    "idx_sent = 40\n",
    "print(len(newssent[idx_news]))\n",
    "print(newssent[idx_news][idx_sent])\n",
    "print(newssenttoken[idx_news][idx_sent])\n",
    "print(texts[idx_news])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsdata[1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In <smN42B1w165w@cybernet.cse.fau.edu> vlasis@cybernet.cse.fau.edu (vlasis theodore) writes:  >tobias@convex.com (Allen Tobias) writes:  >> In article <1993Apr15.024246.8076@Virginia.EDU> ejv2j@Virginia.EDU (\"Erik Vel >> >This happened about a year ago on the Washington DC Beltway.',\n",
       " '>> >Snot nosed drunken kids decided it would be really cool to >> >throw huge rocks down on cars from an overpass.',\n",
       " 'Four or five >> >cars were hit.',\n",
       " 'There were several serious injuries, and sadly >> >a small girl sitting in the front seat of one of them was struck  >> >in the head by one of the larger rocks.',\n",
       " \"I don't recall if she  >> >made it, but I think she was comatose for a month or so and  >> >doctors weren't holding out hope that she'd live.\",\n",
       " '>> > >> >What the hell is happening to this great country of ours?',\n",
       " 'I >> >can see boyhood pranks of peeing off of bridges and such, but >> >20 pound rocks??!',\n",
       " 'Has our society really stooped this low??',\n",
       " '>> > >> >Erik velapold >>  >> Society, as we have known it, it coming apart at the seams!',\n",
       " 'The basic reason >> is that human life has been devalued to the point were killing someone is >> \"No Big Deal\".',\n",
       " \"Kid's see hundreds on murderous acts on TV, we can abort  >> children on demand, and kill the sick and old at will.\",\n",
       " 'So why be surprised >> when some kids drop 20 lbs rocks and kill people.',\n",
       " 'They don\\'t care because the >> message they hear is \"Life is Cheap\"!',\n",
       " '>>  >> AT  >Well people fortunatly or unfortunatly , >only the US is experiencing the devaluation of human life (among  >developed nations).',\n",
       " '>I am an American but I was raised in Europe, where the worst thing that  >can happen to somebody is get his car broken into, or have his pocket >picked by Slaves or Russian refugees.',\n",
       " '>Of cource there will be some nutcases, but thats extremely rare.',\n",
       " '>I.e.',\n",
       " 'in Greece you can walk through any neighborhood at any time during >the night without even worrying.',\n",
       " '>In Germany , you can walk the sidewalks at 4.00 am and not even look  >behind your back, at the sanitation crews that clean the streets to a  >sparkling cleen.',\n",
       " '>Whoever of you have been there you know what I am saying.',\n",
       " '>I dont have any easy answers but if we as a nation do some selfcritisism >we might get somewhere.',\n",
       " \">Of course these postings sould be in soc.culture.US but if we reduce >crime here it 'll mean less car insurance rates ,thus we could spend >more money on modifing our cars.\",\n",
       " '(Now my posting is rec.autos.tech  >revelant).',\n",
       " '>Vlasis  Theodore  >___________________ >Software Engineer >IDB Mobile Communications.',\n",
       " '>Sig under development ...',\n",
       " 'I remember this happening on the I-75 through Michigan and Ohio several years back.',\n",
       " 'A group of guys in an old beater would rear end a car, usually out of state or Canadians.',\n",
       " 'You stop and they smack you with a BB bat.',\n",
       " \"At least they didn't kill you for the sake of a car.\",\n",
       " 'I think the cops put out decoys and this calmed down for a while.']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newssent[1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In <smN42B1w165w@cybernet.cse.fau.edu> vlasis@cybernet.cse.fau.edu ( vlasis theodore ) writes : > tobias@convex.com ( Allen Tobias ) writes : >> In article <1993Apr15.024246.8076@Virginia.EDU> ejv2j@Virginia.EDU ( \" Erik Vel >> > This happened about a year ago on the Washington DC Beltway .',\n",
       " '>> > Snot nosed drunken kids decided it would be really cool to >> > throw huge rocks down on cars from an overpass .',\n",
       " 'Four or five >> > cars were hit .',\n",
       " 'There were several serious injuries , and sadly >> > a small girl sitting in the front seat of one of them was struck >> > in the head by one of the larger rocks .',\n",
       " \"I do n't recall if she >> > made it , but I think she was comatose for a month or so and >> > doctors were n't holding out hope that she 'd live .\",\n",
       " '>> > >> > What the hell is happening to this great country of ours ?',\n",
       " 'I >> > can see boyhood pranks of peeing off of bridges and such , but >> > 20 pound rocks ??!',\n",
       " 'Has our society really stooped this low ??',\n",
       " '>> > >> > Erik velapold >> >> Society , as we have known it , it coming apart at the seams !',\n",
       " 'The basic reason >> is that human life has been devalued to the point were killing someone is >> \" No Big Deal \" .',\n",
       " \"Kid 's see hundreds on murderous acts on TV , we can abort >> children on demand , and kill the sick and old at will .\",\n",
       " 'So why be surprised >> when some kids drop 20 lbs rocks and kill people .',\n",
       " 'They do n\\'t care because the >> message they hear is \" Life is Cheap \" !',\n",
       " '>> >> AT > Well people fortunatly or unfortunatly , > only the US is experiencing the devaluation of human life ( among > developed nations ) . >',\n",
       " 'I am an American but I was raised in Europe , where the worst thing that > can happen to somebody is get his car broken into , or have his pocket > picked by Slaves or Russian refugees . >',\n",
       " 'Of cource there will be some nutcases , but thats extremely rare . >',\n",
       " 'I.e. in Greece you can walk through any neighborhood at any time during > the night without even worrying . >',\n",
       " 'In Germany , you can walk the sidewalks at 4.00 am and not even look > behind your back , at the sanitation crews that clean the streets to a > sparkling cleen . >',\n",
       " 'Whoever of you have been there you know what I am saying . >',\n",
       " 'I dont have any easy answers but if we as a nation do some selfcritisism > we might get somewhere . >',\n",
       " \"Of course these postings sould be in soc.culture.US but if we reduce > crime here it 'll mean less car insurance rates , thus we could spend > more money on modifing our cars .\",\n",
       " '( Now my posting is rec.autos.tech > revelant ) . >',\n",
       " 'Vlasis Theodore > ___________________ > Software Engineer > IDB Mobile Communications . >',\n",
       " 'Sig under development ... I remember this happening on the I-75 through Michigan and Ohio several years back .',\n",
       " 'A group of guys in an old beater would rear end a car , usually out of state or Canadians .',\n",
       " 'You stop and they smack you with a BB bat .',\n",
       " \"At least they did n't kill you for the sake of a car .\",\n",
       " 'I think the cops put out decoys and this calmed down for a while .']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[\" \".join([w[\"originalText\"] for w in sent[\"tokens\"]]) for sent in newsparse[1200][\"sentences\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pos_tag(word_tokenize(newssent[idx_news][43])))\n",
    "print(pp.WNL.lemmatize(\"has\", wordnet.VERB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(newsparse[idx_news][\"sentences\"]))\n",
    "print(newsparse[idx_news][\"sentences\"][idx_sent].keys())\n",
    "examparse = newsparse[idx_news][\"sentences\"][idx_sent]\n",
    "print(examparse[\"enhancedPlusPlusDependencies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tt = Tree.fromstring(examparse[\"parse\"])\n",
    "tt.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t4 = [t[0] for t in top_terms[3]]\n",
    "t4_values = [t[1] for t in top_terms[3]]\n",
    "print(t4)\n",
    "print(t4_values)\n",
    "idxs, importance, count = relation.extract_important_sents(newssenttoken[idx_news], t4, t4_values)\n",
    "print(idxs, importance[40], count[40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relation.extract_word_relation_from_sent(i, newsparse[1200][\"sentences\"][sent_idx][\"enhancedDependencies\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for term in top_terms[3]:\n",
    "    for news in top_docs[3]:\n",
    "        time.sleep(2)\n",
    "        news_idx = news[0]\n",
    "        for sent_idx,sent in enumerate(newssenttoken[news_idx]):\n",
    "            if type(newsparse[news_idx]) is str:\n",
    "                print(news_idx, \"parse\", \"err\")\n",
    "                break\n",
    "            topic_word_idxs = relation.get_word_idx(term[0], sent)\n",
    "            for i in topic_word_idxs:\n",
    "                print(sent_idx, i, news_idx)\n",
    "                rs = relation.extract_word_relation_from_sent(i, newsparse[news_idx][\"sentences\"][sent_idx][\"enhancedDependencies\"])\n",
    "                for r in rs:\n",
    "                    print(relation.convert_relation2str(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = Tree.fromstring(newsparse[idx_news][\"sentences\"][5][\"parse\"])\n",
    "t.pretty_print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test example res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"exampleres.json\",encoding=\"utf8\")as f:\n",
    "    res = json.load(f)\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lda362",
   "language": "python",
   "name": "lda362"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
