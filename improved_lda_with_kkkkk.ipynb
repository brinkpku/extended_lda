{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import random\n",
    "from string import punctuation\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DE: keywords\n",
    "# ID: extended keywords\n",
    "# TI: title\n",
    "# AB: abstract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load data..\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2596 entries, 0 to 2595\n",
      "Data columns (total 4 columns):\n",
      "TI    2596 non-null object\n",
      "DE    2596 non-null object\n",
      "ID    2596 non-null object\n",
      "AB    2596 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 40.6+ KB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TI</th>\n",
       "      <th>DE</th>\n",
       "      <th>ID</th>\n",
       "      <th>AB</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Identifying Emerging Trends and Temporal Patte...</td>\n",
       "      <td>Self-driving car; Clustering; Term burst detec...</td>\n",
       "      <td>SCIENCE; VEHICLES; DYNAMICS</td>\n",
       "      <td>Self-driving is an emerging technology which h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Data Analysis of Tourists' Online Reviews on R...</td>\n",
       "      <td>Online reviews; Text mining; Latent Dirichlet ...</td>\n",
       "      <td>PERCEPTIONS; CULTURE; QUALITY</td>\n",
       "      <td>The proliferation of online consumer reviews h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Development strategies for heavy duty electric...</td>\n",
       "      <td>Heavy duty vehicle; Electric vehicle; Battery;...</td>\n",
       "      <td>EARTH-ELEMENTS DEMAND; FUEL-CELL TECHNOLOGY; P...</td>\n",
       "      <td>This paper investigates the development of hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Topic-based rank search with verifiable social...</td>\n",
       "      <td>Topic-based rank search; Verifiable social dat...</td>\n",
       "      <td>ENABLING EFFICIENT; QUERY</td>\n",
       "      <td>As the explosive development of social network...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Identifying topic relevant hashtags in Twitter...</td>\n",
       "      <td>Text mining; Topic modeling; Latent Dirichlet ...</td>\n",
       "      <td></td>\n",
       "      <td>Hashtags have become a crucial social media to...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  TI  \\\n",
       "0  Identifying Emerging Trends and Temporal Patte...   \n",
       "1  Data Analysis of Tourists' Online Reviews on R...   \n",
       "2  Development strategies for heavy duty electric...   \n",
       "3  Topic-based rank search with verifiable social...   \n",
       "4  Identifying topic relevant hashtags in Twitter...   \n",
       "\n",
       "                                                  DE  \\\n",
       "0  Self-driving car; Clustering; Term burst detec...   \n",
       "1  Online reviews; Text mining; Latent Dirichlet ...   \n",
       "2  Heavy duty vehicle; Electric vehicle; Battery;...   \n",
       "3  Topic-based rank search; Verifiable social dat...   \n",
       "4  Text mining; Topic modeling; Latent Dirichlet ...   \n",
       "\n",
       "                                                  ID  \\\n",
       "0                        SCIENCE; VEHICLES; DYNAMICS   \n",
       "1                      PERCEPTIONS; CULTURE; QUALITY   \n",
       "2  EARTH-ELEMENTS DEMAND; FUEL-CELL TECHNOLOGY; P...   \n",
       "3                          ENABLING EFFICIENT; QUERY   \n",
       "4                                                      \n",
       "\n",
       "                                                  AB  \n",
       "0  Self-driving is an emerging technology which h...  \n",
       "1  The proliferation of online consumer reviews h...  \n",
       "2  This paper investigates the development of hea...  \n",
       "3  As the explosive development of social network...  \n",
       "4  Hashtags have become a crucial social media to...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('load data..')\n",
    "whole_data = None\n",
    "for i in range(1, 7):\n",
    "    df = pd.read_csv('data/{}.txt'.format(str(i)), \n",
    "                     delimiter='\\t',  \n",
    "                     usecols=['DE', 'ID', 'TI', 'AB'], \n",
    "                     encoding='utf8',\n",
    "                     index_col=False,\n",
    "                     dtype=np.str)\n",
    "    df = df[df['AB'].notnull() & df['TI'].notnull()]  # filter null abstract\n",
    "    df = df.fillna('')\n",
    "    if whole_data is None:\n",
    "        whole_data = df\n",
    "    else:\n",
    "        whole_data = pd.concat([whole_data, df], ignore_index=True)\n",
    "whole_data.info()\n",
    "whole_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"In traditional health recommending system the recommendations are not personalized according to the patient's, the recommendation extremely depends on physical, emotional and psychological matters of the user list is generated based on the diseases the patient navigates. In health era, personalized health recommendation helps us to extract personalized health content form overloading information's available on the web. the patient's, the initial step is to identify the patient interest in which health related issues they needs recommendation. In this paper we have used statistical topic modeling technique Hierarchical Latent Dirichlet Allocation (HLDA) to identify the user interest which provides robust and interpretable topic representation. After identifying the user interest, neighborhood selection is done based on ranking and finally recommendation is done according to user preference. In this model we have learned six parameters, in parameter(1) the topical distribution of each document is learnt, in parameter (2) the perspective distribution of each user is learnt, in parameter (3) the word distribution of each topic is learnt, in parameter (4) the tag distribution of each topic is learnt, in parameter(5) the tag distribution of each user perspective is learnt, in parameter(6) the probabilistic of each tag being generated from resource topics or user perspectives is learnt. Our Experimental results show that proposed model is better than State - of - Art than other models.\",\n",
       " \"We present a methodology for analyzing cross-cultural similarities and differences using language as a medium, love as domain, social media as a data source and 'Terms' and 'Topics' as cultural features. We discuss the techniques necessary for the creation of the social data corpus from which emotion terms have been extracted using NLP techniques. Topics of love discussion were then extracted from the corpus by means of Latent Dirichlet Allocation (LDA). Finally, on the basis of these features, a cross-cultural comparison was carried out. For the purpose of cross-cultural analysis, the experimental focus was on comparing data from a culture from the East (India) with a culture from the West (United States of America). Similarities and differences between these cultures have been analyzed with respect to the usage of emotions, their intensities and the topics used during love discussion in social media.\",\n",
       " 'A number of approaches in traceability link recovery and other software engineering tasks incorporate topic models, such as Latent Dirichlet Allocation (LDA). Although in theory these topic models can produce very good results if they are configured properly, in reality their potential may be undermined by improper calibration of their parameters (e. g., number of topics, hyper-parameters), which could potentially lead to sub-optimal results. In our previous work we addressed this issue and proposed LDA-GA, an approach that uses Genetic Algorithms (GA) to find a near-optimal configuration of parameters for LDA, which was shown to produce superior results for traceability link recovery and other tasks than reported ad-hoc configurations. LDA-GA works by optimizing the coherence of topics produced by LDA for a given dataset. In this paper, we instantiate LDA-GA as a TraceLab experiment, making publicly available all the implemented components, the datasets and the results from our previous work. In addition, we provide guidelines on how to extend our LDA-GA approach to other IR techniques and other software engineering tasks using existing TraceLab components.',\n",
       " 'The ease with which data can be created, copied, modified, and deleted over the Internet has made it increasingly difficult to determine the source of web data. Data provenance, which provides information about the origin and lineage of a dataset, assists in determining its genuineness and trustworthiness. Several data provenance techniques record provenance when the data is created or modified. However, many existing datasets have no recorded provenance. Provenance Reconstruction techniques attempt to generate an approximate provenance in these datasets. Current reconstruction techniques require timing metadata to reconstruct provenance. In this paper, we improve our multi-funneling technique, which combines existing techniques, including topic modeling, longest common subsequence, and genetic algorithm to achieve higher accuracy in reconstructing provenance without requiring timing metadata. In addition, we introduce novel funnels that are customized to the provided datasets, which further boosts precision and recall rates. We evaluated our approach with various experiments and compare the results of our approach with existing techniques. Finally, we present lessons learned, including the applicability of our approach to other datasets.',\n",
       " \"While research has investigated various aspects of electronic word of mouth and their effects on business performance, it has neglected detailed topics in reviews. This study explores this new aspect in reviews and examines the effects of review topics on Airbnb listing performance. We use 2,799,420 reviews from 64,464 listings posted on the Airbnb platform in 10 U.S. cities. Using the latent Dirichlet allocation method, we identify 16 key topics of consumer reviews on Airbnb. Then, using a negative binomial regression model, we show that various factors can affect a listing's performance on Airbnb. These findings have implications for Airbnb hosts and the sharing economy.\"]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.choices(whole_data['AB'], k=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wnl = WordNetLemmatizer()\n",
    "url = '(https?|ftp|file)://[-A-Za-z0-9+&@#/%?=~_|!:,.;]+[-A-Za-z0-9+&@#/%=~_|]'\n",
    "def preprocess(abstract):\n",
    "    '''\n",
    "    preprocess abstract: lower, remove punctuations, tokenize\n",
    "    :param abstract: str\n",
    "    :return: list\n",
    "    '''\n",
    "    abstract = re.sub(url, ' ', abstract)\n",
    "    abstract = re.sub('\\d+?', ' ', abstract)\n",
    "    for p in punctuation:\n",
    "        abstract = re.sub(re.escape(p), ' ', abstract)\n",
    "    abstract = abstract.lower()\n",
    "    abstract = [wnl.lemmatize(w) for w in word_tokenize(abstract)]\n",
    "    filtered = [w for w in abstract if w not in stopwords.words('english')]\n",
    "    return filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_lda(input_text, feature_method='tf', topic_num=5, vocabulary=None, method=\"batch\"):\n",
    "    '''\n",
    "    do lda process\n",
    "    :param input_text: list, [str,], preprocessed text\n",
    "    :return: tuple of np.array, terms, doc-topic probability, topic-word probability, perplexity\n",
    "    '''\n",
    "    vector = None\n",
    "    if feature_method == 'tf':\n",
    "        vector = CountVectorizer(ngram_range=(1, 1), vocabulary=vocabulary, stop_words='english')\n",
    "        vector.build_analyzer()\n",
    "    if feature_method == 'idf':\n",
    "        vector = TfidfVectorizer(ngram_range=(2, 2), vocabulary=vocabulary, stop_words='english')\n",
    "        vector.build_analyzer()\n",
    "    x = vector.fit_transform(input_text)\n",
    "    lda = LatentDirichletAllocation(n_components=topic_num, learning_method=method, max_iter=20, random_state=0,\n",
    "                                    batch_size=128, topic_word_prior=0.5 / topic_num)\n",
    "    lda_topics = lda.fit_transform(x)\n",
    "    return np.array(vector.get_feature_names()), lda_topics, lda.components_, lda.perplexity(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_topics(topic_word, terms, num=20):\n",
    "    '''\n",
    "    print topics\n",
    "    :param topic_word: np.array, topic-word probability\n",
    "    :param terms: np.array, feature names\n",
    "    :param num: int, term num of topic to print\n",
    "    :return: None\n",
    "    '''\n",
    "    for idx, t in enumerate(topic_word):\n",
    "        sort_idx = np.argsort(t)\n",
    "        print(\"#\", idx + 1, \"-\" * 20)\n",
    "        print(terms[sort_idx[-1:-num - 1:-1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1 --------------------\n",
      "['review' 'analysis' 'online' 'product' 'research' 'sentiment' 'data'\n",
      " 'study' 'opinion' 'customer' 'text' 'mining' 'aspect' 'consumer'\n",
      " 'business' 'student' 'public' 'topic' 'result' 'service']\n",
      "# 2 --------------------\n",
      "['model' 'topic' 'latent' 'method' 'document' 'dirichlet' 'approach'\n",
      " 'using' 'based' 'paper' 'allocation' 'lda' 'proposed' 'distribution'\n",
      " 'word' 'result' 'sentence' 'summarization' 'algorithm' 'task']\n",
      "# 3 --------------------\n",
      "['software' 'topic' 'code' 'developer' 'source' 'application' 'lda'\n",
      " 'latent' 'result' 'project' 'approach' 'dirichlet' 'allocation' 'based'\n",
      " 'analysis' 'research' 'model' 'method' 'paper' 'using']\n",
      "# 4 --------------------\n",
      "['message' 'drug' 'learning' 'spam' 'latent' 'method' 'approach' 'wa'\n",
      " 'group' 'lda' 'allocation' 'dirichlet' 'study' 'sm' 'ha' 'data'\n",
      " 'information' 'language' 'comment' 'used']\n",
      "# 5 --------------------\n",
      "['topic' 'question' 'information' 'concept' 'model' 'latent' 'method'\n",
      " 'behavior' 'allocation' 'dirichlet' 'robot' 'network' 'using' 'result'\n",
      " 'study' 'paper' 'based' 'used' 'proposed' 'new']\n",
      "# 6 --------------------\n",
      "['information' 'query' 'retrieval' 'search' 'approach' 'web' 'result'\n",
      " 'user' 'method' 'based' 'latent' 'similarity' 'using' 'content' 'feature'\n",
      " 'word' 'semantic' 'service' 'song' 'model']\n",
      "# 7 --------------------\n",
      "['user' 'recommendation' 'tag' 'social' 'service' 'latent' 'item' 'method'\n",
      " 'preference' 'based' 'information' 'model' 'allocation' 'dirichlet'\n",
      " 'approach' 'network' 'proposed' 'propose' 'personalized' 'rating']\n",
      "# 8 --------------------\n",
      "['model' 'algorithm' 'inference' 'latent' 'dirichlet' 'sampling' 'data'\n",
      " 'lda' 'gibbs' 'method' 'allocation' 'variational' 'topic' 'bayesian'\n",
      " 'using' 'parameter' 'distribution' 'learning' 'efficient' 'online']\n",
      "# 9 --------------------\n",
      "['topic' 'activity' 'research' 'time' 'pattern' 'data' 'analysis' 'change'\n",
      " 'study' 'community' 'latent' 'approach' 'trend' 'area' 'model'\n",
      " 'evolution' 'article' 'journal' 'dirichlet' 'allocation']\n",
      "# 10 --------------------\n",
      "['news' 'medium' 'article' 'information' 'analysis' 'social' 'topic'\n",
      " 'time' 'corpus' 'text' 'allocation' 'latent' 'wa' 'using' 'financial'\n",
      " 'paper' 'method' 'twitter' 'digital' 'tweet']\n",
      "# 11 --------------------\n",
      "['patient' 'medical' 'clinical' 'bug' 'health' 'disease' 'treatment'\n",
      " 'report' 'topic' 'method' 'record' 'care' 'pathway' 'process' 'latent'\n",
      " 'approach' 'model' 'healthcare' 'modeling' 'allocation']\n",
      "# 12 --------------------\n",
      "['topic' 'model' 'social' 'data' 'tweet' 'network' 'user' 'information'\n",
      " 'twitter' 'method' 'latent' 'dirichlet' 'lda' 'allocation' 'based'\n",
      " 'emotion' 'approach' 'paper' 'community' 'result']\n",
      "# 13 --------------------\n",
      "['topic' 'document' 'text' 'model' 'lda' 'word' 'method' 'latent'\n",
      " 'dirichlet' 'allocation' 'corpus' 'paper' 'modeling' 'term' 'based'\n",
      " 'result' 'semantic' 'proposed' 'information' 'used']\n",
      "# 14 --------------------\n",
      "['model' 'latent' 'lda' 'topic' 'approach' 'data' 'using' 'dirichlet'\n",
      " 'method' 'language' 'document' 'sentiment' 'allocation' 'analysis'\n",
      " 'result' 'word' 'learning' 'proposed' 'domain' 'paper']\n",
      "# 15 --------------------\n",
      "['event' 'article' 'social' 'result' 'using' 'topic' 'dirichlet'\n",
      " 'disaster' 'data' 'approach' 'detect' 'model' 'policy' 'medium' 'latent'\n",
      " 'meeting' 'change' 'allocation' 'video' 'bank']\n",
      "# 16 --------------------\n",
      "['topic' 'model' 'data' 'lda' 'latent' 'dirichlet' 'allocation' 'word'\n",
      " 'distribution' 'paper' 'modeling' 'time' 'method' 'aspect' 'using' 'used'\n",
      " 'result' 'algorithm' 'number' 'ha']\n",
      "# 17 --------------------\n",
      "['topic' 'study' 'tv' 'lda' 'place' 'latent' 'related' 'wa' 'result'\n",
      " 'model' 'method' 'use' 'relevant' 'label' 'program' 'allocation'\n",
      " 'dirichlet' 'social' 'medium' 'group']\n",
      "# 18 --------------------\n",
      "['service' 'technology' 'topic' 'data' 'research' 'patent' 'method' 'lda'\n",
      " 'latent' 'dirichlet' 'allocation' 'analysis' 'paper' 'study'\n",
      " 'development' 'using' 'field' 'scientific' 'approach' 'literature']\n",
      "# 19 --------------------\n",
      "['model' 'image' 'feature' 'method' 'classification' 'topic' 'latent'\n",
      " 'proposed' 'representation' 'dirichlet' 'allocation' 'lda' 'learning'\n",
      " 'scene' 'result' 'object' 'word' 'performance' 'based' 'using']\n",
      "# 20 --------------------\n",
      "['video' 'method' 'cluster' 'algorithm' 'proposed' 'clustering' 'motion'\n",
      " 'pattern' 'based' 'latent' 'model' 'dirichlet' 'allocation' 'action'\n",
      " 'result' 'lda' 'human' 'sequence' 'behavior' 'data']\n"
     ]
    }
   ],
   "source": [
    "input_text = [' '.join(preprocess(a)) for a in whole_data['AB']]\n",
    "vocab = None\n",
    "topic_param = 20\n",
    "terms, doc_topic, topic_word, perplexity = do_lda(input_text, 'tf', topic_param, vocab)\n",
    "print_topics(topic_word, terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 2596 entries, 0 to 2595\n",
      "Data columns (total 4 columns):\n",
      "TI    2596 non-null object\n",
      "DE    2596 non-null object\n",
      "ID    2596 non-null object\n",
      "AB    2596 non-null object\n",
      "dtypes: object(4)\n",
      "memory usage: 40.6+ KB\n"
     ]
    }
   ],
   "source": [
    "whole_data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords(raw_list):\n",
    "    tmp_set = set()\n",
    "    for r in raw_list:\n",
    "        for keyword in r.split(\";\"):\n",
    "            processed = \" \".join([wnl.lemmatize(w) for w in word_tokenize(keyword.lower())])\n",
    "            tmp_set.add(processed)\n",
    "    return sorted(list(tmp_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['social audience',\n",
       " 'application essay',\n",
       " 'customer',\n",
       " 'theme',\n",
       " 'topic analysis',\n",
       " 'latent dirchlet allocation',\n",
       " 'active learning',\n",
       " 'behavioral segmentation',\n",
       " 'siti',\n",
       " 'hadoop/mapreduce']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keywords = get_keywords(whole_data[\"DE\"])\n",
    "random.choices(keywords, k=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zero\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\decomposition\\_lda.py:804: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  perword_bound = bound / word_cnt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# 1 --------------------\n",
      "['latent dirchlet allocation']\n",
      "# 2 --------------------\n",
      "['latent dirchlet allocation']\n",
      "# 3 --------------------\n",
      "['latent dirchlet allocation']\n",
      "# 4 --------------------\n",
      "['latent dirchlet allocation']\n",
      "# 5 --------------------\n",
      "['latent dirchlet allocation']\n",
      "# 6 --------------------\n",
      "['latent dirchlet allocation']\n",
      "# 7 --------------------\n",
      "['latent dirchlet allocation']\n",
      "# 8 --------------------\n",
      "['latent dirchlet allocation']\n",
      "# 9 --------------------\n",
      "['latent dirchlet allocation']\n",
      "# 10 --------------------\n",
      "['latent dirchlet allocation']\n",
      "# 11 --------------------\n",
      "['latent dirchlet allocation']\n",
      "# 12 --------------------\n",
      "['latent dirchlet allocation']\n",
      "# 13 --------------------\n",
      "['latent dirchlet allocation']\n",
      "# 14 --------------------\n",
      "['latent dirchlet allocation']\n",
      "# 15 --------------------\n",
      "['latent dirchlet allocation']\n",
      "# 16 --------------------\n",
      "['latent dirchlet allocation']\n",
      "# 17 --------------------\n",
      "['latent dirchlet allocation']\n",
      "# 18 --------------------\n",
      "['latent dirchlet allocation']\n",
      "# 19 --------------------\n",
      "['latent dirchlet allocation']\n",
      "# 20 --------------------\n",
      "['latent dirchlet allocation']\n",
      "used: 523.9720187187195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\zero\\appdata\\local\\programs\\python\\python36-32\\lib\\site-packages\\sklearn\\decomposition\\_lda.py:804: RuntimeWarning: divide by zero encountered in double_scalars\n",
      "  perword_bound = bound / word_cnt\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "input_text = [' '.join(preprocess(a)) for a in whole_data['AB']]\n",
    "vocab = [\"latent dirchlet allocation\"]\n",
    "topic_param = 20\n",
    "terms, doc_topic, topic_word, perplexity = do_lda(input_text, 'tf', topic_param, vocab)\n",
    "print_topics(topic_word, terms)\n",
    "print(\"used:\", time.time()-start_time)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
