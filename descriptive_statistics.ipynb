{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not use stanford CoreNLP client!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading sentiwordnet: <urlopen error [Errno 111]\n",
      "[nltk_data]     Connection refused>\n"
     ]
    }
   ],
   "source": [
    "import preprocess as pp\n",
    "import configs\n",
    "import persister\n",
    "import relation\n",
    "import lda\n",
    "import vis\n",
    "\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tree import Tree\n",
    "from nltk import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load abs\n"
     ]
    }
   ],
   "source": [
    "# is_news = 1\n",
    "\n",
    "# l = 7\n",
    "# t = 4\n",
    "# m = \"c_v\"\n",
    "# i = 200\n",
    "# min_df = 1\n",
    "\n",
    "is_news = 0\n",
    "l = 6\n",
    "t = 14\n",
    "m = \"c_v\"\n",
    "i = 200\n",
    "min_df = 1\n",
    "\n",
    "# load\n",
    "if is_news:\n",
    "    _raw = persister.load_json(configs.RAWNEWS)\n",
    "    parse = persister.read_parse()\n",
    "    _input = persister.read_input(configs.NEWSINPUT)\n",
    "    model_name = configs.NEWSMODEL.format(l, t, m, i, min_df)\n",
    "    terms, doc_topic, topic_word = persister.read_lda(configs.NEWSLDA.format(model_name))\n",
    "    model = persister.load_model(model_name)\n",
    "    vec = persister.load_model(configs.NEWSVEC.format(min_df))\n",
    "    print(\"load news\")\n",
    "else:\n",
    "    _raw = persister.load_json(configs.RAWABSTRACT)\n",
    "    _input = persister.read_input(configs.ABSTRACTINPUT)\n",
    "    model_name = configs.ABSTRACTMODEL.format(l, t, m, i, min_df)\n",
    "    terms, doc_topic, topic_word = persister.read_lda(configs.ABSTRACTLDA.format(model_name))\n",
    "    parse = persister.read_parse(configs.ABSTRACTPARSE)\n",
    "    model = persister.load_model(model_name)\n",
    "    vec = persister.load_model(configs.ABSVEC.format(min_df))\n",
    "    print(\"load abs\")\n",
    "tf = vec.fit_transform(_input) \n",
    "top_terms, top_docs = lda.get_topics(topic_word, terms, doc_topic)\n",
    "df_top_words, df_top_docs = lda.pd_topics_vis(top_terms, top_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10453"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs distribution: {9: 152, 5: 33, 11: 969, 12: 1330, 1: 7, 13: 3, 10: 76, 3: 3, 0: 8, 7: 14, 6: 1}\n"
     ]
    }
   ],
   "source": [
    "distr = lda.get_dominant_topic(doc_topic)\n",
    "print(\"docs distribution:\",dict(Counter(distr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top terms info:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "      <th>Word 11</th>\n",
       "      <th>Word 12</th>\n",
       "      <th>Word 13</th>\n",
       "      <th>Word 14</th>\n",
       "      <th>Word 15</th>\n",
       "      <th>Word 16</th>\n",
       "      <th>Word 17</th>\n",
       "      <th>Word 18</th>\n",
       "      <th>Word 19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>(gene, 123.1)</td>\n",
       "      <td>(drug, 96.87)</td>\n",
       "      <td>(expression, 78.19)</td>\n",
       "      <td>(disease, 77.49)</td>\n",
       "      <td>(protein, 75.45)</td>\n",
       "      <td>(biomedical, 64.39)</td>\n",
       "      <td>(pathway, 51.34)</td>\n",
       "      <td>(cell, 51.19)</td>\n",
       "      <td>(identify, 48.51)</td>\n",
       "      <td>(biological, 47.03)</td>\n",
       "      <td>(btm, 40.99)</td>\n",
       "      <td>(ad, 40.76)</td>\n",
       "      <td>(study, 39.19)</td>\n",
       "      <td>(apply, 37.74)</td>\n",
       "      <td>(manufacturing, 31.56)</td>\n",
       "      <td>(mrna, 29.42)</td>\n",
       "      <td>(relationship, 29.31)</td>\n",
       "      <td>(conclusion, 28.59)</td>\n",
       "      <td>(tool, 26.52)</td>\n",
       "      <td>(related, 25.93)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>(patient, 223.06)</td>\n",
       "      <td>(medical, 143.69)</td>\n",
       "      <td>(health, 143.5)</td>\n",
       "      <td>(clinical, 139.72)</td>\n",
       "      <td>(treatment, 99.44)</td>\n",
       "      <td>(diagnosis, 86.78)</td>\n",
       "      <td>(disease, 81.07)</td>\n",
       "      <td>(risk, 79.0)</td>\n",
       "      <td>(record, 78.67)</td>\n",
       "      <td>(care, 68.06)</td>\n",
       "      <td>(friend, 47.64)</td>\n",
       "      <td>(healthcare, 42.31)</td>\n",
       "      <td>(cancer, 39.36)</td>\n",
       "      <td>(code, 36.28)</td>\n",
       "      <td>(data, 32.74)</td>\n",
       "      <td>(conclusion, 30.77)</td>\n",
       "      <td>(physician, 29.99)</td>\n",
       "      <td>(emr, 25.48)</td>\n",
       "      <td>(ehr, 25.13)</td>\n",
       "      <td>(doctor, 24.57)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>(mirna, 42.64)</td>\n",
       "      <td>(permission, 25.17)</td>\n",
       "      <td>(quran, 13.82)</td>\n",
       "      <td>(holy, 9.76)</td>\n",
       "      <td>(aircraft, 8.0)</td>\n",
       "      <td>(provenance, 7.74)</td>\n",
       "      <td>(lv, 7.74)</td>\n",
       "      <td>(laplace, 7.05)</td>\n",
       "      <td>(ast, 6.96)</td>\n",
       "      <td>(chara, 6.65)</td>\n",
       "      <td>(yuru, 6.65)</td>\n",
       "      <td>(ldaclm, 6.22)</td>\n",
       "      <td>(privilege, 5.86)</td>\n",
       "      <td>(mmrm, 5.86)</td>\n",
       "      <td>(iexpand, 5.13)</td>\n",
       "      <td>(configure, 5.01)</td>\n",
       "      <td>(transcriptional, 5.0)</td>\n",
       "      <td>(pcfg, 4.14)</td>\n",
       "      <td>(tctm, 4.12)</td>\n",
       "      <td>(seqlda, 4.12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>(emotion, 181.73)</td>\n",
       "      <td>(drive, 87.04)</td>\n",
       "      <td>(style, 86.19)</td>\n",
       "      <td>(speech, 55.34)</td>\n",
       "      <td>(genre, 39.85)</td>\n",
       "      <td>(recognition, 38.09)</td>\n",
       "      <td>(speaker, 34.84)</td>\n",
       "      <td>(state, 33.33)</td>\n",
       "      <td>(write, 31.62)</td>\n",
       "      <td>(language, 30.16)</td>\n",
       "      <td>(acoustic, 30.1)</td>\n",
       "      <td>(driving, 29.08)</td>\n",
       "      <td>(driver, 27.25)</td>\n",
       "      <td>(essay, 26.57)</td>\n",
       "      <td>(sleep, 24.96)</td>\n",
       "      <td>(vehicle, 24.49)</td>\n",
       "      <td>(authorship, 22.67)</td>\n",
       "      <td>(noca, 22.09)</td>\n",
       "      <td>(author, 21.59)</td>\n",
       "      <td>(disorder, 21.36)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>(sampling, 79.5)</td>\n",
       "      <td>(parallel, 73.15)</td>\n",
       "      <td>(gibbs, 67.68)</td>\n",
       "      <td>(security, 57.19)</td>\n",
       "      <td>(distribute, 52.59)</td>\n",
       "      <td>(collapse, 42.46)</td>\n",
       "      <td>(cluster, 33.26)</td>\n",
       "      <td>(bot, 32.4)</td>\n",
       "      <td>(brand, 31.0)</td>\n",
       "      <td>(large, 30.54)</td>\n",
       "      <td>(crime, 29.71)</td>\n",
       "      <td>(scale, 29.68)</td>\n",
       "      <td>(token, 29.34)</td>\n",
       "      <td>(approximation, 28.55)</td>\n",
       "      <td>(memory, 27.44)</td>\n",
       "      <td>(gpu, 27.32)</td>\n",
       "      <td>(sampler, 26.77)</td>\n",
       "      <td>(matrix, 23.32)</td>\n",
       "      <td>(atomic, 23.12)</td>\n",
       "      <td>(mh, 22.88)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 5</th>\n",
       "      <td>(review, 537.88)</td>\n",
       "      <td>(sentiment, 474.31)</td>\n",
       "      <td>(opinion, 265.32)</td>\n",
       "      <td>(tweet, 235.55)</td>\n",
       "      <td>(product, 228.45)</td>\n",
       "      <td>(post, 206.48)</td>\n",
       "      <td>(online, 196.94)</td>\n",
       "      <td>(topic, 194.3)</td>\n",
       "      <td>(analysis, 180.66)</td>\n",
       "      <td>(twitter, 174.52)</td>\n",
       "      <td>(media, 171.56)</td>\n",
       "      <td>(aspect, 166.01)</td>\n",
       "      <td>(customer, 159.65)</td>\n",
       "      <td>(rating, 142.65)</td>\n",
       "      <td>(consumer, 111.75)</td>\n",
       "      <td>(social, 101.7)</td>\n",
       "      <td>(use, 101.33)</td>\n",
       "      <td>(negative, 88.63)</td>\n",
       "      <td>(positive, 85.19)</td>\n",
       "      <td>(study, 81.34)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 6</th>\n",
       "      <td>(song, 81.12)</td>\n",
       "      <td>(audio, 74.02)</td>\n",
       "      <td>(music, 57.13)</td>\n",
       "      <td>(pn, 35.01)</td>\n",
       "      <td>(broadcast, 28.77)</td>\n",
       "      <td>(oov, 24.22)</td>\n",
       "      <td>(indonesian, 19.81)</td>\n",
       "      <td>(musical, 18.49)</td>\n",
       "      <td>(lyric, 17.49)</td>\n",
       "      <td>(estimate, 16.39)</td>\n",
       "      <td>(writer, 14.97)</td>\n",
       "      <td>(commonness, 14.53)</td>\n",
       "      <td>(submit, 13.59)</td>\n",
       "      <td>(hyperspectral, 11.69)</td>\n",
       "      <td>(eem, 10.71)</td>\n",
       "      <td>(mp, 10.39)</td>\n",
       "      <td>(isoform, 9.79)</td>\n",
       "      <td>(retrieve, 9.73)</td>\n",
       "      <td>(replicate, 9.62)</td>\n",
       "      <td>(timbre, 8.87)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 7</th>\n",
       "      <td>(software, 268.18)</td>\n",
       "      <td>(question, 203.92)</td>\n",
       "      <td>(code, 149.7)</td>\n",
       "      <td>(developer, 143.61)</td>\n",
       "      <td>(source, 143.52)</td>\n",
       "      <td>(bug, 139.43)</td>\n",
       "      <td>(report, 120.26)</td>\n",
       "      <td>(project, 87.55)</td>\n",
       "      <td>(program, 68.98)</td>\n",
       "      <td>(answer, 65.81)</td>\n",
       "      <td>(repository, 53.22)</td>\n",
       "      <td>(requirement, 49.86)</td>\n",
       "      <td>(evolution, 49.8)</td>\n",
       "      <td>(technique, 48.97)</td>\n",
       "      <td>(open, 44.34)</td>\n",
       "      <td>(study, 43.24)</td>\n",
       "      <td>(lsi, 40.78)</td>\n",
       "      <td>(stack, 33.35)</td>\n",
       "      <td>(change, 30.56)</td>\n",
       "      <td>(nfr, 30.05)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 8</th>\n",
       "      <td>(lm, 75.09)</td>\n",
       "      <td>(frame, 55.04)</td>\n",
       "      <td>(library, 31.56)</td>\n",
       "      <td>(asr, 28.23)</td>\n",
       "      <td>(street, 22.76)</td>\n",
       "      <td>(ldta, 18.42)</td>\n",
       "      <td>(adr, 18.41)</td>\n",
       "      <td>(rnn, 17.93)</td>\n",
       "      <td>(abbreviation, 15.75)</td>\n",
       "      <td>(adverse, 15.54)</td>\n",
       "      <td>(adapted, 15.29)</td>\n",
       "      <td>(lsm, 13.09)</td>\n",
       "      <td>(eligibility, 13.06)</td>\n",
       "      <td>(india, 12.3)</td>\n",
       "      <td>(attraction, 11.6)</td>\n",
       "      <td>(salience, 11.02)</td>\n",
       "      <td>(wall, 10.98)</td>\n",
       "      <td>(grams, 10.91)</td>\n",
       "      <td>(journal, 10.27)</td>\n",
       "      <td>(dntm, 10.12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 9</th>\n",
       "      <td>(topic, 893.39)</td>\n",
       "      <td>(research, 662.45)</td>\n",
       "      <td>(study, 393.56)</td>\n",
       "      <td>(analysis, 331.47)</td>\n",
       "      <td>(use, 254.09)</td>\n",
       "      <td>(article, 239.38)</td>\n",
       "      <td>(identify, 225.84)</td>\n",
       "      <td>(technology, 215.99)</td>\n",
       "      <td>(trend, 201.75)</td>\n",
       "      <td>(literature, 154.3)</td>\n",
       "      <td>(field, 150.61)</td>\n",
       "      <td>(text, 137.1)</td>\n",
       "      <td>(papers, 135.72)</td>\n",
       "      <td>(latent, 131.54)</td>\n",
       "      <td>(patent, 131.04)</td>\n",
       "      <td>(researcher, 130.56)</td>\n",
       "      <td>(scientific, 129.8)</td>\n",
       "      <td>(area, 125.64)</td>\n",
       "      <td>(dirichlet, 123.53)</td>\n",
       "      <td>(allocation, 122.98)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 10</th>\n",
       "      <td>(image, 923.95)</td>\n",
       "      <td>(feature, 404.79)</td>\n",
       "      <td>(object, 384.84)</td>\n",
       "      <td>(video, 298.9)</td>\n",
       "      <td>(visual, 296.7)</td>\n",
       "      <td>(scene, 291.52)</td>\n",
       "      <td>(propose, 268.69)</td>\n",
       "      <td>(method, 208.2)</td>\n",
       "      <td>(level, 190.16)</td>\n",
       "      <td>(model, 189.69)</td>\n",
       "      <td>(spatial, 171.52)</td>\n",
       "      <td>(region, 157.06)</td>\n",
       "      <td>(annotation, 151.42)</td>\n",
       "      <td>(classification, 148.49)</td>\n",
       "      <td>(human, 143.97)</td>\n",
       "      <td>(recognition, 138.61)</td>\n",
       "      <td>(result, 128.93)</td>\n",
       "      <td>(framework, 127.79)</td>\n",
       "      <td>(segmentation, 122.98)</td>\n",
       "      <td>(motion, 117.66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 11</th>\n",
       "      <td>(user, 1748.72)</td>\n",
       "      <td>(use, 1591.63)</td>\n",
       "      <td>(datum, 1381.07)</td>\n",
       "      <td>(topic, 1103.26)</td>\n",
       "      <td>(information, 1036.7)</td>\n",
       "      <td>(method, 971.08)</td>\n",
       "      <td>(latent, 941.61)</td>\n",
       "      <td>(propose, 858.59)</td>\n",
       "      <td>(allocation, 801.15)</td>\n",
       "      <td>(social, 798.08)</td>\n",
       "      <td>(approach, 753.49)</td>\n",
       "      <td>(dirichlet, 750.28)</td>\n",
       "      <td>(model, 704.64)</td>\n",
       "      <td>(result, 701.73)</td>\n",
       "      <td>(base, 692.76)</td>\n",
       "      <td>(network, 607.05)</td>\n",
       "      <td>(lda, 589.42)</td>\n",
       "      <td>(service, 569.67)</td>\n",
       "      <td>(paper, 561.59)</td>\n",
       "      <td>(time, 553.23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 12</th>\n",
       "      <td>(topic, 5025.38)</td>\n",
       "      <td>(model, 4893.76)</td>\n",
       "      <td>(lda, 2687.1)</td>\n",
       "      <td>(latent, 2152.96)</td>\n",
       "      <td>(document, 1992.37)</td>\n",
       "      <td>(use, 1943.68)</td>\n",
       "      <td>(method, 1839.26)</td>\n",
       "      <td>(word, 1773.7)</td>\n",
       "      <td>(dirichlet, 1564.05)</td>\n",
       "      <td>(propose, 1468.27)</td>\n",
       "      <td>(text, 1357.69)</td>\n",
       "      <td>(allocation, 1355.55)</td>\n",
       "      <td>(result, 1055.09)</td>\n",
       "      <td>(base, 997.56)</td>\n",
       "      <td>(approach, 918.45)</td>\n",
       "      <td>(paper, 905.81)</td>\n",
       "      <td>(algorithm, 897.52)</td>\n",
       "      <td>(datum, 891.62)</td>\n",
       "      <td>(feature, 814.18)</td>\n",
       "      <td>(information, 704.53)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 13</th>\n",
       "      <td>(hashtag, 94.97)</td>\n",
       "      <td>(membership, 54.98)</td>\n",
       "      <td>(chain, 51.44)</td>\n",
       "      <td>(routine, 49.23)</td>\n",
       "      <td>(functional, 38.29)</td>\n",
       "      <td>(carlo, 35.55)</td>\n",
       "      <td>(monte, 35.55)</td>\n",
       "      <td>(population, 30.26)</td>\n",
       "      <td>(incident, 26.4)</td>\n",
       "      <td>(trait, 24.96)</td>\n",
       "      <td>(signature, 23.26)</td>\n",
       "      <td>(poi, 23.24)</td>\n",
       "      <td>(ecosystem, 22.99)</td>\n",
       "      <td>(personality, 22.4)</td>\n",
       "      <td>(tissue, 21.34)</td>\n",
       "      <td>(hyperparameter, 21.08)</td>\n",
       "      <td>(composition, 20.08)</td>\n",
       "      <td>(zone, 19.08)</td>\n",
       "      <td>(site, 18.45)</td>\n",
       "      <td>(type, 17.93)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Word 0               Word 1               Word 2  \\\n",
       "Topic 0        (gene, 123.1)        (drug, 96.87)  (expression, 78.19)   \n",
       "Topic 1    (patient, 223.06)    (medical, 143.69)      (health, 143.5)   \n",
       "Topic 2       (mirna, 42.64)  (permission, 25.17)       (quran, 13.82)   \n",
       "Topic 3    (emotion, 181.73)       (drive, 87.04)       (style, 86.19)   \n",
       "Topic 4     (sampling, 79.5)    (parallel, 73.15)       (gibbs, 67.68)   \n",
       "Topic 5     (review, 537.88)  (sentiment, 474.31)    (opinion, 265.32)   \n",
       "Topic 6        (song, 81.12)       (audio, 74.02)       (music, 57.13)   \n",
       "Topic 7   (software, 268.18)   (question, 203.92)        (code, 149.7)   \n",
       "Topic 8          (lm, 75.09)       (frame, 55.04)     (library, 31.56)   \n",
       "Topic 9      (topic, 893.39)   (research, 662.45)      (study, 393.56)   \n",
       "Topic 10     (image, 923.95)    (feature, 404.79)     (object, 384.84)   \n",
       "Topic 11     (user, 1748.72)       (use, 1591.63)     (datum, 1381.07)   \n",
       "Topic 12    (topic, 5025.38)     (model, 4893.76)        (lda, 2687.1)   \n",
       "Topic 13    (hashtag, 94.97)  (membership, 54.98)       (chain, 51.44)   \n",
       "\n",
       "                       Word 3                 Word 4                Word 5  \\\n",
       "Topic 0      (disease, 77.49)       (protein, 75.45)   (biomedical, 64.39)   \n",
       "Topic 1    (clinical, 139.72)     (treatment, 99.44)    (diagnosis, 86.78)   \n",
       "Topic 2          (holy, 9.76)        (aircraft, 8.0)    (provenance, 7.74)   \n",
       "Topic 3       (speech, 55.34)         (genre, 39.85)  (recognition, 38.09)   \n",
       "Topic 4     (security, 57.19)    (distribute, 52.59)     (collapse, 42.46)   \n",
       "Topic 5       (tweet, 235.55)      (product, 228.45)        (post, 206.48)   \n",
       "Topic 6           (pn, 35.01)     (broadcast, 28.77)          (oov, 24.22)   \n",
       "Topic 7   (developer, 143.61)       (source, 143.52)         (bug, 139.43)   \n",
       "Topic 8          (asr, 28.23)        (street, 22.76)         (ldta, 18.42)   \n",
       "Topic 9    (analysis, 331.47)          (use, 254.09)     (article, 239.38)   \n",
       "Topic 10       (video, 298.9)        (visual, 296.7)       (scene, 291.52)   \n",
       "Topic 11     (topic, 1103.26)  (information, 1036.7)      (method, 971.08)   \n",
       "Topic 12    (latent, 2152.96)    (document, 1992.37)        (use, 1943.68)   \n",
       "Topic 13     (routine, 49.23)    (functional, 38.29)        (carlo, 35.55)   \n",
       "\n",
       "                       Word 6                Word 7                 Word 8  \\\n",
       "Topic 0      (pathway, 51.34)         (cell, 51.19)      (identify, 48.51)   \n",
       "Topic 1      (disease, 81.07)          (risk, 79.0)        (record, 78.67)   \n",
       "Topic 2            (lv, 7.74)       (laplace, 7.05)            (ast, 6.96)   \n",
       "Topic 3      (speaker, 34.84)        (state, 33.33)         (write, 31.62)   \n",
       "Topic 4      (cluster, 33.26)           (bot, 32.4)          (brand, 31.0)   \n",
       "Topic 5      (online, 196.94)        (topic, 194.3)     (analysis, 180.66)   \n",
       "Topic 6   (indonesian, 19.81)      (musical, 18.49)         (lyric, 17.49)   \n",
       "Topic 7      (report, 120.26)      (project, 87.55)       (program, 68.98)   \n",
       "Topic 8          (adr, 18.41)          (rnn, 17.93)  (abbreviation, 15.75)   \n",
       "Topic 9    (identify, 225.84)  (technology, 215.99)        (trend, 201.75)   \n",
       "Topic 10    (propose, 268.69)       (method, 208.2)        (level, 190.16)   \n",
       "Topic 11     (latent, 941.61)     (propose, 858.59)   (allocation, 801.15)   \n",
       "Topic 12    (method, 1839.26)        (word, 1773.7)   (dirichlet, 1564.05)   \n",
       "Topic 13       (monte, 35.55)   (population, 30.26)       (incident, 26.4)   \n",
       "\n",
       "                       Word 9              Word 10                Word 11  \\\n",
       "Topic 0   (biological, 47.03)         (btm, 40.99)            (ad, 40.76)   \n",
       "Topic 1         (care, 68.06)      (friend, 47.64)    (healthcare, 42.31)   \n",
       "Topic 2         (chara, 6.65)         (yuru, 6.65)         (ldaclm, 6.22)   \n",
       "Topic 3     (language, 30.16)     (acoustic, 30.1)       (driving, 29.08)   \n",
       "Topic 4        (large, 30.54)       (crime, 29.71)         (scale, 29.68)   \n",
       "Topic 5     (twitter, 174.52)      (media, 171.56)       (aspect, 166.01)   \n",
       "Topic 6     (estimate, 16.39)      (writer, 14.97)    (commonness, 14.53)   \n",
       "Topic 7       (answer, 65.81)  (repository, 53.22)   (requirement, 49.86)   \n",
       "Topic 8      (adverse, 15.54)     (adapted, 15.29)           (lsm, 13.09)   \n",
       "Topic 9   (literature, 154.3)      (field, 150.61)          (text, 137.1)   \n",
       "Topic 10      (model, 189.69)    (spatial, 171.52)       (region, 157.06)   \n",
       "Topic 11     (social, 798.08)   (approach, 753.49)    (dirichlet, 750.28)   \n",
       "Topic 12   (propose, 1468.27)      (text, 1357.69)  (allocation, 1355.55)   \n",
       "Topic 13       (trait, 24.96)   (signature, 23.26)           (poi, 23.24)   \n",
       "\n",
       "                       Word 12                   Word 13  \\\n",
       "Topic 0         (study, 39.19)            (apply, 37.74)   \n",
       "Topic 1        (cancer, 39.36)             (code, 36.28)   \n",
       "Topic 2      (privilege, 5.86)              (mmrm, 5.86)   \n",
       "Topic 3        (driver, 27.25)            (essay, 26.57)   \n",
       "Topic 4         (token, 29.34)    (approximation, 28.55)   \n",
       "Topic 5     (customer, 159.65)          (rating, 142.65)   \n",
       "Topic 6        (submit, 13.59)    (hyperspectral, 11.69)   \n",
       "Topic 7      (evolution, 49.8)        (technique, 48.97)   \n",
       "Topic 8   (eligibility, 13.06)             (india, 12.3)   \n",
       "Topic 9       (papers, 135.72)          (latent, 131.54)   \n",
       "Topic 10  (annotation, 151.42)  (classification, 148.49)   \n",
       "Topic 11       (model, 704.64)          (result, 701.73)   \n",
       "Topic 12     (result, 1055.09)            (base, 997.56)   \n",
       "Topic 13    (ecosystem, 22.99)       (personality, 22.4)   \n",
       "\n",
       "                         Word 14                  Word 15  \\\n",
       "Topic 0   (manufacturing, 31.56)            (mrna, 29.42)   \n",
       "Topic 1            (data, 32.74)      (conclusion, 30.77)   \n",
       "Topic 2          (iexpand, 5.13)        (configure, 5.01)   \n",
       "Topic 3           (sleep, 24.96)         (vehicle, 24.49)   \n",
       "Topic 4          (memory, 27.44)             (gpu, 27.32)   \n",
       "Topic 5       (consumer, 111.75)          (social, 101.7)   \n",
       "Topic 6             (eem, 10.71)              (mp, 10.39)   \n",
       "Topic 7            (open, 44.34)           (study, 43.24)   \n",
       "Topic 8       (attraction, 11.6)        (salience, 11.02)   \n",
       "Topic 9         (patent, 131.04)     (researcher, 130.56)   \n",
       "Topic 10         (human, 143.97)    (recognition, 138.61)   \n",
       "Topic 11          (base, 692.76)        (network, 607.05)   \n",
       "Topic 12      (approach, 918.45)          (paper, 905.81)   \n",
       "Topic 13         (tissue, 21.34)  (hyperparameter, 21.08)   \n",
       "\n",
       "                         Word 16              Word 17                 Word 18  \\\n",
       "Topic 0    (relationship, 29.31)  (conclusion, 28.59)           (tool, 26.52)   \n",
       "Topic 1       (physician, 29.99)         (emr, 25.48)            (ehr, 25.13)   \n",
       "Topic 2   (transcriptional, 5.0)         (pcfg, 4.14)            (tctm, 4.12)   \n",
       "Topic 3      (authorship, 22.67)        (noca, 22.09)         (author, 21.59)   \n",
       "Topic 4         (sampler, 26.77)      (matrix, 23.32)         (atomic, 23.12)   \n",
       "Topic 5            (use, 101.33)    (negative, 88.63)       (positive, 85.19)   \n",
       "Topic 6          (isoform, 9.79)     (retrieve, 9.73)       (replicate, 9.62)   \n",
       "Topic 7             (lsi, 40.78)       (stack, 33.35)         (change, 30.56)   \n",
       "Topic 8            (wall, 10.98)       (grams, 10.91)        (journal, 10.27)   \n",
       "Topic 9      (scientific, 129.8)       (area, 125.64)     (dirichlet, 123.53)   \n",
       "Topic 10        (result, 128.93)  (framework, 127.79)  (segmentation, 122.98)   \n",
       "Topic 11           (lda, 589.42)    (service, 569.67)         (paper, 561.59)   \n",
       "Topic 12     (algorithm, 897.52)      (datum, 891.62)       (feature, 814.18)   \n",
       "Topic 13    (composition, 20.08)        (zone, 19.08)           (site, 18.45)   \n",
       "\n",
       "                        Word 19  \n",
       "Topic 0        (related, 25.93)  \n",
       "Topic 1         (doctor, 24.57)  \n",
       "Topic 2          (seqlda, 4.12)  \n",
       "Topic 3       (disorder, 21.36)  \n",
       "Topic 4             (mh, 22.88)  \n",
       "Topic 5          (study, 81.34)  \n",
       "Topic 6          (timbre, 8.87)  \n",
       "Topic 7            (nfr, 30.05)  \n",
       "Topic 8           (dntm, 10.12)  \n",
       "Topic 9    (allocation, 122.98)  \n",
       "Topic 10       (motion, 117.66)  \n",
       "Topic 11         (time, 553.23)  \n",
       "Topic 12  (information, 704.53)  \n",
       "Topic 13          (type, 17.93)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"top terms info:\")\n",
    "df_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top docs info:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>(198, 0.548)</td>\n",
       "      <td>(263, 0.493)</td>\n",
       "      <td>(107, 0.407)</td>\n",
       "      <td>(1764, 0.386)</td>\n",
       "      <td>(2009, 0.383)</td>\n",
       "      <td>(1770, 0.371)</td>\n",
       "      <td>(1740, 0.361)</td>\n",
       "      <td>(894, 0.346)</td>\n",
       "      <td>(896, 0.334)</td>\n",
       "      <td>(65, 0.32)</td>\n",
       "      <td>(751, 0.318)</td>\n",
       "      <td>(725, 0.318)</td>\n",
       "      <td>(122, 0.314)</td>\n",
       "      <td>(52, 0.285)</td>\n",
       "      <td>(2577, 0.271)</td>\n",
       "      <td>(562, 0.264)</td>\n",
       "      <td>(526, 0.263)</td>\n",
       "      <td>(50, 0.257)</td>\n",
       "      <td>(685, 0.247)</td>\n",
       "      <td>(2248, 0.245)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>(489, 0.751)</td>\n",
       "      <td>(142, 0.651)</td>\n",
       "      <td>(115, 0.587)</td>\n",
       "      <td>(722, 0.42)</td>\n",
       "      <td>(1566, 0.37)</td>\n",
       "      <td>(1101, 0.364)</td>\n",
       "      <td>(1112, 0.346)</td>\n",
       "      <td>(80, 0.346)</td>\n",
       "      <td>(32, 0.345)</td>\n",
       "      <td>(627, 0.339)</td>\n",
       "      <td>(415, 0.319)</td>\n",
       "      <td>(2304, 0.319)</td>\n",
       "      <td>(1037, 0.313)</td>\n",
       "      <td>(1446, 0.291)</td>\n",
       "      <td>(1402, 0.291)</td>\n",
       "      <td>(1129, 0.281)</td>\n",
       "      <td>(174, 0.277)</td>\n",
       "      <td>(64, 0.276)</td>\n",
       "      <td>(338, 0.275)</td>\n",
       "      <td>(1624, 0.266)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>(1855, 0.23)</td>\n",
       "      <td>(1319, 0.208)</td>\n",
       "      <td>(1400, 0.199)</td>\n",
       "      <td>(872, 0.198)</td>\n",
       "      <td>(1309, 0.196)</td>\n",
       "      <td>(1079, 0.187)</td>\n",
       "      <td>(1340, 0.151)</td>\n",
       "      <td>(1310, 0.146)</td>\n",
       "      <td>(1164, 0.133)</td>\n",
       "      <td>(1350, 0.131)</td>\n",
       "      <td>(2158, 0.115)</td>\n",
       "      <td>(2386, 0.114)</td>\n",
       "      <td>(2188, 0.112)</td>\n",
       "      <td>(1242, 0.11)</td>\n",
       "      <td>(2220, 0.107)</td>\n",
       "      <td>(1868, 0.105)</td>\n",
       "      <td>(1932, 0.104)</td>\n",
       "      <td>(2422, 0.102)</td>\n",
       "      <td>(2457, 0.075)</td>\n",
       "      <td>(1876, 0.074)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>(81, 0.685)</td>\n",
       "      <td>(874, 0.517)</td>\n",
       "      <td>(109, 0.374)</td>\n",
       "      <td>(771, 0.349)</td>\n",
       "      <td>(18, 0.338)</td>\n",
       "      <td>(195, 0.293)</td>\n",
       "      <td>(1178, 0.287)</td>\n",
       "      <td>(344, 0.285)</td>\n",
       "      <td>(1281, 0.265)</td>\n",
       "      <td>(1813, 0.255)</td>\n",
       "      <td>(171, 0.254)</td>\n",
       "      <td>(1422, 0.254)</td>\n",
       "      <td>(1761, 0.244)</td>\n",
       "      <td>(1306, 0.24)</td>\n",
       "      <td>(1266, 0.238)</td>\n",
       "      <td>(737, 0.233)</td>\n",
       "      <td>(1278, 0.228)</td>\n",
       "      <td>(113, 0.222)</td>\n",
       "      <td>(1175, 0.22)</td>\n",
       "      <td>(365, 0.216)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>(508, 0.371)</td>\n",
       "      <td>(461, 0.346)</td>\n",
       "      <td>(413, 0.333)</td>\n",
       "      <td>(98, 0.282)</td>\n",
       "      <td>(1076, 0.27)</td>\n",
       "      <td>(720, 0.268)</td>\n",
       "      <td>(740, 0.268)</td>\n",
       "      <td>(741, 0.268)</td>\n",
       "      <td>(1666, 0.252)</td>\n",
       "      <td>(606, 0.248)</td>\n",
       "      <td>(693, 0.247)</td>\n",
       "      <td>(2027, 0.247)</td>\n",
       "      <td>(1379, 0.246)</td>\n",
       "      <td>(82, 0.241)</td>\n",
       "      <td>(2447, 0.231)</td>\n",
       "      <td>(886, 0.226)</td>\n",
       "      <td>(89, 0.225)</td>\n",
       "      <td>(1273, 0.217)</td>\n",
       "      <td>(450, 0.215)</td>\n",
       "      <td>(56, 0.206)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 5</th>\n",
       "      <td>(85, 0.951)</td>\n",
       "      <td>(57, 0.727)</td>\n",
       "      <td>(386, 0.669)</td>\n",
       "      <td>(111, 0.651)</td>\n",
       "      <td>(1, 0.631)</td>\n",
       "      <td>(575, 0.57)</td>\n",
       "      <td>(434, 0.518)</td>\n",
       "      <td>(698, 0.513)</td>\n",
       "      <td>(1577, 0.493)</td>\n",
       "      <td>(786, 0.486)</td>\n",
       "      <td>(103, 0.48)</td>\n",
       "      <td>(59, 0.471)</td>\n",
       "      <td>(6, 0.466)</td>\n",
       "      <td>(270, 0.455)</td>\n",
       "      <td>(957, 0.454)</td>\n",
       "      <td>(2272, 0.452)</td>\n",
       "      <td>(67, 0.452)</td>\n",
       "      <td>(973, 0.451)</td>\n",
       "      <td>(410, 0.435)</td>\n",
       "      <td>(1936, 0.42)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 6</th>\n",
       "      <td>(1570, 0.465)</td>\n",
       "      <td>(1117, 0.415)</td>\n",
       "      <td>(1606, 0.337)</td>\n",
       "      <td>(477, 0.302)</td>\n",
       "      <td>(757, 0.276)</td>\n",
       "      <td>(970, 0.254)</td>\n",
       "      <td>(1874, 0.252)</td>\n",
       "      <td>(1167, 0.169)</td>\n",
       "      <td>(1520, 0.166)</td>\n",
       "      <td>(684, 0.162)</td>\n",
       "      <td>(1668, 0.15)</td>\n",
       "      <td>(854, 0.147)</td>\n",
       "      <td>(2526, 0.144)</td>\n",
       "      <td>(702, 0.141)</td>\n",
       "      <td>(581, 0.141)</td>\n",
       "      <td>(1021, 0.131)</td>\n",
       "      <td>(2325, 0.126)</td>\n",
       "      <td>(2396, 0.125)</td>\n",
       "      <td>(1720, 0.124)</td>\n",
       "      <td>(995, 0.12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 7</th>\n",
       "      <td>(1586, 0.575)</td>\n",
       "      <td>(747, 0.526)</td>\n",
       "      <td>(224, 0.503)</td>\n",
       "      <td>(831, 0.447)</td>\n",
       "      <td>(453, 0.428)</td>\n",
       "      <td>(898, 0.427)</td>\n",
       "      <td>(20, 0.397)</td>\n",
       "      <td>(1594, 0.396)</td>\n",
       "      <td>(731, 0.392)</td>\n",
       "      <td>(2283, 0.386)</td>\n",
       "      <td>(380, 0.385)</td>\n",
       "      <td>(1473, 0.382)</td>\n",
       "      <td>(891, 0.379)</td>\n",
       "      <td>(1181, 0.377)</td>\n",
       "      <td>(2312, 0.376)</td>\n",
       "      <td>(2533, 0.375)</td>\n",
       "      <td>(945, 0.371)</td>\n",
       "      <td>(2467, 0.368)</td>\n",
       "      <td>(752, 0.357)</td>\n",
       "      <td>(141, 0.357)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 8</th>\n",
       "      <td>(441, 0.215)</td>\n",
       "      <td>(1958, 0.173)</td>\n",
       "      <td>(890, 0.172)</td>\n",
       "      <td>(626, 0.172)</td>\n",
       "      <td>(2121, 0.167)</td>\n",
       "      <td>(2184, 0.165)</td>\n",
       "      <td>(70, 0.161)</td>\n",
       "      <td>(802, 0.16)</td>\n",
       "      <td>(1442, 0.157)</td>\n",
       "      <td>(273, 0.152)</td>\n",
       "      <td>(2561, 0.148)</td>\n",
       "      <td>(1343, 0.147)</td>\n",
       "      <td>(1440, 0.137)</td>\n",
       "      <td>(576, 0.136)</td>\n",
       "      <td>(2505, 0.129)</td>\n",
       "      <td>(2233, 0.125)</td>\n",
       "      <td>(1882, 0.123)</td>\n",
       "      <td>(849, 0.114)</td>\n",
       "      <td>(2522, 0.113)</td>\n",
       "      <td>(621, 0.108)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 9</th>\n",
       "      <td>(2, 0.994)</td>\n",
       "      <td>(41, 0.9)</td>\n",
       "      <td>(1044, 0.871)</td>\n",
       "      <td>(317, 0.861)</td>\n",
       "      <td>(1013, 0.836)</td>\n",
       "      <td>(97, 0.813)</td>\n",
       "      <td>(996, 0.8)</td>\n",
       "      <td>(47, 0.798)</td>\n",
       "      <td>(1475, 0.772)</td>\n",
       "      <td>(335, 0.772)</td>\n",
       "      <td>(1305, 0.772)</td>\n",
       "      <td>(388, 0.765)</td>\n",
       "      <td>(308, 0.755)</td>\n",
       "      <td>(120, 0.752)</td>\n",
       "      <td>(546, 0.741)</td>\n",
       "      <td>(51, 0.737)</td>\n",
       "      <td>(8, 0.727)</td>\n",
       "      <td>(617, 0.724)</td>\n",
       "      <td>(264, 0.716)</td>\n",
       "      <td>(564, 0.712)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 10</th>\n",
       "      <td>(745, 0.69)</td>\n",
       "      <td>(399, 0.664)</td>\n",
       "      <td>(291, 0.662)</td>\n",
       "      <td>(2580, 0.654)</td>\n",
       "      <td>(2520, 0.652)</td>\n",
       "      <td>(1111, 0.645)</td>\n",
       "      <td>(1224, 0.645)</td>\n",
       "      <td>(284, 0.642)</td>\n",
       "      <td>(760, 0.638)</td>\n",
       "      <td>(695, 0.633)</td>\n",
       "      <td>(1694, 0.626)</td>\n",
       "      <td>(1410, 0.624)</td>\n",
       "      <td>(1222, 0.621)</td>\n",
       "      <td>(1900, 0.618)</td>\n",
       "      <td>(374, 0.612)</td>\n",
       "      <td>(2339, 0.605)</td>\n",
       "      <td>(662, 0.604)</td>\n",
       "      <td>(2363, 0.598)</td>\n",
       "      <td>(1550, 0.589)</td>\n",
       "      <td>(538, 0.582)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 11</th>\n",
       "      <td>(14, 0.994)</td>\n",
       "      <td>(149, 0.993)</td>\n",
       "      <td>(529, 0.993)</td>\n",
       "      <td>(619, 0.992)</td>\n",
       "      <td>(392, 0.991)</td>\n",
       "      <td>(2018, 0.989)</td>\n",
       "      <td>(1526, 0.989)</td>\n",
       "      <td>(1997, 0.988)</td>\n",
       "      <td>(162, 0.988)</td>\n",
       "      <td>(1549, 0.987)</td>\n",
       "      <td>(1180, 0.987)</td>\n",
       "      <td>(2326, 0.987)</td>\n",
       "      <td>(974, 0.987)</td>\n",
       "      <td>(2503, 0.986)</td>\n",
       "      <td>(206, 0.985)</td>\n",
       "      <td>(589, 0.985)</td>\n",
       "      <td>(485, 0.983)</td>\n",
       "      <td>(1741, 0.982)</td>\n",
       "      <td>(1862, 0.982)</td>\n",
       "      <td>(1703, 0.98)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 12</th>\n",
       "      <td>(966, 0.992)</td>\n",
       "      <td>(739, 0.991)</td>\n",
       "      <td>(1033, 0.991)</td>\n",
       "      <td>(1075, 0.991)</td>\n",
       "      <td>(2306, 0.991)</td>\n",
       "      <td>(354, 0.991)</td>\n",
       "      <td>(211, 0.991)</td>\n",
       "      <td>(870, 0.991)</td>\n",
       "      <td>(1071, 0.991)</td>\n",
       "      <td>(2168, 0.991)</td>\n",
       "      <td>(1408, 0.99)</td>\n",
       "      <td>(173, 0.99)</td>\n",
       "      <td>(1788, 0.99)</td>\n",
       "      <td>(1704, 0.99)</td>\n",
       "      <td>(1828, 0.99)</td>\n",
       "      <td>(2543, 0.99)</td>\n",
       "      <td>(422, 0.99)</td>\n",
       "      <td>(824, 0.99)</td>\n",
       "      <td>(390, 0.989)</td>\n",
       "      <td>(2477, 0.989)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 13</th>\n",
       "      <td>(53, 0.618)</td>\n",
       "      <td>(377, 0.343)</td>\n",
       "      <td>(158, 0.312)</td>\n",
       "      <td>(1762, 0.288)</td>\n",
       "      <td>(55, 0.285)</td>\n",
       "      <td>(356, 0.284)</td>\n",
       "      <td>(1744, 0.278)</td>\n",
       "      <td>(1499, 0.255)</td>\n",
       "      <td>(288, 0.225)</td>\n",
       "      <td>(342, 0.222)</td>\n",
       "      <td>(665, 0.221)</td>\n",
       "      <td>(75, 0.206)</td>\n",
       "      <td>(1045, 0.199)</td>\n",
       "      <td>(68, 0.194)</td>\n",
       "      <td>(1003, 0.189)</td>\n",
       "      <td>(777, 0.189)</td>\n",
       "      <td>(269, 0.186)</td>\n",
       "      <td>(1657, 0.164)</td>\n",
       "      <td>(601, 0.164)</td>\n",
       "      <td>(562, 0.162)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0              1              2              3  \\\n",
       "Topic 0    (198, 0.548)   (263, 0.493)   (107, 0.407)  (1764, 0.386)   \n",
       "Topic 1    (489, 0.751)   (142, 0.651)   (115, 0.587)    (722, 0.42)   \n",
       "Topic 2    (1855, 0.23)  (1319, 0.208)  (1400, 0.199)   (872, 0.198)   \n",
       "Topic 3     (81, 0.685)   (874, 0.517)   (109, 0.374)   (771, 0.349)   \n",
       "Topic 4    (508, 0.371)   (461, 0.346)   (413, 0.333)    (98, 0.282)   \n",
       "Topic 5     (85, 0.951)    (57, 0.727)   (386, 0.669)   (111, 0.651)   \n",
       "Topic 6   (1570, 0.465)  (1117, 0.415)  (1606, 0.337)   (477, 0.302)   \n",
       "Topic 7   (1586, 0.575)   (747, 0.526)   (224, 0.503)   (831, 0.447)   \n",
       "Topic 8    (441, 0.215)  (1958, 0.173)   (890, 0.172)   (626, 0.172)   \n",
       "Topic 9      (2, 0.994)      (41, 0.9)  (1044, 0.871)   (317, 0.861)   \n",
       "Topic 10    (745, 0.69)   (399, 0.664)   (291, 0.662)  (2580, 0.654)   \n",
       "Topic 11    (14, 0.994)   (149, 0.993)   (529, 0.993)   (619, 0.992)   \n",
       "Topic 12   (966, 0.992)   (739, 0.991)  (1033, 0.991)  (1075, 0.991)   \n",
       "Topic 13    (53, 0.618)   (377, 0.343)   (158, 0.312)  (1762, 0.288)   \n",
       "\n",
       "                      4              5              6              7  \\\n",
       "Topic 0   (2009, 0.383)  (1770, 0.371)  (1740, 0.361)   (894, 0.346)   \n",
       "Topic 1    (1566, 0.37)  (1101, 0.364)  (1112, 0.346)    (80, 0.346)   \n",
       "Topic 2   (1309, 0.196)  (1079, 0.187)  (1340, 0.151)  (1310, 0.146)   \n",
       "Topic 3     (18, 0.338)   (195, 0.293)  (1178, 0.287)   (344, 0.285)   \n",
       "Topic 4    (1076, 0.27)   (720, 0.268)   (740, 0.268)   (741, 0.268)   \n",
       "Topic 5      (1, 0.631)    (575, 0.57)   (434, 0.518)   (698, 0.513)   \n",
       "Topic 6    (757, 0.276)   (970, 0.254)  (1874, 0.252)  (1167, 0.169)   \n",
       "Topic 7    (453, 0.428)   (898, 0.427)    (20, 0.397)  (1594, 0.396)   \n",
       "Topic 8   (2121, 0.167)  (2184, 0.165)    (70, 0.161)    (802, 0.16)   \n",
       "Topic 9   (1013, 0.836)    (97, 0.813)     (996, 0.8)    (47, 0.798)   \n",
       "Topic 10  (2520, 0.652)  (1111, 0.645)  (1224, 0.645)   (284, 0.642)   \n",
       "Topic 11   (392, 0.991)  (2018, 0.989)  (1526, 0.989)  (1997, 0.988)   \n",
       "Topic 12  (2306, 0.991)   (354, 0.991)   (211, 0.991)   (870, 0.991)   \n",
       "Topic 13    (55, 0.285)   (356, 0.284)  (1744, 0.278)  (1499, 0.255)   \n",
       "\n",
       "                      8              9             10             11  \\\n",
       "Topic 0    (896, 0.334)     (65, 0.32)   (751, 0.318)   (725, 0.318)   \n",
       "Topic 1     (32, 0.345)   (627, 0.339)   (415, 0.319)  (2304, 0.319)   \n",
       "Topic 2   (1164, 0.133)  (1350, 0.131)  (2158, 0.115)  (2386, 0.114)   \n",
       "Topic 3   (1281, 0.265)  (1813, 0.255)   (171, 0.254)  (1422, 0.254)   \n",
       "Topic 4   (1666, 0.252)   (606, 0.248)   (693, 0.247)  (2027, 0.247)   \n",
       "Topic 5   (1577, 0.493)   (786, 0.486)    (103, 0.48)    (59, 0.471)   \n",
       "Topic 6   (1520, 0.166)   (684, 0.162)   (1668, 0.15)   (854, 0.147)   \n",
       "Topic 7    (731, 0.392)  (2283, 0.386)   (380, 0.385)  (1473, 0.382)   \n",
       "Topic 8   (1442, 0.157)   (273, 0.152)  (2561, 0.148)  (1343, 0.147)   \n",
       "Topic 9   (1475, 0.772)   (335, 0.772)  (1305, 0.772)   (388, 0.765)   \n",
       "Topic 10   (760, 0.638)   (695, 0.633)  (1694, 0.626)  (1410, 0.624)   \n",
       "Topic 11   (162, 0.988)  (1549, 0.987)  (1180, 0.987)  (2326, 0.987)   \n",
       "Topic 12  (1071, 0.991)  (2168, 0.991)   (1408, 0.99)    (173, 0.99)   \n",
       "Topic 13   (288, 0.225)   (342, 0.222)   (665, 0.221)    (75, 0.206)   \n",
       "\n",
       "                     12             13             14             15  \\\n",
       "Topic 0    (122, 0.314)    (52, 0.285)  (2577, 0.271)   (562, 0.264)   \n",
       "Topic 1   (1037, 0.313)  (1446, 0.291)  (1402, 0.291)  (1129, 0.281)   \n",
       "Topic 2   (2188, 0.112)   (1242, 0.11)  (2220, 0.107)  (1868, 0.105)   \n",
       "Topic 3   (1761, 0.244)   (1306, 0.24)  (1266, 0.238)   (737, 0.233)   \n",
       "Topic 4   (1379, 0.246)    (82, 0.241)  (2447, 0.231)   (886, 0.226)   \n",
       "Topic 5      (6, 0.466)   (270, 0.455)   (957, 0.454)  (2272, 0.452)   \n",
       "Topic 6   (2526, 0.144)   (702, 0.141)   (581, 0.141)  (1021, 0.131)   \n",
       "Topic 7    (891, 0.379)  (1181, 0.377)  (2312, 0.376)  (2533, 0.375)   \n",
       "Topic 8   (1440, 0.137)   (576, 0.136)  (2505, 0.129)  (2233, 0.125)   \n",
       "Topic 9    (308, 0.755)   (120, 0.752)   (546, 0.741)    (51, 0.737)   \n",
       "Topic 10  (1222, 0.621)  (1900, 0.618)   (374, 0.612)  (2339, 0.605)   \n",
       "Topic 11   (974, 0.987)  (2503, 0.986)   (206, 0.985)   (589, 0.985)   \n",
       "Topic 12   (1788, 0.99)   (1704, 0.99)   (1828, 0.99)   (2543, 0.99)   \n",
       "Topic 13  (1045, 0.199)    (68, 0.194)  (1003, 0.189)   (777, 0.189)   \n",
       "\n",
       "                     16             17             18             19  \n",
       "Topic 0    (526, 0.263)    (50, 0.257)   (685, 0.247)  (2248, 0.245)  \n",
       "Topic 1    (174, 0.277)    (64, 0.276)   (338, 0.275)  (1624, 0.266)  \n",
       "Topic 2   (1932, 0.104)  (2422, 0.102)  (2457, 0.075)  (1876, 0.074)  \n",
       "Topic 3   (1278, 0.228)   (113, 0.222)   (1175, 0.22)   (365, 0.216)  \n",
       "Topic 4     (89, 0.225)  (1273, 0.217)   (450, 0.215)    (56, 0.206)  \n",
       "Topic 5     (67, 0.452)   (973, 0.451)   (410, 0.435)   (1936, 0.42)  \n",
       "Topic 6   (2325, 0.126)  (2396, 0.125)  (1720, 0.124)    (995, 0.12)  \n",
       "Topic 7    (945, 0.371)  (2467, 0.368)   (752, 0.357)   (141, 0.357)  \n",
       "Topic 8   (1882, 0.123)   (849, 0.114)  (2522, 0.113)   (621, 0.108)  \n",
       "Topic 9      (8, 0.727)   (617, 0.724)   (264, 0.716)   (564, 0.712)  \n",
       "Topic 10   (662, 0.604)  (2363, 0.598)  (1550, 0.589)   (538, 0.582)  \n",
       "Topic 11   (485, 0.983)  (1741, 0.982)  (1862, 0.982)   (1703, 0.98)  \n",
       "Topic 12    (422, 0.99)    (824, 0.99)   (390, 0.989)  (2477, 0.989)  \n",
       "Topic 13   (269, 0.186)  (1657, 0.164)   (601, 0.164)   (562, 0.162)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"top docs info:\")\n",
    "df_top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis.pyLDA(topic_word, doc_topic, [len(s) for s in [word_tokenize(corp) for corp in _input]], vec.get_feature_names(), np.array(sum(tf).todense())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['index', 'parse', 'basicDependencies', 'enhancedDependencies', 'enhancedPlusPlusDependencies', 'tokens'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parse[0][\"sentences\"][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### ('patient', 223.06) ####################\n",
      "3 12 489\n",
      "latent Dirichlet allocation ( lda ) may offer statistical rigor in summarize patient ' concern and cope strategy in a life-threatening illness .\n",
      "concerns - [nmod:poss] - patients\n",
      "patients - [case] - '\n",
      "4 20 489\n",
      "we aim to apply LDA to interview datum collect as part of a prospective , longitudinal study of qol in patient undergo radical cystectomy and urinary diversion for bladder cancer .\n",
      "patients - [case] - in\n",
      "QOL - [nmod:in] - patients\n",
      "patients - [acl] - undergoing\n",
      "5 8 489\n",
      "lda show that , prior to surgery , patient ' priority be primarily in cancer surgery and recovery .\n",
      "priorities - [nmod:poss] - patients\n",
      "patients - [case] - '\n",
      "2 16 115\n",
      "latent Dirichlet allocation ( lda ) offer statistical rigor and consistency in automate the interpretation of patient ' express concern and cope strategy .\n",
      "patients - [case] - of\n",
      "interpretation - [nmod:'] - patients\n",
      "patients - [case] - '\n",
      "patients - [dep] - concerns\n",
      "3 22 115\n",
      "method LDA be apply to interview datum collect as part of a prospective , longitudinal study of qol in n = 211 patient undergo radical cystectomy and urinary diversion for bladder cancer .\n",
      "patients - [nummod] - 211\n",
      "= - [dep] - patients\n",
      "patients - [acl] - undergoing\n",
      "4 20 115\n",
      "lda analyze personal goal statement to extract the latent topic and theme , stratify by time , and on thing patient want to accomplish and prevent .\n",
      "patients - [case] - on\n",
      "patients - [compound] - things\n",
      "analyzed - [dobj] - patients\n",
      "statements - [conj:and] - patients\n",
      "patients - [acl] - wanted\n",
      "7 4 115\n",
      "prior to surgery , patient ' priority be primarily in cancer surgery and recovery .\n",
      "priorities - [nmod:poss] - patients\n",
      "patients - [case] - '\n",
      "0 24 722\n",
      "the present study aim to investigate difference in prognosis base on human papillomavirus ( hpv ) infection , persistent infection and genotype variation for patient exhibit atypical squamous cell of undetermined significance ( ascus ) in they initial Papanicolaou ( PAP ) test result .\n",
      "patients - [case] - for\n",
      "infection - [nmod:for] - patients\n",
      "patients - [acl] - exhibiting\n",
      "2 5 722\n",
      "the present study assess 491 patient ( 139 hpv-positive and 352 hpv-negative case ) with a PAP test result of ascus with a follow-up period > = 2 year .\n",
      "patients - [nummod] - 491\n",
      "assessed - [dobj] - patients\n",
      "patients - [dep] - HPV-positive\n",
      "patients - [dep] - cases\n",
      "patients - [nmod:with] - result\n",
      "3 0 722\n",
      "patient undergo PAP and HPV DNA chip test between January 2006 and January 2009 .\n",
      "underwent - [nsubj] - Patients\n",
      "6 8 722\n",
      "a total of 33.3 % ( 12/36 ) patient positive for hpv-16 have cervical intraepithelial neoplasia ( cin ) 2 or a worse result , which be significantly higher than the prevalence of cin2 of 1.8 % ( 8/455 ) in patient negative for hpv-16 ( p < 0.001 ) , while no significant association be identify for other genotype in term of genotype and clinical progress .\n",
      "% - [dep] - patients\n",
      "patients - [amod] - positive\n",
      "6 41 722\n",
      "a total of 33.3 % ( 12/36 ) patient positive for hpv-16 have cervical intraepithelial neoplasia ( cin ) 2 or a worse result , which be significantly higher than the prevalence of cin2 of 1.8 % ( 8/455 ) in patient negative for hpv-16 ( p < 0.001 ) , while no significant association be identify for other genotype in term of genotype and clinical progress .\n",
      "patients - [case] - in\n",
      "prevalence - [nmod:in] - patients\n",
      "patients - [amod] - negative\n",
      "8 5 722\n",
      "persistent infection be higher in patient aged > = 51 year ( 38.7 % ) than in those aged < = 50 year ( 20.4 % ; p = 0.036 ) .\n",
      "patients - [case] - in\n",
      "higher - [nmod:in] - patients\n",
      "patients - [amod] - aged\n",
      "12 21 722\n",
      "therefore , lda result may be present as explanatory evidence during time-constrained patient-doctor consultation in order to deliver information regard the patient 's status .\n",
      "patient - [det] - the\n",
      "status - [nmod:poss] - patient\n",
      "patient - [case] - 's\n",
      "1 24 1566\n",
      "by examine the clinical observation ( such as diagnosis , risk factor , and medication ) mention in longitudinal emr , we can use patient ' medical chronology to automatically predict the progression of they pathology .\n",
      "chronologies - [nmod:poss] - patients\n",
      "patients - [case] - '\n",
      "2 19 1566\n",
      "in this paper , we present a novel probabilistic model which jointly learn how to ( 1 ) group patient base on the similarity between they clinical observation as well as how to ( 2 ) predict the way a new patient 's clinical observation might evolve in the future .\n",
      "patients - [compound] - group\n",
      "1 - [dobj] - patients\n",
      "patients - [acl] - based\n",
      "2 41 1566\n",
      "in this paper , we present a novel probabilistic model which jointly learn how to ( 1 ) group patient base on the similarity between they clinical observation as well as how to ( 2 ) predict the way a new patient 's clinical observation might evolve in the future .\n",
      "patient - [det] - a\n",
      "patient - [amod] - new\n",
      "observations - [nmod:poss] - patient\n",
      "patient - [case] - 's\n",
      "3 14 1566\n",
      "we show that we model can be use to not only track how a patient 's clinical finding might change over time , but to also identify which patient be due for preventative visit .\n",
      "patient - [det] - a\n",
      "findings - [nmod:poss] - patient\n",
      "patient - [case] - 's\n",
      "3 28 1566\n",
      "we show that we model can be use to not only track how a patient 's clinical finding might change over time , but to also identify which patient be due for preventative visit .\n",
      "due - [nsubj] - patients\n",
      "4 14 1566\n",
      "in addition , we model have the potential to improve the quality of over-all patient care in practice by predict the most likely set of clinical observation at a arbitrary point in the future .\n",
      "care - [compound] - patient\n",
      "1 11 1101\n",
      "a hospital emr dataset typically consist of medical record of hospitalize patient .\n",
      "patients - [case] - of\n",
      "patients - [amod] - hospitalized\n",
      "records - [nmod:of] - patients\n",
      "3 32 1101\n",
      "traditional topic model , such as latent dirichlet allocation ( lda ) and hierarchical dirichlet process ( hdp ) , can be employ to discover disease topic from emr datum by treat patient as document and diagnosis code as word .\n",
      "treating - [dobj] - patients\n",
      "4 9 1101\n",
      "this topic modeling help to understand the constitution of patient disease and offer a tool for better planning of treatment .\n",
      "diseases - [compound] - patient\n",
      "0 27 1112\n",
      "information and communication technology have enable healthcare institution to accumulate large amount of healthcare datum that include diagnosis , medication , and additional contextual information such as patient demographic .\n",
      "demographics - [compound] - patient\n",
      "9 36 80\n",
      "however , this study suggest that imperfect datum ( e.g. , icd code in combination with other ehr feature ) can serve as a silver standard to develop a risk model , apply that model to patient without dementia code , and then select a case-detection threshold .\n",
      "patients - [case] - to\n",
      "apply - [nmod:to] - patients\n",
      "patients - [nmod:without] - codes\n",
      "1 17 32\n",
      "predict diabetic complication be regard as a highly effective technique for increase the survival rate of diabetic patient .\n",
      "patients - [case] - of\n",
      "patients - [amod] - diabetic\n",
      "rate - [nmod:of] - patients\n",
      "2 10 415\n",
      "mkl method be use to divide parameter between heart disease patient and normal individual .\n",
      "patients - [case] - between\n",
      "patients - [compound] - heart\n",
      "patients - [compound] - disease\n",
      "parameters - [nmod:between] - patients\n",
      "patients - [cc] - and\n",
      "patients - [conj:and] - individuals\n",
      "3 20 415\n",
      "the result obtain from the mkl method be give to the ANFIS classifier to classify the heart disease and healthy patient .\n",
      "patients - [amod] - healthy\n",
      "classify - [dobj] - patients\n",
      "disease - [conj:and] - patients\n",
      "5 10 2304\n",
      "the result show that manifestation sub-category actually exist in t2dm patient that need specific , individualised cm therapy .\n",
      "patients - [case] - in\n",
      "patients - [compound] - T2DM\n",
      "exist - [nmod:in] - patients\n",
      "need - [nsubj] - patients\n",
      "patients - [ref] - that\n",
      "patients - [acl:relcl] - need\n",
      "5 2 1037\n",
      "of 4687 patient with inpatient discharge summary , 470 be readmit within 30 day .\n",
      "patients - [case] - Of\n",
      "patients - [nummod] - 4687\n",
      "readmitted - [nmod:of] - patients\n",
      "patients - [nmod:with] - summaries\n",
      "0 26 1446\n",
      "clinical pathway ( cp ) analysis play a important role in health-care management in ensure specialize , standardized , normalize and sophisticated therapy procedure for individual patient .\n",
      "patients - [case] - for\n",
      "patients - [amod] - individual\n",
      "procedures - [nmod:for] - patients\n",
      "3 11 1446\n",
      "more specifically , we develop a probabilistic topic model to link patient feature and treatment behavior together to mine treatment pattern hide in emr .\n",
      "features - [compound] - patient\n",
      "4 13 1446\n",
      "discover treatment pattern , as actionable knowledge represent the best practice for most patient in most time of they treatment process , form the backbone of cp , and can be exploit to help physician better understand they specialty and learn from previous experience for cp analysis and improvement .\n",
      "patients - [case] - for\n",
      "patients - [amod] - most\n",
      "practice - [nmod:for] - patients\n",
      "0 16 1402\n",
      "background and objective : risk stratification aim to provide physician with the accurate assessment of a patient 's clinical risk such that a individualized prevention or management strategy can be develop and deliver .\n",
      "patient - [det] - a\n",
      "risk - [nmod:poss] - patient\n",
      "patient - [case] - 's\n",
      "1 14 1402\n",
      "exist risk stratification technique mainly focus on predict the overall risk of a individual patient in a supervised manner , and , at the cohort level , often offer little insight beyond a flat score-based segmentation from the label clinical dataset .\n",
      "patient - [case] - of\n",
      "patient - [det] - an\n",
      "patient - [amod] - individual\n",
      "risk - [nmod:of] - patient\n",
      "patient - [nmod:in] - manner\n",
      "4 5 1402\n",
      "the propose PRSM recognize a patient clinical state as a probabilistic combination of latent sub-profile , and generate sub-profile-specific risk tier of patient from they ehr in a fully unsupervised fashion .\n",
      "state - [compound] - patient\n",
      "4 22 1402\n",
      "the propose PRSM recognize a patient clinical state as a probabilistic combination of latent sub-profile , and generate sub-profile-specific risk tier of patient from they ehr in a fully unsupervised fashion .\n",
      "patients - [case] - of\n",
      "tiers - [nmod:of] - patients\n",
      "patients - [nmod:from] - EHRs\n",
      "7 22 1402\n",
      "result : we verify the effectiveness of the propose approach on a clinical dataset contain 3463 coronary heart disease ( chd ) patient instance .\n",
      "instances - [compound] - patient\n",
      "12 2 1402\n",
      "moreover , patient sub-profile and sub-profile-specific risk tier generate by we model be coherent and informative , and provide significant potential to be explore for the further task , such as patient cohort analysis .\n",
      "sub-profiles - [compound] - patient\n",
      "12 31 1402\n",
      "moreover , patient sub-profile and sub-profile-specific risk tier generate by we model be coherent and informative , and provide significant potential to be explore for the further task , such as patient cohort analysis .\n",
      "analysis - [compound] - patient\n",
      "0 17 1129\n",
      "in healthcare organizational setting , the design of a clinical pathway ( cp ) be challenge since patient follow a particular pathway may have not only one single first-diagnosis but also several typical comorbidity , and thus it require different discipline involve to put together they partial knowledge about the overall pathway .\n",
      "patients - [case] - since\n",
      "challenging - [nmod:since] - patients\n",
      "patients - [nmod:following] - pathway\n",
      "7 15 1129\n",
      "we verify the effectiveness of the propose model on a real clinical dataset contain 12,120 patient trace , which pertain to the unstable angina cp .\n",
      "traces - [compound] - patient\n",
      "0 10 174\n",
      "objective few study have examine ankylose spondylitis ( as ) patient ' concern about and perception of biologic therapy , apart from traditional survey .\n",
      "concerns - [nmod:poss] - patients\n",
      "patients - [case] - '\n",
      "1 20 174\n",
      "in this study , we use social media data to examine the knowledge , attitude , and belief of as patient regard biologic therapy .\n",
      "patients - [case] - of\n",
      "patients - [amod] - AS\n",
      "beliefs - [nmod:of] - patients\n",
      "patients - [acl] - regarding\n",
      "11 2 174\n",
      "additional implicit patient need ( e.g. , support ) be identify use qualitative analysis .\n",
      "needs - [compound] - patient\n",
      "12 11 174\n",
      "conclusion Social media reveal a dynamic range of theme govern as patient ' experience with and choice of biologic agent .\n",
      "patients - [amod] - AS\n",
      "experience - [nmod:poss] - patients\n",
      "patients - [case] - '\n",
      "13 25 174\n",
      "the complexity of select biologic from among many such agent and navigate they risk/benefit profile suggest the merit of create online tool tailor to support patient ' decision-making with regard to biologic therapy for as .\n",
      "decision-making - [nmod:poss] - patients\n",
      "patients - [case] - '\n",
      "4 25 338\n",
      "objective : this study aim to develop a hierarchical topic taxonomy to uncover the latent structure of physician review and illustrate its application for mining patient ' interest base on the propose taxonomy and algorithm .\n",
      "patients - [compound] - mining\n",
      "interests - [nmod:poss] - patients\n",
      "patients - [case] - '\n",
      "9 8 338\n",
      "the patient-related domain include the category of the patient profile , symptom , diagnosis , and pathogenesis .\n",
      "profile - [compound] - patient\n",
      "12 26 338\n",
      "symptom ( Cohen d = 1.58 , Delta u = 0.216 , t = 229.75 , and p < .001 ) be more often mention by patient with acute disease , whereas communication skill ( Cohen d = -0.29 , Delta u = -0.038 , t = -42.01 , and p < .001 ) , financing ( Cohen d = -0.68 , Delta u = -0.098 , t = -99.26 , and p < .001 ) , and diagnosis and pathogenesis ( Cohen d = -0.55 , Delta u = -0.078 , t = -80.09 , and p < .001 ) be more often mention by patient with chronic disease .\n",
      "patients - [case] - by\n",
      "mentioned - [nmod:agent] - patients\n",
      "patients - [nmod:with] - diseases\n",
      "12 106 338\n",
      "symptom ( Cohen d = 1.58 , Delta u = 0.216 , t = 229.75 , and p < .001 ) be more often mention by patient with acute disease , whereas communication skill ( Cohen d = -0.29 , Delta u = -0.038 , t = -42.01 , and p < .001 ) , financing ( Cohen d = -0.68 , Delta u = -0.098 , t = -99.26 , and p < .001 ) , and diagnosis and pathogenesis ( Cohen d = -0.55 , Delta u = -0.078 , t = -80.09 , and p < .001 ) be more often mention by patient with chronic disease .\n",
      "patients - [case] - by\n",
      "mentioned - [nmod:agent] - patients\n",
      "patients - [nmod:with] - diseases\n",
      "13 0 338\n",
      "patient with mild disease be more interested in medical ethic ( Cohen d = 0.25 , Delta u 0.039 , t = 8.33 , and p < .001 ) , operation process ( Cohen d = 0.57 , Delta u 0.060 , t = 18.75 , and p < .001 ) , patient profile ( Cohen d = 1.19 , Delta u 0.132 , t = 39.33 , and p < .001 ) , and symptom ( Cohen d = 1.91 , Delta u = 0.274 , t = 62.82 , and p < .001 ) .\n",
      "interested - [nsubj] - Patients\n",
      "Patients - [nmod:with] - diseases\n",
      "13 52 338\n",
      "patient with mild disease be more interested in medical ethic ( Cohen d = 0.25 , Delta u 0.039 , t = 8.33 , and p < .001 ) , operation process ( Cohen d = 0.57 , Delta u 0.060 , t = 18.75 , and p < .001 ) , patient profile ( Cohen d = 1.19 , Delta u 0.132 , t = 39.33 , and p < .001 ) , and symptom ( Cohen d = 1.91 , Delta u = 0.274 , t = 62.82 , and p < .001 ) .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "profile - [compound] - patient\n",
      "14 2 338\n",
      "meanwhile , patient with serious disease be more interested in medical competence ( Cohen d = -0.99 , Delta u = -0.165 , t = -32.58 , and p < .001 ) , medical advice and prescription ( Cohen d = -0.65 , Delta u = -0.082 , t = -21.45 , and p < .001 ) , financing ( Cohen d = -0.26 , Delta u = -0.018 , t = -8.45 , and p < .001 ) , and diagnosis and pathogenesis ( Cohen d = -1.55 , Delta u = -0.229 , t = -50.93 , and p < .001 ) .\n",
      "interested - [nsubj] - patients\n",
      "patients - [nmod:with] - diseases\n",
      "16 15 338\n",
      "the propose algorithm base on labeled-latent Dirichlet allocation can achieve impressive classification result for mining patient ' interest .\n",
      "patients - [compound] - mining\n",
      "interests - [nmod:poss] - patients\n",
      "patients - [case] - '\n",
      "17 9 338\n",
      "furthermore , the mining result reveal marked difference in patient ' interest across different disease type , socioeconomic development level , and hospital level .\n",
      "interests - [nmod:poss] - patients\n",
      "patients - [case] - '\n",
      "1 1 1624\n",
      "these patient record contain valuable medical information include patient information , diagnosis , treatment method , and eventual patient outcome .\n",
      "records - [compound] - patient\n",
      "1 8 1624\n",
      "these patient record contain valuable medical information include patient information , diagnosis , treatment method , and eventual patient outcome .\n",
      "information - [compound] - patient\n",
      "1 18 1624\n",
      "these patient record contain valuable medical information include patient information , diagnosis , treatment method , and eventual patient outcome .\n",
      "outcomes - [compound] - patient\n",
      "3 16 1624\n",
      "in this paper , we present a method for automatically discover underlie theme and pattern within patient datum .\n",
      "data - [compound] - patient\n",
      "#################### ('medical', 143.69) ####################\n",
      "0 10 1566\n",
      "the expand clinical information provide by the advent of electronic medical record offer a exciting opportunity to substantially improve the quality of health care .\n",
      "records - [amod] - medical\n",
      "1 26 1566\n",
      "by examine the clinical observation ( such as diagnosis , risk factor , and medication ) mention in longitudinal emr , we can use patient ' medical chronology to automatically predict the progression of they pathology .\n",
      "chronologies - [amod] - medical\n",
      "1 7 1101\n",
      "a hospital emr dataset typically consist of medical record of hospitalize patient .\n",
      "records - [amod] - medical\n",
      "2 1 1101\n",
      "a medical record contain diagnostic information ( diagnosis code ) , procedure perform ( procedure code ) and admission detail .\n",
      "record - [amod] - medical\n",
      "11 8 1101\n",
      "we evaluate the propose model on two real-world medical dataset - polyvascular disease and acute myocardial infarction disease .\n",
      "datasets - [amod] - medical\n",
      "2 5 32\n",
      "while many study currently use medical image and structured medical record , very limited effort have be dedicate to apply datum mining technique for unstructured textual medical record , such as admission and discharge record .\n",
      "images - [amod] - medical\n",
      "2 9 32\n",
      "while many study currently use medical image and structured medical record , very limited effort have be dedicate to apply datum mining technique for unstructured textual medical record , such as admission and discharge record .\n",
      "records - [amod] - medical\n",
      "2 26 32\n",
      "while many study currently use medical image and structured medical record , very limited effort have be dedicate to apply datum mining technique for unstructured textual medical record , such as admission and discharge record .\n",
      "records - [amod] - medical\n",
      "3 5 32\n",
      "moreover , the similarity among medical record that be overlook by exist approach could potentially improve the accuracy of prediction model .\n",
      "records - [amod] - medical\n",
      "5 9 32\n",
      "specifically , we first estimate the similarity between textual medical record after datum preprocessing , and then we perform selda-based diabetic complication topic mining base on similarity constraint .\n",
      "records - [amod] - medical\n",
      "0 2 627\n",
      "obstetric electronic medical record ( emr ) contain massive amount of medical datum and health information .\n",
      "records - [amod] - medical\n",
      "0 11 627\n",
      "obstetric electronic medical record ( emr ) contain massive amount of medical datum and health information .\n",
      "data - [amod] - medical\n",
      "6 15 627\n",
      "the result of the diagnosis assistant can be introduce as a supplementary learning method for medical student .\n",
      "students - [amod] - medical\n",
      "7 16 627\n",
      "additionally , the method can be use not only for obstetric emr but also for other medical record .\n",
      "records - [amod] - medical\n",
      "4 19 2304\n",
      "we obtain meaningful diagnosis and treatment topic ( cluster ) from the datum , which clinically indicate some important medical group correspond to comorbidity disease ( e.g. , heart disease and diabetic kidney disease in t2dm inpatient ) .\n",
      "groups - [amod] - medical\n",
      "1 16 1446\n",
      "recently , with the rapid development of hospital information system , a large volume of electronic medical record ( emr ) have be produce , which provide a comprehensive source for cp analysis .\n",
      "records - [amod] - medical\n",
      "9 5 1129\n",
      "in addition , a possible medical application in term of treatment recommendation be provide to illustrate the potential of the propose model .\n",
      "application - [amod] - medical\n",
      "9 12 174\n",
      "other theme , include the psychological impact of as , reporting of medical literature , and as disease consequence , account for the remain 40 % ( n = 45 ) .\n",
      "literature - [amod] - medical\n",
      "8 7 338\n",
      "the physician-related domain include the category of medical ethic , medical competence , communication skill , medical advice , and prescription .\n",
      "ethics - [amod] - medical\n",
      "8 10 338\n",
      "the physician-related domain include the category of medical ethic , medical competence , communication skill , medical advice , and prescription .\n",
      "competence - [amod] - medical\n",
      "8 16 338\n",
      "the physician-related domain include the category of medical ethic , medical competence , communication skill , medical advice , and prescription .\n",
      "advice - [amod] - medical\n",
      "13 8 338\n",
      "patient with mild disease be more interested in medical ethic ( Cohen d = 0.25 , Delta u 0.039 , t = 8.33 , and p < .001 ) , operation process ( Cohen d = 0.57 , Delta u 0.060 , t = 18.75 , and p < .001 ) , patient profile ( Cohen d = 1.19 , Delta u 0.132 , t = 39.33 , and p < .001 ) , and symptom ( Cohen d = 1.91 , Delta u = 0.274 , t = 62.82 , and p < .001 ) .\n",
      "ethics - [amod] - medical\n",
      "14 10 338\n",
      "meanwhile , patient with serious disease be more interested in medical competence ( Cohen d = -0.99 , Delta u = -0.165 , t = -32.58 , and p < .001 ) , medical advice and prescription ( Cohen d = -0.65 , Delta u = -0.082 , t = -21.45 , and p < .001 ) , financing ( Cohen d = -0.26 , Delta u = -0.018 , t = -8.45 , and p < .001 ) , and diagnosis and pathogenesis ( Cohen d = -1.55 , Delta u = -0.229 , t = -50.93 , and p < .001 ) .\n",
      "competence - [amod] - medical\n",
      "14 33 338\n",
      "meanwhile , patient with serious disease be more interested in medical competence ( Cohen d = -0.99 , Delta u = -0.165 , t = -32.58 , and p < .001 ) , medical advice and prescription ( Cohen d = -0.65 , Delta u = -0.082 , t = -21.45 , and p < .001 ) , financing ( Cohen d = -0.26 , Delta u = -0.018 , t = -8.45 , and p < .001 ) , and diagnosis and pathogenesis ( Cohen d = -1.55 , Delta u = -0.229 , t = -50.93 , and p < .001 ) .\n",
      "advice - [amod] - medical\n",
      "1 5 1624\n",
      "these patient record contain valuable medical information include patient information , diagnosis , treatment method , and eventual patient outcome .\n",
      "information - [amod] - medical\n",
      "#################### ('health', 143.5) ####################\n",
      "0 8 489\n",
      "as we begin to leverage Big Data in health care setting and particularly in assess patient-reported outcome , there be a need for novel analytic to address unique challenge .\n",
      "settings - [compound] - health\n",
      "0 9 115\n",
      "purpose as we begin to leverage Big Data in health care setting and particularly in assess patient-reported outcome , there be a need for novel analytic to address unique challenge .\n",
      "settings - [compound] - health\n",
      "0 22 1566\n",
      "the expand clinical information provide by the advent of electronic medical record offer a exciting opportunity to substantially improve the quality of health care .\n",
      "care - [compound] - health\n",
      "0 18 1101\n",
      "Electronic Medical Record ( EMR ) have establish itself as a valuable resource for large scale analysis of health datum .\n",
      "data - [compound] - health\n",
      "2 8 1112\n",
      "the propose MCLDA model assume that a latent health status group structure be responsible for the observe co-occurrence among diagnosis , medication , and contextual information .\n",
      "structure - [compound] - health\n",
      "3 28 80\n",
      "this study seek to identify case of undiagnosed dementia by develop and validate a weakly supervise machine-learning approach that incorporate the analysis of both structured and unstructured electronic health record ( ehr ) data.methodsa topic modeling approach that include latent Dirichlet allocation , stable topic extraction , and random sampling be apply to VHA ehr .\n",
      "record - [compound] - health\n",
      "0 12 32\n",
      "Diabetes and its complication have be recognize worldwide as a major public health threat .\n",
      "threat - [compound] - health\n",
      "0 14 627\n",
      "obstetric electronic medical record ( emr ) contain massive amount of medical datum and health information .\n",
      "information - [compound] - health\n",
      "1 27 1037\n",
      "the symptom or characteristic of illness course necessary to develop reliable predictor be not available in code billing datum , but may be present in narrative electronic health record ( ehr ) discharge summary .\n",
      "record - [compound] - health\n",
      "2 23 1402\n",
      "to this end , in this paper , we propose a new approach for risk stratification by explore a large volume of electronic health record ( ehr ) in a unsupervised fashion .\n",
      "records - [compound] - health\n",
      "0 12 1624\n",
      "the recent year have see a surge in the implementation of electronic health care record .\n",
      "records - [compound] - health\n",
      "5 12 1624\n",
      "in we research , we partition graph from term gather from electronic health record .\n",
      "records - [compound] - health\n",
      "#################### ('clinical', 139.72) ####################\n",
      "6 65 722\n",
      "a total of 33.3 % ( 12/36 ) patient positive for hpv-16 have cervical intraepithelial neoplasia ( cin ) 2 or a worse result , which be significantly higher than the prevalence of cin2 of 1.8 % ( 8/455 ) in patient negative for hpv-16 ( p < 0.001 ) , while no significant association be identify for other genotype in term of genotype and clinical progress .\n",
      "progress - [amod] - clinical\n",
      "11 24 722\n",
      "statistical and lda analysis produce consistent result regard the association between persistent infection of hpv-16 , old age and long infection period with a clinical progression of cin2 or worse .\n",
      "progression - [amod] - clinical\n",
      "0 2 1566\n",
      "the expand clinical information provide by the advent of electronic medical record offer a exciting opportunity to substantially improve the quality of health care .\n",
      "information - [amod] - clinical\n",
      "1 3 1566\n",
      "by examine the clinical observation ( such as diagnosis , risk factor , and medication ) mention in longitudinal emr , we can use patient ' medical chronology to automatically predict the progression of they pathology .\n",
      "observations - [amod] - clinical\n",
      "2 26 1566\n",
      "in this paper , we present a novel probabilistic model which jointly learn how to ( 1 ) group patient base on the similarity between they clinical observation as well as how to ( 2 ) predict the way a new patient 's clinical observation might evolve in the future .\n",
      "observations - [amod] - clinical\n",
      "2 43 1566\n",
      "in this paper , we present a novel probabilistic model which jointly learn how to ( 1 ) group patient base on the similarity between they clinical observation as well as how to ( 2 ) predict the way a new patient 's clinical observation might evolve in the future .\n",
      "observations - [amod] - clinical\n",
      "3 16 1566\n",
      "we show that we model can be use to not only track how a patient 's clinical finding might change over time , but to also identify which patient be due for preventative visit .\n",
      "findings - [amod] - clinical\n",
      "4 25 1566\n",
      "in addition , we model have the potential to improve the quality of over-all patient care in practice by predict the most likely set of clinical observation at a arbitrary point in the future .\n",
      "observations - [amod] - clinical\n",
      "1 14 1112\n",
      "to gain a better understanding of big healthcare datum and to develop better data-driven clinical decision support system , we propose a novel multiple - channel latent dirichlet allocation ( mclda ) approach for modeling diagnosis , medication , and contextual information in healthcare datum .\n",
      "systems - [amod] - clinical\n",
      "8 12 1112\n",
      "thus , mclda represent a promising approach to modeling healthcare datum for clinical decision support .\n",
      "support - [amod] - clinical\n",
      "0 8 2304\n",
      "induction of common knowledge or regularity from large-scale clinical datum be a vital task for chinese medicine ( cm ) .\n",
      "data - [amod] - clinical\n",
      "1 36 2304\n",
      "in this paper , we propose a data mining method , call the Symptom-Herb-Diagnosis topic ( shot ) model , to automatically extract the common relationship among symptom , herb combination and diagnosis from large-scale cm clinical datum .\n",
      "data - [amod] - clinical\n",
      "6 13 2304\n",
      "furthermore , the result demonstrate that this method be helpful for generate cm clinical guideline for t2dm base on structured collect clinical datum .\n",
      "guidelines - [amod] - clinical\n",
      "6 21 2304\n",
      "furthermore , the result demonstrate that this method be helpful for generate cm clinical guideline for t2dm base on structured collect clinical datum .\n",
      "data - [amod] - clinical\n",
      "4 32 1037\n",
      "the cohort be randomly split to derive a training ( 70 % ) and testing ( 30 % ) data set , and we train separate support vector machine model for baseline clinical feature alone , baseline feature plus common individual word and the above plus topic identify from the 75-topic LDA model .\n",
      "features - [amod] - clinical\n",
      "9 21 1037\n",
      "topic modeling and related approach offer the potential to improve prediction use ehr , if generalizability can be establish in other clinical cohort .\n",
      "cohorts - [amod] - clinical\n",
      "0 0 1446\n",
      "clinical pathway ( cp ) analysis play a important role in health-care management in ensure specialize , standardized , normalize and sophisticated therapy procedure for individual patient .\n",
      "pathway - [amod] - Clinical\n",
      "0 18 1402\n",
      "background and objective : risk stratification aim to provide physician with the accurate assessment of a patient 's clinical risk such that a individualized prevention or management strategy can be develop and deliver .\n",
      "risk - [amod] - clinical\n",
      "1 39 1402\n",
      "exist risk stratification technique mainly focus on predict the overall risk of a individual patient in a supervised manner , and , at the cohort level , often offer little insight beyond a flat score-based segmentation from the label clinical dataset .\n",
      "dataset - [amod] - clinical\n",
      "4 6 1402\n",
      "the propose PRSM recognize a patient clinical state as a probabilistic combination of latent sub-profile , and generate sub-profile-specific risk tier of patient from they ehr in a fully unsupervised fashion .\n",
      "state - [amod] - clinical\n",
      "7 12 1402\n",
      "result : we verify the effectiveness of the propose approach on a clinical dataset contain 3463 coronary heart disease ( chd ) patient instance .\n",
      "dataset - [amod] - clinical\n",
      "0 9 1129\n",
      "in healthcare organizational setting , the design of a clinical pathway ( cp ) be challenge since patient follow a particular pathway may have not only one single first-diagnosis but also several typical comorbidity , and thus it require different discipline involve to put together they partial knowledge about the overall pathway .\n",
      "pathway - [amod] - clinical\n",
      "1 23 1129\n",
      "although many datum mining technique have be propose to discover latent treatment information for cp analysis and reconstruction from a large volume of clinical datum , they be specific to extract nontrivial information about the therapy and treatment of the first-diagnosis .\n",
      "data - [amod] - clinical\n",
      "7 11 1129\n",
      "we verify the effectiveness of the propose model on a real clinical dataset contain 12,120 patient trace , which pertain to the unstable angina cp .\n",
      "dataset - [amod] - clinical\n",
      "#################### ('treatment', 99.44) ####################\n",
      "4 19 1101\n",
      "this topic modeling help to understand the constitution of patient disease and offer a tool for better planning of treatment .\n",
      "treatment - [case] - of\n",
      "planning - [nmod:of] - treatment\n",
      "3 12 2304\n",
      "we apply the SHDT model to discover the common cm diagnosis and treatment knowledge for type 2 diabetes mellitus ( t2dm ) use 3 238 inpatient case .\n",
      "knowledge - [compound] - treatment\n",
      "4 5 2304\n",
      "we obtain meaningful diagnosis and treatment topic ( cluster ) from the datum , which clinically indicate some important medical group correspond to comorbidity disease ( e.g. , heart disease and diabetic kidney disease in t2dm inpatient ) .\n",
      "topics - [compound] - treatment\n",
      "3 14 1446\n",
      "more specifically , we develop a probabilistic topic model to link patient feature and treatment behavior together to mine treatment pattern hide in emr .\n",
      "behaviors - [compound] - treatment\n",
      "3 19 1446\n",
      "more specifically , we develop a probabilistic topic model to link patient feature and treatment behavior together to mine treatment pattern hide in emr .\n",
      "patterns - [compound] - treatment\n",
      "4 1 1446\n",
      "discover treatment pattern , as actionable knowledge represent the best practice for most patient in most time of they treatment process , form the backbone of cp , and can be exploit to help physician better understand they specialty and learn from previous experience for cp analysis and improvement .\n",
      "patterns - [compound] - treatment\n",
      "4 19 1446\n",
      "discover treatment pattern , as actionable knowledge represent the best practice for most patient in most time of they treatment process , form the backbone of cp , and can be exploit to help physician better understand they specialty and learn from previous experience for cp analysis and improvement .\n",
      "processes - [compound] - treatment\n",
      "5 23 1446\n",
      "experimental result on a real collection of 985 emr collect from a chinese hospital show that the propose approach can effectively identify meaningful treatment pattern from emr .\n",
      "patterns - [compound] - treatment\n",
      "1 11 1129\n",
      "although many datum mining technique have be propose to discover latent treatment information for cp analysis and reconstruction from a large volume of clinical datum , they be specific to extract nontrivial information about the therapy and treatment of the first-diagnosis .\n",
      "information - [compound] - treatment\n",
      "1 37 1129\n",
      "although many datum mining technique have be propose to discover latent treatment information for cp analysis and reconstruction from a large volume of clinical datum , they be specific to extract nontrivial information about the therapy and treatment of the first-diagnosis .\n",
      "information - [nmod:about] - treatment\n",
      "therapy - [conj:and] - treatment\n",
      "2 7 1129\n",
      "the influence of comorbidity on adopt essential treatment be crucial for a pathway but have seldom be explore .\n",
      "treatments - [amod] - essential\n",
      "adopting - [dobj] - treatments\n",
      "3 6 1129\n",
      "this study propose to extract latent treatment pattem that characterize essential treatment for both first-diagnosis and typical comorbidity from the execution datum of a pathway .\n",
      "pattems - [compound] - treatment\n",
      "3 11 1129\n",
      "this study propose to extract latent treatment pattem that characterize essential treatment for both first-diagnosis and typical comorbidity from the execution datum of a pathway .\n",
      "treatments - [amod] - essential\n",
      "characterize - [dobj] - treatments\n",
      "treatments - [nmod:for] - first-diagnosis\n",
      "treatments - [nmod:for] - comorbidities\n",
      "4 12 1129\n",
      "in particular , we propose a generative statistical model to extract underlie treatment pattern , unveil the latent association between diagnosis label ( include both first-diagnosis and comorbidity ) and treatment , and compute the contribution of comorbidity in these pattem .\n",
      "patterns - [compound] - treatment\n",
      "4 30 1129\n",
      "in particular , we propose a generative statistical model to extract underlie treatment pattern , unveil the latent association between diagnosis label ( include both first-diagnosis and comorbidity ) and treatment , and compute the contribution of comorbidity in these pattem .\n",
      "associations - [nmod:between] - treatments\n",
      "labels - [conj:and] - treatments\n",
      "6 7 1129\n",
      "it first generate a set of latent treatment pattern from diagnosis label , follow by sampling treatment from each pattern .\n",
      "patterns - [compound] - treatment\n",
      "6 16 1129\n",
      "it first generate a set of latent treatment pattern from diagnosis label , follow by sampling treatment from each pattern .\n",
      "treatments - [case] - by\n",
      "treatments - [compound] - sampling\n",
      "followed - [nmod:by] - treatments\n",
      "treatments - [nmod:from] - pattern\n",
      "8 1 1129\n",
      "three treatment pattern be discover from datum , indicate latent correlation between comorbidity and treatment in the pathway .\n",
      "patterns - [compound] - treatment\n",
      "8 14 1129\n",
      "three treatment pattern be discover from datum , indicate latent correlation between comorbidity and treatment in the pathway .\n",
      "correlations - [nmod:between] - treatments\n",
      "comorbidities - [conj:and] - treatments\n",
      "9 10 1129\n",
      "in addition , a possible medical application in term of treatment recommendation be provide to illustrate the potential of the propose model .\n",
      "recommendation - [compound] - treatment\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 12 1129\n",
      "experimental result indicate that we approach can discover not only meaningful latent treatment pattern exhibit comorbidity focus , but also implicit change of treatment of first-diagnosis due to the incorporation of typical comorbidity potentially .\n",
      "patterns - [compound] - treatment\n",
      "10 23 1129\n",
      "experimental result indicate that we approach can discover not only meaningful latent treatment pattern exhibit comorbidity focus , but also implicit change of treatment of first-diagnosis due to the incorporation of typical comorbidity potentially .\n",
      "treatments - [case] - of\n",
      "changes - [nmod:of] - treatments\n",
      "treatments - [nmod:of] - first-diagnosis\n",
      "8 19 174\n",
      "the majority of theme ( n = 67 [ 60 % ] ) focus on discussion relate to as treatment .\n",
      "treatment - [case] - to\n",
      "treatment - [amod] - AS\n",
      "related - [nmod:to] - treatment\n",
      "10 4 174\n",
      "in discussion regard as treatment , most topic involve biologic , and most subtheme involve side effect ( e.g. , fatigue , allergic reaction ) , biologic treatment attribute ( e.g. , dosing , frequency ) , and concern about use of biologic ( e.g. , increase cancer risk ) .\n",
      "treatment - [case] - regarding\n",
      "treatment - [amod] - AS\n",
      "discussions - [nmod:regarding] - treatment\n",
      "10 27 174\n",
      "in discussion regard as treatment , most topic involve biologic , and most subtheme involve side effect ( e.g. , fatigue , allergic reaction ) , biologic treatment attribute ( e.g. , dosing , frequency ) , and concern about use of biologic ( e.g. , increase cancer risk ) .\n",
      "attributes - [compound] - treatment\n",
      "1 13 1624\n",
      "these patient record contain valuable medical information include patient information , diagnosis , treatment method , and eventual patient outcome .\n",
      "methods - [compound] - treatment\n",
      "#################### ('diagnosis', 86.78) ####################\n",
      "1 8 1566\n",
      "by examine the clinical observation ( such as diagnosis , risk factor , and medication ) mention in longitudinal emr , we can use patient ' medical chronology to automatically predict the progression of they pathology .\n",
      "diagnoses - [punct] - -LRB-\n",
      "diagnoses - [case] - such\n",
      "observations - [nmod:such_as] - diagnoses\n",
      "diagnoses - [punct] - ,\n",
      "diagnoses - [conj:and] - factors\n",
      "diagnoses - [punct] - ,\n",
      "diagnoses - [cc] - and\n",
      "diagnoses - [conj:and] - medications\n",
      "diagnoses - [punct] - -RRB-\n",
      "2 7 1101\n",
      "a medical record contain diagnostic information ( diagnosis code ) , procedure perform ( procedure code ) and admission detail .\n",
      "codes - [compound] - diagnosis\n",
      "3 36 1101\n",
      "traditional topic model , such as latent dirichlet allocation ( lda ) and hierarchical dirichlet process ( hdp ) , can be employ to discover disease topic from emr datum by treat patient as document and diagnosis code as word .\n",
      "codes - [compound] - diagnosis\n",
      "6 7 1101\n",
      "we be motivate by the fact that diagnosis code be connect in the form of icd-10 tree structure which present semantic relationship between code .\n",
      "codes - [compound] - diagnosis\n",
      "9 9 1101\n",
      "furthermore , since procedure code be often correlate with diagnosis code , we develop the correspondence wddcrf ( corr-wddcrf ) to explore conditional relationship of procedure code for a give disease pattern .\n",
      "codes - [compound] - diagnosis\n",
      "0 17 1112\n",
      "information and communication technology have enable healthcare institution to accumulate large amount of healthcare datum that include diagnosis , medication , and additional contextual information such as patient demographic .\n",
      "include - [dobj] - diagnoses\n",
      "diagnoses - [punct] - ,\n",
      "diagnoses - [conj:and] - medications\n",
      "diagnoses - [punct] - ,\n",
      "diagnoses - [cc] - and\n",
      "diagnoses - [conj:and] - information\n",
      "1 35 1112\n",
      "to gain a better understanding of big healthcare datum and to develop better data-driven clinical decision support system , we propose a novel multiple - channel latent dirichlet allocation ( mclda ) approach for modeling diagnosis , medication , and contextual information in healthcare datum .\n",
      "diagnoses - [case] - for\n",
      "diagnoses - [compound] - modeling\n",
      "approach - [nmod:for] - diagnoses\n",
      "diagnoses - [punct] - ,\n",
      "diagnoses - [conj:and] - medications\n",
      "diagnoses - [punct] - ,\n",
      "diagnoses - [cc] - and\n",
      "diagnoses - [conj:and] - information\n",
      "2 19 1112\n",
      "the propose MCLDA model assume that a latent health status group structure be responsible for the observe co-occurrence among diagnosis , medication , and contextual information .\n",
      "diagnoses - [case] - among\n",
      "co-occurrences - [nmod:among] - diagnoses\n",
      "diagnoses - [punct] - ,\n",
      "diagnoses - [conj:and] - medications\n",
      "diagnoses - [punct] - ,\n",
      "diagnoses - [cc] - and\n",
      "diagnoses - [conj:and] - information\n",
      "5 10 1112\n",
      "moreover , MCLDA be able to identify the pairing between diagnosis and medication in a record base on the assign latent group .\n",
      "diagnoses - [case] - between\n",
      "pairing - [nmod:between] - diagnoses\n",
      "diagnoses - [cc] - and\n",
      "diagnoses - [conj:and] - medications\n",
      "6 10 1112\n",
      "MCLDA can also be employ to predict miss medication or diagnosis give partial record .\n",
      "predict - [dobj] - diagnoses\n",
      "medications - [conj:or] - diagnoses\n",
      "7 35 1112\n",
      "we evaluation result also show that , in most case , MCLDA outperform alternative method such as logistic regression and the k-nearest-neighbor ( knn ) model for two prediction task , i.e. , medication and diagnosis prediction .\n",
      "prediction - [compound] - diagnosis\n",
      "8 18 80\n",
      "ConclusionsDementia be underdiagnosed , and thus , icd code alone can not serve as a gold standard for diagnosis .\n",
      "diagnosis - [case] - for\n",
      "standard - [nmod:for] - diagnosis\n",
      "10 20 80\n",
      "the study be one of the first to utilize both structured and unstructured ehr to develop risk score for the diagnosis of dementia .\n",
      "diagnosis - [case] - for\n",
      "diagnosis - [det] - the\n",
      "scores - [nmod:for] - diagnosis\n",
      "diagnosis - [nmod:of] - dementia\n",
      "1 4 627\n",
      "the information extraction and diagnosis assistant of obstetric emr be of great significance in improve the fertility level of the population .\n",
      "assistants - [compound] - diagnosis\n",
      "2 2 627\n",
      "the admit diagnosis in the first course record of the emr be reason from various source , such as chief complaint , auxiliary examination , and physical examination .\n",
      "diagnosis - [det] - The\n",
      "diagnosis - [amod] - admitting\n",
      "reasoned - [nsubjpass] - diagnosis\n",
      "diagnosis - [nmod:in] - record\n",
      "3 4 627\n",
      "this paper treat the diagnosis assistant as a multilabel classification task base on the analysis of obstetric emr .\n",
      "assistant - [compound] - diagnosis\n",
      "4 56 627\n",
      "the latent Dirichlet allocation ( lda ) topic and the word vector be use as feature and the four multilabel classification method , bp-mll ( backpropagation multilabel learning ) , rakel ( random k labelset ) , mlknn ( multilabel k-nearest neighbor ) , and cc ( chain classifier ) , be utilize to build the diagnosis assistant model .\n",
      "models - [compound] - diagnosis\n",
      "6 4 627\n",
      "the result of the diagnosis assistant can be introduce as a supplementary learning method for medical student .\n",
      "assistant - [compound] - diagnosis\n",
      "0 25 415\n",
      "multiple Kernel Learning with adaptive Neuro-Fuzzy Inference System ( mkl with anfis ) base deep learning method be propose in this paper for heart disease diagnosis .\n",
      "diagnosis - [case] - for\n",
      "diagnosis - [compound] - heart\n",
      "diagnosis - [compound] - disease\n",
      "proposed - [nmod:for] - diagnosis\n",
      "1 32 2304\n",
      "in this paper , we propose a data mining method , call the Symptom-Herb-Diagnosis topic ( shot ) model , to automatically extract the common relationship among symptom , herb combination and diagnosis from large-scale cm clinical datum .\n",
      "relationships - [nmod:among] - diagnoses\n",
      "symptoms - [conj:and] - diagnoses\n",
      "3 10 2304\n",
      "we apply the SHDT model to discover the common cm diagnosis and treatment knowledge for type 2 diabetes mellitus ( t2dm ) use 3 238 inpatient case .\n",
      "diagnosis - [det] - the\n",
      "diagnosis - [amod] - common\n",
      "diagnosis - [compound] - CM\n",
      "discover - [dobj] - diagnosis\n",
      "diagnosis - [cc] - and\n",
      "diagnosis - [conj:and] - knowledge\n",
      "diagnosis - [nmod:for] - mellitus\n",
      "4 3 2304\n",
      "we obtain meaningful diagnosis and treatment topic ( cluster ) from the datum , which clinically indicate some important medical group correspond to comorbidity disease ( e.g. , heart disease and diabetic kidney disease in t2dm inpatient ) .\n",
      "diagnosis - [amod] - meaningful\n",
      "obtained - [dobj] - diagnosis\n",
      "diagnosis - [cc] - and\n",
      "diagnosis - [conj:and] - topics\n",
      "2 19 1037\n",
      "we identify a cohort of individual admit to a psychiatric inpatient unit between 1994 and 2012 with a principal diagnosis of major depressive disorder , and extract inpatient psychiatric discharge narrative note .\n",
      "diagnosis - [case] - with\n",
      "diagnosis - [det] - a\n",
      "diagnosis - [amod] - principal\n",
      "admitted - [nmod:with] - diagnosis\n",
      "diagnosis - [nmod:of] - disorder\n",
      "4 20 1129\n",
      "in particular , we propose a generative statistical model to extract underlie treatment pattern , unveil the latent association between diagnosis label ( include both first-diagnosis and comorbidity ) and treatment , and compute the contribution of comorbidity in these pattem .\n",
      "labels - [compound] - diagnosis\n",
      "5 12 1129\n",
      "the propose model extend latent Dirichlet allocation with a additional layer for diagnosis modeling .\n",
      "modeling - [compound] - diagnosis\n",
      "6 10 1129\n",
      "it first generate a set of latent treatment pattern from diagnosis label , follow by sampling treatment from each pattern .\n",
      "labels - [compound] - diagnosis\n",
      "9 13 338\n",
      "the patient-related domain include the category of the patient profile , symptom , diagnosis , and pathogenesis .\n",
      "categories - [nmod:of] - diagnosis\n",
      "profile - [conj:and] - diagnosis\n",
      "12 78 338\n",
      "symptom ( Cohen d = 1.58 , Delta u = 0.216 , t = 229.75 , and p < .001 ) be more often mention by patient with acute disease , whereas communication skill ( Cohen d = -0.29 , Delta u = -0.038 , t = -42.01 , and p < .001 ) , financing ( Cohen d = -0.68 , Delta u = -0.098 , t = -99.26 , and p < .001 ) , and diagnosis and pathogenesis ( Cohen d = -0.55 , Delta u = -0.078 , t = -80.09 , and p < .001 ) be more often mention by patient with chronic disease .\n",
      "skills - [conj:and] - diagnosis\n",
      "mentioned - [nsubjpass] - diagnosis\n",
      "diagnosis - [cc] - and\n",
      "diagnosis - [conj:and] - pathogenesis\n",
      "diagnosis - [dep] - d\n",
      "diagnosis - [dep] - P\n",
      "14 81 338\n",
      "meanwhile , patient with serious disease be more interested in medical competence ( Cohen d = -0.99 , Delta u = -0.165 , t = -32.58 , and p < .001 ) , medical advice and prescription ( Cohen d = -0.65 , Delta u = -0.082 , t = -21.45 , and p < .001 ) , financing ( Cohen d = -0.26 , Delta u = -0.018 , t = -8.45 , and p < .001 ) , and diagnosis and pathogenesis ( Cohen d = -1.55 , Delta u = -0.229 , t = -50.93 , and p < .001 ) .\n",
      "interested - [nmod:in] - diagnosis\n",
      "competence - [conj:and] - diagnosis\n",
      "diagnosis - [cc] - and\n",
      "diagnosis - [conj:and] - pathogenesis\n",
      "diagnosis - [dep] - d\n",
      "diagnosis - [dep] - P\n",
      "1 11 1624\n",
      "these patient record contain valuable medical information include patient information , diagnosis , treatment method , and eventual patient outcome .\n",
      "information - [nmod:including] - diagnosis\n",
      "information - [conj:and] - diagnosis\n",
      "#################### ('disease', 81.07) ####################\n",
      "3 25 1101\n",
      "traditional topic model , such as latent dirichlet allocation ( lda ) and hierarchical dirichlet process ( hdp ) , can be employ to discover disease topic from emr datum by treat patient as document and diagnosis code as word .\n",
      "topics - [compound] - disease\n",
      "4 10 1101\n",
      "this topic modeling help to understand the constitution of patient disease and offer a tool for better planning of treatment .\n",
      "diseases - [case] - of\n",
      "diseases - [compound] - patient\n",
      "constitution - [nmod:of] - diseases\n",
      "5 33 1101\n",
      "in this paper , we propose a novel and flexible hierarchical bayesian nonparametric model , the word distance dependent chinese restaurant franchise ( wddcrf ) , which incorporate word-to-word distance to discover semantically-coherent disease topic .\n",
      "topics - [compound] - disease\n",
      "9 30 1101\n",
      "furthermore , since procedure code be often correlate with diagnosis code , we develop the correspondence wddcrf ( corr-wddcrf ) to explore conditional relationship of procedure code for a give disease pattern .\n",
      "pattern - [compound] - disease\n",
      "11 12 1101\n",
      "we evaluate the propose model on two real-world medical dataset - polyvascular disease and acute myocardial infarction disease .\n",
      "disease - [amod] - PolyVascular\n",
      "datasets - [dep] - disease\n",
      "disease - [cc] - and\n",
      "disease - [conj:and] - disease\n",
      "11 17 1101\n",
      "we evaluate the propose model on two real-world medical dataset - polyvascular disease and acute myocardial infarction disease .\n",
      "disease - [amod] - Acute\n",
      "disease - [amod] - Myocardial\n",
      "disease - [compound] - Infarction\n",
      "datasets - [dep] - disease\n",
      "disease - [conj:and] - disease\n",
      "13 3 1101\n",
      "we also use disease topic proportion as new feature and show that use feature from the corr-wddcrf outperform the baseline on 14-day readmission prediction .\n",
      "proportions - [compound] - disease\n",
      "0 24 415\n",
      "multiple Kernel Learning with adaptive Neuro-Fuzzy Inference System ( mkl with anfis ) base deep learning method be propose in this paper for heart disease diagnosis .\n",
      "diagnosis - [compound] - disease\n",
      "2 9 415\n",
      "mkl method be use to divide parameter between heart disease patient and normal individual .\n",
      "patients - [compound] - disease\n",
      "3 17 415\n",
      "the result obtain from the mkl method be give to the ANFIS classifier to classify the heart disease and healthy patient .\n",
      "disease - [det] - the\n",
      "disease - [compound] - heart\n",
      "classify - [dobj] - disease\n",
      "disease - [cc] - and\n",
      "disease - [conj:and] - patients\n",
      "4 24 2304\n",
      "we obtain meaningful diagnosis and treatment topic ( cluster ) from the datum , which clinically indicate some important medical group correspond to comorbidity disease ( e.g. , heart disease and diabetic kidney disease in t2dm inpatient ) .\n",
      "diseases - [case] - to\n",
      "diseases - [compound] - comorbidity\n",
      "corresponding - [nmod:to] - diseases\n",
      "diseases - [dep] - disease\n",
      "diseases - [dep] - diseases\n",
      "4 29 2304\n",
      "we obtain meaningful diagnosis and treatment topic ( cluster ) from the datum , which clinically indicate some important medical group correspond to comorbidity disease ( e.g. , heart disease and diabetic kidney disease in t2dm inpatient ) .\n",
      "disease - [punct] - -LRB-\n",
      "disease - [dep] - e.g.\n",
      "disease - [punct] - ,\n",
      "disease - [compound] - heart\n",
      "diseases - [dep] - disease\n",
      "disease - [cc] - and\n",
      "disease - [conj:and] - diseases\n",
      "disease - [nmod:in] - inpatients\n",
      "disease - [punct] - -RRB-\n",
      "4 33 2304\n",
      "we obtain meaningful diagnosis and treatment topic ( cluster ) from the datum , which clinically indicate some important medical group correspond to comorbidity disease ( e.g. , heart disease and diabetic kidney disease in t2dm inpatient ) .\n",
      "diseases - [amod] - diabetic\n",
      "diseases - [compound] - kidney\n",
      "diseases - [dep] - diseases\n",
      "disease - [conj:and] - diseases\n",
      "6 40 1037\n",
      "the 75-topic LDA model include topic link to psychiatric symptom ( suicide , severe depression , anxiety , trauma , eating/weight and panic ) and major depressive disorder comorbidity ( infection , postpartum , brain tumor , diarrhea and pulmonary disease ) .\n",
      "disease - [amod] - pulmonary\n",
      "comorbidities - [dep] - disease\n",
      "infection - [conj:and] - disease\n",
      "6 48 1402\n",
      "in addition , we present a extension of PRSM , call weakly supervise prsm ( ws-prsm ) by incorporate minimum prior information into the model , in order to improve the risk stratification accuracy , and to make we model highly portable to risk stratification task of various disease .\n",
      "diseases - [case] - of\n",
      "diseases - [amod] - various\n",
      "tasks - [nmod:of] - diseases\n",
      "7 18 1402\n",
      "result : we verify the effectiveness of the propose approach on a clinical dataset contain 3463 coronary heart disease ( chd ) patient instance .\n",
      "disease - [nummod] - 3463\n",
      "disease - [amod] - coronary\n",
      "disease - [compound] - heart\n",
      "instances - [compound] - disease\n",
      "disease - [appos] - CHD\n",
      "11 20 1402\n",
      "in addition , the unsupervised nature of we model make they highly portable to the risk stratification task of various disease .\n",
      "diseases - [case] - of\n",
      "diseases - [amod] - various\n",
      "tasks - [nmod:of] - diseases\n",
      "9 17 174\n",
      "other theme , include the psychological impact of as , reporting of medical literature , and as disease consequence , account for the remain 40 % ( n = 45 ) .\n",
      "consequences - [compound] - disease\n",
      "12 29 338\n",
      "symptom ( Cohen d = 1.58 , Delta u = 0.216 , t = 229.75 , and p < .001 ) be more often mention by patient with acute disease , whereas communication skill ( Cohen d = -0.29 , Delta u = -0.038 , t = -42.01 , and p < .001 ) , financing ( Cohen d = -0.68 , Delta u = -0.098 , t = -99.26 , and p < .001 ) , and diagnosis and pathogenesis ( Cohen d = -0.55 , Delta u = -0.078 , t = -80.09 , and p < .001 ) be more often mention by patient with chronic disease .\n",
      "diseases - [case] - with\n",
      "diseases - [amod] - acute\n",
      "patients - [nmod:with] - diseases\n",
      "12 109 338\n",
      "symptom ( Cohen d = 1.58 , Delta u = 0.216 , t = 229.75 , and p < .001 ) be more often mention by patient with acute disease , whereas communication skill ( Cohen d = -0.29 , Delta u = -0.038 , t = -42.01 , and p < .001 ) , financing ( Cohen d = -0.68 , Delta u = -0.098 , t = -99.26 , and p < .001 ) , and diagnosis and pathogenesis ( Cohen d = -0.55 , Delta u = -0.078 , t = -80.09 , and p < .001 ) be more often mention by patient with chronic disease .\n",
      "diseases - [case] - with\n",
      "diseases - [amod] - chronic\n",
      "patients - [nmod:with] - diseases\n",
      "13 3 338\n",
      "patient with mild disease be more interested in medical ethic ( Cohen d = 0.25 , Delta u 0.039 , t = 8.33 , and p < .001 ) , operation process ( Cohen d = 0.57 , Delta u 0.060 , t = 18.75 , and p < .001 ) , patient profile ( Cohen d = 1.19 , Delta u 0.132 , t = 39.33 , and p < .001 ) , and symptom ( Cohen d = 1.91 , Delta u = 0.274 , t = 62.82 , and p < .001 ) .\n",
      "diseases - [case] - with\n",
      "diseases - [amod] - mild\n",
      "Patients - [nmod:with] - diseases\n",
      "14 5 338\n",
      "meanwhile , patient with serious disease be more interested in medical competence ( Cohen d = -0.99 , Delta u = -0.165 , t = -32.58 , and p < .001 ) , medical advice and prescription ( Cohen d = -0.65 , Delta u = -0.082 , t = -21.45 , and p < .001 ) , financing ( Cohen d = -0.26 , Delta u = -0.018 , t = -8.45 , and p < .001 ) , and diagnosis and pathogenesis ( Cohen d = -1.55 , Delta u = -0.229 , t = -50.93 , and p < .001 ) .\n",
      "diseases - [case] - with\n",
      "diseases - [amod] - serious\n",
      "patients - [nmod:with] - diseases\n",
      "17 14 338\n",
      "furthermore , the mining result reveal marked difference in patient ' interest across different disease type , socioeconomic development level , and hospital level .\n",
      "types - [compound] - disease\n",
      "#################### ('risk', 79.0) ####################\n",
      "1 10 1566\n",
      "by examine the clinical observation ( such as diagnosis , risk factor , and medication ) mention in longitudinal emr , we can use patient ' medical chronology to automatically predict the progression of they pathology .\n",
      "factors - [compound] - risk\n",
      "9 29 80\n",
      "however , this study suggest that imperfect datum ( e.g. , icd code in combination with other ehr feature ) can serve as a silver standard to develop a risk model , apply that model to patient without dementia code , and then select a case-detection threshold .\n",
      "model - [compound] - risk\n",
      "10 16 80\n",
      "the study be one of the first to utilize both structured and unstructured ehr to develop risk score for the diagnosis of dementia .\n",
      "scores - [compound] - risk\n",
      "0 15 1037\n",
      "the ability to predict psychiatric readmission would facilitate the development of intervention to reduce this risk , a major driver of psychiatric health-care cost .\n",
      "risk - [det] - this\n",
      "reduce - [dobj] - risk\n",
      "8 15 1037\n",
      "inclusion of topic derive from narrative note allow more accurate discrimination of individual at high risk for psychiatric readmission in this cohort .\n",
      "risk - [case] - at\n",
      "risk - [amod] - high\n",
      "discrimination - [nmod:at] - risk\n",
      "risk - [nmod:for] - readmission\n",
      "0 4 1402\n",
      "background and objective : risk stratification aim to provide physician with the accurate assessment of a patient 's clinical risk such that a individualized prevention or management strategy can be develop and deliver .\n",
      "stratification - [compound] - Risk\n",
      "0 19 1402\n",
      "background and objective : risk stratification aim to provide physician with the accurate assessment of a patient 's clinical risk such that a individualized prevention or management strategy can be develop and deliver .\n",
      "risk - [case] - of\n",
      "risk - [nmod:poss] - patient\n",
      "risk - [amod] - clinical\n",
      "assessment - [nmod:of] - risk\n",
      "1 1 1402\n",
      "exist risk stratification technique mainly focus on predict the overall risk of a individual patient in a supervised manner , and , at the cohort level , often offer little insight beyond a flat score-based segmentation from the label clinical dataset .\n",
      "techniques - [compound] - risk\n",
      "1 10 1402\n",
      "exist risk stratification technique mainly focus on predict the overall risk of a individual patient in a supervised manner , and , at the cohort level , often offer little insight beyond a flat score-based segmentation from the label clinical dataset .\n",
      "risk - [det] - the\n",
      "risk - [amod] - overall\n",
      "predicting - [dobj] - risk\n",
      "risk - [nmod:of] - patient\n",
      "2 14 1402\n",
      "to this end , in this paper , we propose a new approach for risk stratification by explore a large volume of electronic health record ( ehr ) in a unsupervised fashion .\n",
      "stratification - [compound] - risk\n",
      "3 17 1402\n",
      "method : along this line , this paper propose a novel probabilistic topic modeling framework call probabilistic risk stratification model ( prsm ) base on latent Dirichlet allocation ( lda ) .\n",
      "model - [compound] - risk\n",
      "4 19 1402\n",
      "the propose PRSM recognize a patient clinical state as a probabilistic combination of latent sub-profile , and generate sub-profile-specific risk tier of patient from they ehr in a fully unsupervised fashion .\n",
      "tiers - [compound] - risk\n",
      "6 31 1402\n",
      "in addition , we present a extension of PRSM , call weakly supervise prsm ( ws-prsm ) by incorporate minimum prior information into the model , in order to improve the risk stratification accuracy , and to make we model highly portable to risk stratification task of various disease .\n",
      "accuracy - [compound] - risk\n",
      "6 43 1402\n",
      "in addition , we present a extension of PRSM , call weakly supervise prsm ( ws-prsm ) by incorporate minimum prior information into the model , in order to improve the risk stratification accuracy , and to make we model highly portable to risk stratification task of various disease .\n",
      "risk - [nsubj:xsubj] - models\n",
      "risk - [mark] - to\n",
      "portable - [xcomp] - risk\n",
      "risk - [dobj] - tasks\n",
      "8 10 1402\n",
      "both prsm and ws-prsm be compare with two established supervised risk stratification algorithm , i.e. , logistic regression and support vector machine , and show the effectiveness of we model in risk stratification of chd in term of the Area under the receiver operating characteristic curve ( auc ) analysis .\n",
      "algorithms - [compound] - risk\n",
      "8 31 1402\n",
      "both prsm and ws-prsm be compare with two established supervised risk stratification algorithm , i.e. , logistic regression and support vector machine , and show the effectiveness of we model in risk stratification of chd in term of the Area under the receiver operating characteristic curve ( auc ) analysis .\n",
      "stratification - [compound] - risk\n",
      "9 24 1402\n",
      "as well , in comparison with prsm , ws-prsm have over 2 % performance gain , on the experimental dataset , demonstrate that incorporate risk score knowledge as prior information can improve the performance in risk stratification .\n",
      "incorporating - [dobj] - risk\n",
      "risk - [acl] - scoring\n",
      "9 35 1402\n",
      "as well , in comparison with prsm , ws-prsm have over 2 % performance gain , on the experimental dataset , demonstrate that incorporate risk score knowledge as prior information can improve the performance in risk stratification .\n",
      "stratification - [compound] - risk\n",
      "10 12 1402\n",
      "conclusion : experimental result reveal that we model achieve competitive performance in risk stratification in comparison with exist supervised approach .\n",
      "stratification - [compound] - risk\n",
      "11 15 1402\n",
      "in addition , the unsupervised nature of we model make they highly portable to the risk stratification task of various disease .\n",
      "tasks - [compound] - risk\n",
      "12 6 1402\n",
      "moreover , patient sub-profile and sub-profile-specific risk tier generate by we model be coherent and informative , and provide significant potential to be explore for the further task , such as patient cohort analysis .\n",
      "tiers - [compound] - risk\n",
      "13 12 1402\n",
      "we hypothesize that the propose framework can readily meet the demand for risk stratification from a large volume of ehr in a open-ended fashion .\n",
      "stratification - [compound] - risk\n",
      "10 48 174\n",
      "in discussion regard as treatment , most topic involve biologic , and most subtheme involve side effect ( e.g. , fatigue , allergic reaction ) , biologic treatment attribute ( e.g. , dosing , frequency ) , and concern about use of biologic ( e.g. , increase cancer risk ) .\n",
      "risk - [amod] - increased\n",
      "risk - [compound] - cancer\n",
      "e.g. - [appos] - risk\n",
      "#################### ('record', 78.67) ####################\n",
      "0 11 1566\n",
      "the expand clinical information provide by the advent of electronic medical record offer a exciting opportunity to substantially improve the quality of health care .\n",
      "records - [case] - of\n",
      "records - [amod] - electronic\n",
      "records - [amod] - medical\n",
      "advent - [nmod:of] - records\n",
      "1 8 1101\n",
      "a hospital emr dataset typically consist of medical record of hospitalize patient .\n",
      "records - [case] - of\n",
      "records - [amod] - medical\n",
      "consists - [nmod:of] - records\n",
      "records - [nmod:of] - patients\n",
      "2 2 1101\n",
      "a medical record contain diagnostic information ( diagnosis code ) , procedure perform ( procedure code ) and admission detail .\n",
      "record - [det] - A\n",
      "record - [amod] - medical\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "contains - [nsubj] - record\n",
      "3 12 1112\n",
      "use a real-world research testb that include one million healthcare insurance claim record , we investigate the utility of MCLDA .\n",
      "records - [nummod] - million\n",
      "records - [compound] - healthcare\n",
      "records - [compound] - insurance\n",
      "records - [compound] - claim\n",
      "includes - [dobj] - records\n",
      "5 15 1112\n",
      "moreover , MCLDA be able to identify the pairing between diagnosis and medication in a record base on the assign latent group .\n",
      "record - [case] - in\n",
      "record - [det] - a\n",
      "identify - [nmod:in] - record\n",
      "record - [nmod:based_on] - groups\n",
      "6 13 1112\n",
      "MCLDA can also be employ to predict miss medication or diagnosis give partial record .\n",
      "records - [case] - given\n",
      "records - [amod] - partial\n",
      "medications - [nmod:given] - records\n",
      "3 29 80\n",
      "this study seek to identify case of undiagnosed dementia by develop and validate a weakly supervise machine-learning approach that incorporate the analysis of both structured and unstructured electronic health record ( ehr ) data.methodsa topic modeling approach that include latent Dirichlet allocation , stable topic extraction , and random sampling be apply to VHA ehr .\n",
      "record - [case] - of\n",
      "record - [amod] - structured\n",
      "record - [amod] - unstructured\n",
      "record - [amod] - electronic\n",
      "record - [compound] - health\n",
      "analysis - [nmod:of] - record\n",
      "included - [nsubj] - record\n",
      "record - [appos] - EHR\n",
      "record - [dep] - approach\n",
      "record - [ref] - that\n",
      "record - [acl:relcl] - included\n",
      "6 25 80\n",
      "these score be validate in a subset of veteran without icd-9 dementia code ( n = 120 ) by expert in dementia who perform manual record review and achieve a high level of inter-rater agreement .\n",
      "reviews - [compound] - record\n",
      "2 10 32\n",
      "while many study currently use medical image and structured medical record , very limited effort have be dedicate to apply datum mining technique for unstructured textual medical record , such as admission and discharge record .\n",
      "records - [amod] - structured\n",
      "records - [amod] - medical\n",
      "use - [dobj] - records\n",
      "images - [conj:and] - records\n",
      "2 27 32\n",
      "while many study currently use medical image and structured medical record , very limited effort have be dedicate to apply datum mining technique for unstructured textual medical record , such as admission and discharge record .\n",
      "records - [case] - for\n",
      "records - [amod] - unstructured\n",
      "records - [amod] - textual\n",
      "records - [amod] - medical\n",
      "applying - [nmod:for] - records\n",
      "records - [punct] - ,\n",
      "records - [nmod:such_as] - admission\n",
      "records - [nmod:such_as] - records\n",
      "2 34 32\n",
      "while many study currently use medical image and structured medical record , very limited effort have be dedicate to apply datum mining technique for unstructured textual medical record , such as admission and discharge record .\n",
      "records - [compound] - discharge\n",
      "records - [nmod:such_as] - records\n",
      "admission - [conj:and] - records\n",
      "3 6 32\n",
      "moreover , the similarity among medical record that be overlook by exist approach could potentially improve the accuracy of prediction model .\n",
      "records - [case] - among\n",
      "records - [amod] - medical\n",
      "similarities - [nmod:among] - records\n",
      "overlooked - [nsubjpass] - records\n",
      "records - [ref] - that\n",
      "records - [acl:relcl] - overlooked\n",
      "5 10 32\n",
      "specifically , we first estimate the similarity between textual medical record after datum preprocessing , and then we perform selda-based diabetic complication topic mining base on similarity constraint .\n",
      "records - [case] - between\n",
      "records - [amod] - textual\n",
      "records - [amod] - medical\n",
      "similarity - [nmod:between] - records\n",
      "0 3 627\n",
      "obstetric electronic medical record ( emr ) contain massive amount of medical datum and health information .\n",
      "records - [amod] - Obstetric\n",
      "records - [amod] - electronic\n",
      "records - [amod] - medical\n",
      "contain - [nsubj] - records\n",
      "records - [appos] - EMRs\n",
      "2 7 627\n",
      "the admit diagnosis in the first course record of the emr be reason from various source , such as chief complaint , auxiliary examination , and physical examination .\n",
      "record - [case] - in\n",
      "record - [det] - the\n",
      "record - [amod] - first\n",
      "record - [compound] - course\n",
      "diagnosis - [nmod:in] - record\n",
      "record - [nmod:of] - EMR\n",
      "7 17 627\n",
      "additionally , the method can be use not only for obstetric emr but also for other medical record .\n",
      "records - [advmod] - also\n",
      "records - [case] - for\n",
      "records - [amod] - other\n",
      "records - [amod] - medical\n",
      "used - [nmod:for] - records\n",
      "1 28 1037\n",
      "the symptom or characteristic of illness course necessary to develop reliable predictor be not available in code billing datum , but may be present in narrative electronic health record ( ehr ) discharge summary .\n",
      "record - [case] - in\n",
      "record - [amod] - narrative\n",
      "record - [amod] - electronic\n",
      "record - [compound] - health\n",
      "present - [nmod:in] - record\n",
      "record - [appos] - EHR\n",
      "record - [dep] - summaries\n",
      "1 17 1446\n",
      "recently , with the rapid development of hospital information system , a large volume of electronic medical record ( emr ) have be produce , which provide a comprehensive source for cp analysis .\n",
      "records - [case] - of\n",
      "records - [amod] - electronic\n",
      "records - [amod] - medical\n",
      "volume - [nmod:of] - records\n",
      "records - [appos] - EMRs\n",
      "2 24 1402\n",
      "to this end , in this paper , we propose a new approach for risk stratification by explore a large volume of electronic health record ( ehr ) in a unsupervised fashion .\n",
      "records - [case] - of\n",
      "records - [amod] - electronic\n",
      "records - [compound] - health\n",
      "volume - [nmod:of] - records\n",
      "records - [appos] - EHRs\n",
      "0 14 1624\n",
      "the recent year have see a surge in the implementation of electronic health care record .\n",
      "records - [case] - of\n",
      "records - [amod] - electronic\n",
      "records - [compound] - health\n",
      "records - [compound] - care\n",
      "implementation - [nmod:of] - records\n",
      "1 2 1624\n",
      "these patient record contain valuable medical information include patient information , diagnosis , treatment method , and eventual patient outcome .\n",
      "records - [det] - These\n",
      "records - [compound] - patient\n",
      "contain - [nsubj] - records\n",
      "2 8 1624\n",
      "it be important to analyze pattern within these record in order to more efficiently treat individual .\n",
      "records - [case] - within\n",
      "records - [det] - these\n",
      "patterns - [nmod:within] - records\n",
      "5 13 1624\n",
      "in we research , we partition graph from term gather from electronic health record .\n",
      "records - [case] - from\n",
      "records - [amod] - electronic\n",
      "records - [compound] - health\n",
      "gathered - [nmod:from] - records\n",
      "#################### ('care', 68.06) ####################\n",
      "0 9 489\n",
      "as we begin to leverage Big Data in health care setting and particularly in assess patient-reported outcome , there be a need for novel analytic to address unique challenge .\n",
      "settings - [compound] - care\n",
      "0 10 115\n",
      "purpose as we begin to leverage Big Data in health care setting and particularly in assess patient-reported outcome , there be a need for novel analytic to address unique challenge .\n",
      "settings - [compound] - care\n",
      "0 23 1566\n",
      "the expand clinical information provide by the advent of electronic medical record offer a exciting opportunity to substantially improve the quality of health care .\n",
      "care - [case] - of\n",
      "care - [compound] - health\n",
      "quality - [nmod:of] - care\n",
      "4 15 1566\n",
      "in addition , we model have the potential to improve the quality of over-all patient care in practice by predict the most likely set of clinical observation at a arbitrary point in the future .\n",
      "care - [case] - of\n",
      "care - [amod] - over-all\n",
      "care - [compound] - patient\n",
      "quality - [nmod:of] - care\n",
      "0 13 1624\n",
      "the recent year have see a surge in the implementation of electronic health care record .\n",
      "records - [compound] - care\n",
      "#################### ('friend', 47.64) ####################\n",
      "8 31 115\n",
      "six month after the surgery , they be replace by goal on regain a sense of normalcy , to resume work , to enjoy life more fully , and to appreciate friend and family more .\n",
      "appreciate - [dobj] - friends\n",
      "friends - [cc] - and\n",
      "friends - [conj:and] - family\n",
      "#################### ('healthcare', 42.31) ####################\n",
      "0 6 1112\n",
      "information and communication technology have enable healthcare institution to accumulate large amount of healthcare datum that include diagnosis , medication , and additional contextual information such as patient demographic .\n",
      "institutions - [compound] - healthcare\n",
      "0 13 1112\n",
      "information and communication technology have enable healthcare institution to accumulate large amount of healthcare datum that include diagnosis , medication , and additional contextual information such as patient demographic .\n",
      "data - [compound] - healthcare\n",
      "1 7 1112\n",
      "to gain a better understanding of big healthcare datum and to develop better data-driven clinical decision support system , we propose a novel multiple - channel latent dirichlet allocation ( mclda ) approach for modeling diagnosis , medication , and contextual information in healthcare datum .\n",
      "data - [compound] - healthcare\n",
      "1 43 1112\n",
      "to gain a better understanding of big healthcare datum and to develop better data-driven clinical decision support system , we propose a novel multiple - channel latent dirichlet allocation ( mclda ) approach for modeling diagnosis , medication , and contextual information in healthcare datum .\n",
      "data - [compound] - healthcare\n",
      "3 9 1112\n",
      "use a real-world research testb that include one million healthcare insurance claim record , we investigate the utility of MCLDA .\n",
      "records - [compound] - healthcare\n",
      "8 9 1112\n",
      "thus , mclda represent a promising approach to modeling healthcare datum for clinical decision support .\n",
      "data - [compound] - healthcare\n",
      "0 1 1129\n",
      "in healthcare organizational setting , the design of a clinical pathway ( cp ) be challenge since patient follow a particular pathway may have not only one single first-diagnosis but also several typical comorbidity , and thus it require different discipline involve to put together they partial knowledge about the overall pathway .\n",
      "settings - [compound] - healthcare\n",
      "#################### ('cancer', 39.36) ####################\n",
      "4 29 489\n",
      "we aim to apply LDA to interview datum collect as part of a prospective , longitudinal study of qol in patient undergo radical cystectomy and urinary diversion for bladder cancer .\n",
      "cancer - [case] - for\n",
      "cancer - [compound] - bladder\n",
      "undergoing - [nmod:for] - cancer\n",
      "5 14 489\n",
      "lda show that , prior to surgery , patient ' priority be primarily in cancer surgery and recovery .\n",
      "surgery - [compound] - cancer\n",
      "3 31 115\n",
      "method LDA be apply to interview datum collect as part of a prospective , longitudinal study of qol in n = 211 patient undergo radical cystectomy and urinary diversion for bladder cancer .\n",
      "cancer - [case] - for\n",
      "cancer - [compound] - bladder\n",
      "undergoing - [nmod:for] - cancer\n",
      "7 10 115\n",
      "prior to surgery , patient ' priority be primarily in cancer surgery and recovery .\n",
      "surgery - [compound] - cancer\n",
      "9 24 115\n",
      "LDA model parameter show change priority , e.g. , immediate concern on surgery and resume employment decrease post-surgery and be replace by concern over cancer recurrence and a desire to remain healthy and strong .\n",
      "recurrence - [compound] - cancer\n",
      "10 47 174\n",
      "in discussion regard as treatment , most topic involve biologic , and most subtheme involve side effect ( e.g. , fatigue , allergic reaction ) , biologic treatment attribute ( e.g. , dosing , frequency ) , and concern about use of biologic ( e.g. , increase cancer risk ) .\n",
      "risk - [compound] - cancer\n",
      "#################### ('code', 36.28) ####################\n",
      "1 5 489\n",
      "one such challenge be in code transcribe interview datum , typically free-text entry of statement make by interviewee during face-to-face interview .\n",
      "ROOT - [ROOT] - coding\n",
      "coding - [nsubj] - challenge\n",
      "coding - [cop] - is\n",
      "coding - [mark] - in\n",
      "coding - [dobj] - data\n",
      "coding - [punct] - .\n",
      "7 25 489\n",
      "novel analytic such as lda offer the possibility of summarize personal goal in real time without the need for conventional fixed-length measure and qualitative datum code .\n",
      "data - [acl] - coding\n",
      "1 5 115\n",
      "one such challenge be in code transcribe interview datum , typically free-text entry of statement make during a face-to-face interview .\n",
      "ROOT - [ROOT] - coding\n",
      "coding - [nsubj] - challenge\n",
      "coding - [cop] - is\n",
      "coding - [mark] - in\n",
      "coding - [dobj] - data\n",
      "coding - [punct] - .\n",
      "10 26 115\n",
      "conclusion novel Big Data analytic such as lda offer the possibility of summarize personal goal without the need for conventional fixed-length measure and resource-intensive qualitative datum code .\n",
      "data - [acl] - coding\n",
      "2 8 1101\n",
      "a medical record contain diagnostic information ( diagnosis code ) , procedure perform ( procedure code ) and admission detail .\n",
      "codes - [punct] - -LRB-\n",
      "codes - [compound] - diagnosis\n",
      "information - [appos] - codes\n",
      "codes - [punct] - -RRB-\n",
      "2 15 1101\n",
      "a medical record contain diagnostic information ( diagnosis code ) , procedure perform ( procedure code ) and admission detail .\n",
      "codes - [punct] - -LRB-\n",
      "codes - [compound] - procedure\n",
      "performed - [appos] - codes\n",
      "codes - [punct] - -RRB-\n",
      "3 37 1101\n",
      "traditional topic model , such as latent dirichlet allocation ( lda ) and hierarchical dirichlet process ( hdp ) , can be employ to discover disease topic from emr datum by treat patient as document and diagnosis code as word .\n",
      "codes - [compound] - diagnosis\n",
      "treating - [nmod:as] - codes\n",
      "documents - [conj:and] - codes\n",
      "6 8 1101\n",
      "we be motivate by the fact that diagnosis code be connect in the form of icd-10 tree structure which present semantic relationship between code .\n",
      "codes - [compound] - diagnosis\n",
      "connected - [nsubjpass] - codes\n",
      "6 23 1101\n",
      "we be motivate by the fact that diagnosis code be connect in the form of icd-10 tree structure which present semantic relationship between code .\n",
      "codes - [case] - between\n",
      "relationships - [nmod:between] - codes\n",
      "9 4 1101\n",
      "furthermore , since procedure code be often correlate with diagnosis code , we develop the correspondence wddcrf ( corr-wddcrf ) to explore conditional relationship of procedure code for a give disease pattern .\n",
      "codes - [compound] - procedure\n",
      "correlated - [nsubjpass] - codes\n",
      "9 10 1101\n",
      "furthermore , since procedure code be often correlate with diagnosis code , we develop the correspondence wddcrf ( corr-wddcrf ) to explore conditional relationship of procedure code for a give disease pattern .\n",
      "codes - [case] - with\n",
      "codes - [compound] - diagnosis\n",
      "correlated - [nmod:with] - codes\n",
      "9 26 1101\n",
      "furthermore , since procedure code be often correlate with diagnosis code , we develop the correspondence wddcrf ( corr-wddcrf ) to explore conditional relationship of procedure code for a give disease pattern .\n",
      "codes - [case] - of\n",
      "codes - [compound] - procedure\n",
      "relationships - [nmod:of] - codes\n",
      "14 7 1101\n",
      "beside these , the prediction for procedure code base on the corr-wddcrf also show considerable accuracy .\n",
      "codes - [case] - for\n",
      "codes - [compound] - procedure\n",
      "prediction - [nmod:for] - codes\n",
      "4 29 80\n",
      "topic feature from unstructured datum and feature from structured datum be compare between veteran with ( n = 1861 ) and without ( n = 9305 ) icd-9 dementia code .\n",
      "codes - [case] - without\n",
      "codes - [compound] - ICD-9\n",
      "codes - [compound] - dementia\n",
      "Veterans - [nmod:without] - codes\n",
      "5 35 80\n",
      "a logistic regression model be use to develop dementia prediction score , and manual review be conduct to validate the machine-learning results.resultsa total of 853 feature be identify ( 290 topic , 174 non-dementia icd code , 159 CPT code , 59 medication , and 171 note type ) for the development of logistic regression prediction score .\n",
      "codes - [nummod] - 174\n",
      "codes - [amod] - non-dementia\n",
      "codes - [compound] - ICD\n",
      "topics - [appos] - codes\n",
      "5 39 80\n",
      "a logistic regression model be use to develop dementia prediction score , and manual review be conduct to validate the machine-learning results.resultsa total of 853 feature be identify ( 290 topic , 174 non-dementia icd code , 159 CPT code , 59 medication , and 171 note type ) for the development of logistic regression prediction score .\n",
      "codes - [nummod] - 159\n",
      "codes - [compound] - CPT\n",
      "topics - [appos] - codes\n",
      "6 12 80\n",
      "these score be validate in a subset of veteran without icd-9 dementia code ( n = 120 ) by expert in dementia who perform manual record review and achieve a high level of inter-rater agreement .\n",
      "codes - [case] - without\n",
      "codes - [compound] - ICD-9\n",
      "codes - [compound] - dementia\n",
      "validated - [nmod:without] - codes\n",
      "codes - [dep] - n\n",
      "codes - [nmod:by] - experts\n",
      "8 8 80\n",
      "ConclusionsDementia be underdiagnosed , and thus , icd code alone can not serve as a gold standard for diagnosis .\n",
      "codes - [compound] - ICD\n",
      "serve - [nsubj] - codes\n",
      "codes - [advmod] - alone\n",
      "9 12 80\n",
      "however , this study suggest that imperfect datum ( e.g. , icd code in combination with other ehr feature ) can serve as a silver standard to develop a risk model , apply that model to patient without dementia code , and then select a case-detection threshold .\n",
      "codes - [compound] - ICD\n",
      "serve - [nsubj] - codes\n",
      "apply - [nsubj] - codes\n",
      "select - [nsubj] - codes\n",
      "codes - [nmod:in] - combination\n",
      "codes - [punct] - -RRB-\n",
      "9 39 80\n",
      "however , this study suggest that imperfect datum ( e.g. , icd code in combination with other ehr feature ) can serve as a silver standard to develop a risk model , apply that model to patient without dementia code , and then select a case-detection threshold .\n",
      "codes - [case] - without\n",
      "codes - [compound] - dementia\n",
      "patients - [nmod:without] - codes\n",
      "1 16 1037\n",
      "the symptom or characteristic of illness course necessary to develop reliable predictor be not available in code billing datum , but may be present in narrative electronic health record ( ehr ) discharge summary .\n",
      "data - [amod] - coded\n",
      "#################### ('data', 32.74) ####################\n",
      "1 7 2304\n",
      "in this paper , we propose a data mining method , call the Symptom-Herb-Diagnosis topic ( shot ) model , to automatically extract the common relationship among symptom , herb combination and diagnosis from large-scale cm clinical datum .\n",
      "method - [compound] - data\n",
      "4 19 1037\n",
      "the cohort be randomly split to derive a training ( 70 % ) and testing ( 30 % ) data set , and we train separate support vector machine model for baseline clinical feature alone , baseline feature plus common individual word and the above plus topic identify from the 75-topic LDA model .\n",
      "set - [compound] - data\n",
      "7 20 1037\n",
      "by include LDA topic , prediction of readmission , as measure by area under receiver-operating characteristic curve in the testing data set , be improve from baseline ( area under the curve 0.618 ) to baseline + 1000 word ( 0.682 ) to baseline +75 topic ( 0.784 ) .\n",
      "set - [compound] - data\n",
      "1 8 174\n",
      "in this study , we use social media data to examine the knowledge , attitude , and belief of as patient regard biologic therapy .\n",
      "data - [amod] - social\n",
      "data - [compound] - media\n",
      "used - [dobj] - data\n",
      "4 23 174\n",
      "to explore theme within the collection of post in a unsupervised manner , a latent Dirichlet allocation topic model be fit to the data set .\n",
      "set - [compound] - data\n",
      "6 14 174\n",
      "the topic be manually review to identify theme , which be confirm use thematic data analysis .\n",
      "analysis - [compound] - data\n",
      "#################### ('conclusion', 30.77) ####################\n",
      "10 0 115\n",
      "conclusion novel Big Data analytic such as lda offer the possibility of summarize personal goal without the need for conventional fixed-length measure and resource-intensive qualitative datum code .\n",
      "analytics - [compound] - Conclusions\n",
      "10 0 1402\n",
      "conclusion : experimental result reveal that we model achieve competitive performance in risk stratification in comparison with exist supervised approach .\n",
      "ROOT - [ROOT] - Conclusions\n",
      "Conclusions - [punct] - :\n",
      "Conclusions - [appos] - reveal\n",
      "Conclusions - [punct] - .\n",
      "12 0 174\n",
      "conclusion Social media reveal a dynamic range of theme govern as patient ' experience with and choice of biologic agent .\n",
      "media - [compound] - Conclusion\n",
      "15 0 338\n",
      "conclusion : this mixed-methods approach , integrate literature review , data-driven topic discovery , and human annotation , be a effective and rigorous way to develop a physician review topic taxonomy .\n",
      "ROOT - [ROOT] - Conclusions\n",
      "Conclusions - [punct] - :\n",
      "Conclusions - [dep] - way\n",
      "Conclusions - [punct] - .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### ('physician', 29.99) ####################\n",
      "4 34 1446\n",
      "discover treatment pattern , as actionable knowledge represent the best practice for most patient in most time of they treatment process , form the backbone of cp , and can be exploit to help physician better understand they specialty and learn from previous experience for cp analysis and improvement .\n",
      "help - [dobj] - physicians\n",
      "0 9 1402\n",
      "background and objective : risk stratification aim to provide physician with the accurate assessment of a patient 's clinical risk such that a individualized prevention or management strategy can be develop and deliver .\n",
      "provide - [dobj] - physicians\n",
      "physicians - [nmod:with] - assessment\n",
      "0 3 338\n",
      "background : web-based physician review be invaluable gold mine that merit further investigation .\n",
      "reviews - [compound] - physician\n",
      "1 9 338\n",
      "although many study have explore the text information of physician review , very few have focus on develop a systematic topic taxonomy embed in physician review .\n",
      "reviews - [compound] - physician\n",
      "1 24 338\n",
      "although many study have explore the text information of physician review , very few have focus on develop a systematic topic taxonomy embed in physician review .\n",
      "reviews - [compound] - physician\n",
      "2 5 338\n",
      "the first step toward mining physician review be to determine how the natural structure or dimension be embed in review .\n",
      "reviews - [compound] - physician\n",
      "4 17 338\n",
      "objective : this study aim to develop a hierarchical topic taxonomy to uncover the latent structure of physician review and illustrate its application for mining patient ' interest base on the propose taxonomy and algorithm .\n",
      "reviews - [compound] - physician\n",
      "5 5 338\n",
      "method : datum comprise 122,716 physician review , include review of 8501 doctor from a lead physician review website in China ( haodf.com ) , collect between 2007 and 2015 .\n",
      "reviews - [compound] - physician\n",
      "5 16 338\n",
      "method : datum comprise 122,716 physician review , include review of 8501 doctor from a lead physician review website in China ( haodf.com ) , collect between 2007 and 2015 .\n",
      "website - [compound] - physician\n",
      "6 20 338\n",
      "mixed method , include a literature review , data-driven-based topic discovery , and human annotation be use to develop the physician review topic taxonomy .\n",
      "taxonomy - [compound] - physician\n",
      "15 27 338\n",
      "conclusion : this mixed-methods approach , integrate literature review , data-driven topic discovery , and human annotation , be a effective and rigorous way to develop a physician review topic taxonomy .\n",
      "taxonomy - [compound] - physician\n",
      "#################### ('emr', 25.48) ####################\n",
      "1 19 1566\n",
      "by examine the clinical observation ( such as diagnosis , risk factor , and medication ) mention in longitudinal emr , we can use patient ' medical chronology to automatically predict the progression of they pathology .\n",
      "EMRs - [case] - in\n",
      "EMRs - [amod] - longitudinal\n",
      "mentioned - [nmod:in] - EMRs\n",
      "1 2 1101\n",
      "a hospital emr dataset typically consist of medical record of hospitalize patient .\n",
      "dataset - [compound] - EMR\n",
      "3 28 1101\n",
      "traditional topic model , such as latent dirichlet allocation ( lda ) and hierarchical dirichlet process ( hdp ) , can be employ to discover disease topic from emr datum by treat patient as document and diagnosis code as word .\n",
      "data - [compound] - EMR\n",
      "0 5 627\n",
      "obstetric electronic medical record ( emr ) contain massive amount of medical datum and health information .\n",
      "EMRs - [punct] - -LRB-\n",
      "records - [appos] - EMRs\n",
      "EMRs - [punct] - -RRB-\n",
      "1 8 627\n",
      "the information extraction and diagnosis assistant of obstetric emr be of great significance in improve the fertility level of the population .\n",
      "EMRs - [case] - of\n",
      "EMRs - [amod] - obstetric\n",
      "extraction - [nmod:of] - EMRs\n",
      "2 10 627\n",
      "the admit diagnosis in the first course record of the emr be reason from various source , such as chief complaint , auxiliary examination , and physical examination .\n",
      "EMR - [case] - of\n",
      "EMR - [det] - the\n",
      "record - [nmod:of] - EMR\n",
      "3 17 627\n",
      "this paper treat the diagnosis assistant as a multilabel classification task base on the analysis of obstetric emr .\n",
      "EMRs - [case] - of\n",
      "EMRs - [amod] - obstetric\n",
      "analyses - [nmod:of] - EMRs\n",
      "7 11 627\n",
      "additionally , the method can be use not only for obstetric emr but also for other medical record .\n",
      "EMRs - [cc:preconj] - only\n",
      "EMRs - [case] - for\n",
      "EMRs - [amod] - obstetric\n",
      "used - [nmod:for] - EMRs\n",
      "1 19 1446\n",
      "recently , with the rapid development of hospital information system , a large volume of electronic medical record ( emr ) have be produce , which provide a comprehensive source for cp analysis .\n",
      "EMRs - [punct] - -LRB-\n",
      "records - [appos] - EMRs\n",
      "EMRs - [punct] - -RRB-\n",
      "2 14 1446\n",
      "in this paper , we be concern with the problem of utilize the heterogeneous emr to assist cp analysis and improvement .\n",
      "EMRs - [det] - the\n",
      "EMRs - [amod] - heterogeneous\n",
      "utilizing - [dobj] - EMRs\n",
      "3 23 1446\n",
      "more specifically , we develop a probabilistic topic model to link patient feature and treatment behavior together to mine treatment pattern hide in emr .\n",
      "EMRs - [case] - in\n",
      "hidden - [nmod:in] - EMRs\n",
      "5 8 1446\n",
      "experimental result on a real collection of 985 emr collect from a chinese hospital show that the propose approach can effectively identify meaningful treatment pattern from emr .\n",
      "EMRs - [case] - of\n",
      "EMRs - [compound] - 985\n",
      "collection - [nmod:of] - EMRs\n",
      "EMRs - [acl] - collected\n",
      "5 26 1446\n",
      "experimental result on a real collection of 985 emr collect from a chinese hospital show that the propose approach can effectively identify meaningful treatment pattern from emr .\n",
      "EMRs - [case] - from\n",
      "patterns - [nmod:from] - EMRs\n",
      "#################### ('ehr', 25.13) ####################\n",
      "3 31 80\n",
      "this study seek to identify case of undiagnosed dementia by develop and validate a weakly supervise machine-learning approach that incorporate the analysis of both structured and unstructured electronic health record ( ehr ) data.methodsa topic modeling approach that include latent Dirichlet allocation , stable topic extraction , and random sampling be apply to VHA ehr .\n",
      "EHR - [punct] - -LRB-\n",
      "record - [appos] - EHR\n",
      "EHR - [punct] - -RRB-\n",
      "3 54 80\n",
      "this study seek to identify case of undiagnosed dementia by develop and validate a weakly supervise machine-learning approach that incorporate the analysis of both structured and unstructured electronic health record ( ehr ) data.methodsa topic modeling approach that include latent Dirichlet allocation , stable topic extraction , and random sampling be apply to VHA ehr .\n",
      "EHRs - [case] - to\n",
      "EHRs - [compound] - VHA\n",
      "applied - [nmod:to] - EHRs\n",
      "9 17 80\n",
      "however , this study suggest that imperfect datum ( e.g. , icd code in combination with other ehr feature ) can serve as a silver standard to develop a risk model , apply that model to patient without dementia code , and then select a case-detection threshold .\n",
      "features - [compound] - EHR\n",
      "10 13 80\n",
      "the study be one of the first to utilize both structured and unstructured ehr to develop risk score for the diagnosis of dementia .\n",
      "EHRs - [amod] - structured\n",
      "EHRs - [amod] - unstructured\n",
      "utilize - [dobj] - EHRs\n",
      "1 30 1037\n",
      "the symptom or characteristic of illness course necessary to develop reliable predictor be not available in code billing datum , but may be present in narrative electronic health record ( ehr ) discharge summary .\n",
      "EHR - [punct] - -LRB-\n",
      "record - [appos] - EHR\n",
      "EHR - [punct] - -RRB-\n",
      "9 12 1037\n",
      "topic modeling and related approach offer the potential to improve prediction use ehr , if generalizability can be establish in other clinical cohort .\n",
      "using - [dobj] - EHRs\n",
      "2 26 1402\n",
      "to this end , in this paper , we propose a new approach for risk stratification by explore a large volume of electronic health record ( ehr ) in a unsupervised fashion .\n",
      "EHRs - [punct] - -LRB-\n",
      "records - [appos] - EHRs\n",
      "EHRs - [punct] - -RRB-\n",
      "4 25 1402\n",
      "the propose PRSM recognize a patient clinical state as a probabilistic combination of latent sub-profile , and generate sub-profile-specific risk tier of patient from they ehr in a fully unsupervised fashion .\n",
      "EHRs - [case] - from\n",
      "EHRs - [nmod:poss] - their\n",
      "patients - [nmod:from] - EHRs\n",
      "13 19 1402\n",
      "we hypothesize that the propose framework can readily meet the demand for risk stratification from a large volume of ehr in a open-ended fashion .\n",
      "EHRs - [case] - of\n",
      "volume - [nmod:of] - EHRs\n",
      "#################### ('doctor', 24.57) ####################\n",
      "5 12 338\n",
      "method : datum comprise 122,716 physician review , include review of 8501 doctor from a lead physician review website in China ( haodf.com ) , collect between 2007 and 2015 .\n",
      "doctors - [case] - of\n",
      "doctors - [nummod] - 8501\n",
      "reviews - [nmod:of] - doctors\n"
     ]
    }
   ],
   "source": [
    "t_idx = 1\n",
    "for term in top_terms[t_idx]:\n",
    "    print(\"#\"*20, term, \"#\"*20)\n",
    "    for top_doc in top_docs[t_idx]:\n",
    "        _idx = top_doc[0]\n",
    "        if type(parse[_idx]) is str:\n",
    "            print(_idx, \"parse\", \"err\")\n",
    "            continue\n",
    "        sents = relation.convert_parse2lemma_sents(parse[_idx])\n",
    "        for sent_idx,sent in enumerate(sents):\n",
    "            topic_word_idxs = relation.get_word_idx(term[0], sent)\n",
    "            for i in topic_word_idxs:\n",
    "                print(sent_idx, i, _idx)\n",
    "                print(\" \".join(relation.convert_parse2lemma_sents(parse[_idx])[sent_idx]).strip())\n",
    "                rs = relation.extract_word_relation_from_sent(i, parse[_idx][\"sentences\"][sent_idx][\"enhancedPlusPlusDependencies\"])\n",
    "                for r in rs:\n",
    "                    print(relation.convert_relation2str(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(array([ 3,  5, 12, 13, 11,  4,  8,  7, 14,  0, 15, 10,  9,  6,  2,  1]), [0.043299350822176175, 0, 0.0, 3.7019845632225143, 0.9999999999999999, 2.440323084578266, 0, 0.24744436858488492, 0.2867838503998056, 0, 0, 1.1462952891213534, 1.9695384010090606, 1.312160801694111, 0.06781767476306744, 0], [1, 0, 1, 9, 1, 5, 0, 1, 1, 0, 0, 2, 2, 3, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "_idx = 1101\n",
    "sents = relation.convert_parse2lemma_sents(parse[_idx])\n",
    "print(relation.extract_important_sents(sents, [x[0] for x in top_terms[12]], [x[1] for x in top_terms[12]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Traditional topic models, such as latent Dirichlet allocation (LDA) and hierarchical Dirichlet process (HDP), can be employed to discover disease topics from EMR data by treating patients as documents and diagnosis codes as words.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent_tokenize(_raw[_idx])[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func get_phrases_by_pattern exec time: 0.5644145011901855\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('latent Dirichlet allocation', 1216),\n",
       " ('topic model', 692),\n",
       " ('experimental result', 431),\n",
       " ('topic modeling', 337),\n",
       " ('latent topic', 203),\n",
       " ('LDA model', 166),\n",
       " ('Dirichlet allocation', 158),\n",
       " ('short text', 123),\n",
       " ('Elsevier B.V. All rights', 108),\n",
       " ('sentiment analysis', 103),\n",
       " ('dirichlet allocation', 102),\n",
       " ('social media', 101),\n",
       " ('topic distribution', 93),\n",
       " ('social network', 91),\n",
       " ('recent year', 86),\n",
       " ('large number', 82),\n",
       " ('Elsevier Ltd.', 81),\n",
       " ('text mining', 79),\n",
       " ('Gibbs sampling', 77),\n",
       " ('case study', 73),\n",
       " ('information retrieval', 70),\n",
       " ('probabilistic topic model', 70),\n",
       " ('natural language processing', 66),\n",
       " ('previous work', 66),\n",
       " ('latent Dirichlet Allocation', 65),\n",
       " ('large amount', 61),\n",
       " ('visual word', 60),\n",
       " ('latent Dirichlet allocation model', 58),\n",
       " ('novel approach', 57),\n",
       " ('extensive experiment', 54),\n",
       " ('machine learning', 53),\n",
       " ('big datum', 53),\n",
       " ('better performance', 53),\n",
       " ('new method', 52),\n",
       " ('online review', 51),\n",
       " ('important role', 51),\n",
       " ('language model', 51),\n",
       " ('semantic analysis', 51),\n",
       " ('text document', 50),\n",
       " ('generative model', 50),\n",
       " ('textual datum', 50),\n",
       " ('different type', 49),\n",
       " ('different topic', 48),\n",
       " ('datum set', 46),\n",
       " ('new approach', 45),\n",
       " ('training datum', 45),\n",
       " ('source code', 45),\n",
       " ('probability distribution', 45),\n",
       " ('topic detection', 44),\n",
       " ('latent variable', 43),\n",
       " ('topic modelling', 42),\n",
       " ('large collection', 42),\n",
       " ('hidden topic', 41),\n",
       " ('word embedding', 41),\n",
       " ('text datum', 40),\n",
       " ('text classification', 40),\n",
       " ('large volume', 40),\n",
       " ('state-of-the-art method', 39),\n",
       " ('user interest', 39),\n",
       " ('significant improvement', 38),\n",
       " ('other hand', 37),\n",
       " ('support vector machine', 37),\n",
       " ('novel method', 37),\n",
       " ('natural language', 37),\n",
       " ('probabilistic model', 37),\n",
       " ('Elsevier Inc.', 36),\n",
       " ('challenging task', 36),\n",
       " ('latent Dirichlet Allocation model', 36),\n",
       " ('research topic', 34),\n",
       " ('text categorization', 34),\n",
       " ('model parameter', 34),\n",
       " ('document collection', 34),\n",
       " ('text corpora', 33),\n",
       " ('proposed system', 33),\n",
       " ('real-world dataset', 32),\n",
       " ('news article', 32),\n",
       " ('previous study', 32),\n",
       " ('prior knowledge', 32),\n",
       " ('topic modeling technique', 32),\n",
       " ('topic space', 32),\n",
       " ('same time', 31),\n",
       " ('user preference', 30),\n",
       " ('text analysis', 30),\n",
       " ('unsupervised manner', 30),\n",
       " ('support Vector machine', 29),\n",
       " ('semantic information', 29),\n",
       " ('sentiment classification', 29),\n",
       " ('bug report', 29),\n",
       " ('Twitter datum', 29),\n",
       " ('recommender system', 29),\n",
       " ('topic analysis', 29),\n",
       " ('classification accuracy', 28),\n",
       " ('hot topic', 28),\n",
       " ('empirical study', 27),\n",
       " ('neural network', 27),\n",
       " ('latent Dirichlet allocation topic model', 27),\n",
       " ('Semantic analysis', 27),\n",
       " ('recommendation system', 27),\n",
       " ('other method', 26),\n",
       " ('rapid development', 26),\n",
       " ('traditional method', 26),\n",
       " ('baseline method', 26),\n",
       " ('small number', 26),\n",
       " ('experiment result', 26),\n",
       " ('real dataset', 26),\n",
       " ('huge amount', 26),\n",
       " ('web service', 26),\n",
       " ('topic discovery', 26),\n",
       " ('parameter estimation', 26),\n",
       " ('many application', 25),\n",
       " ('topic modeling approach', 25),\n",
       " ('contextual information', 25),\n",
       " ('k-means clustering', 25),\n",
       " ('unsupervised learning', 25),\n",
       " ('topic modeling algorithm', 25),\n",
       " ('topic extraction', 25),\n",
       " ('real datum', 25),\n",
       " ('user behavior', 25),\n",
       " ('classification task', 24),\n",
       " ('word distribution', 24),\n",
       " ('useful information', 24),\n",
       " ('clustering algorithm', 24),\n",
       " ('document classification', 24),\n",
       " ('mixture model', 24),\n",
       " ('unstructured datum', 23),\n",
       " ('topic feature', 23),\n",
       " ('variational inference', 23),\n",
       " ('data set', 23),\n",
       " ('topic evolution', 23),\n",
       " ('specific topic', 22),\n",
       " ('unsupervised method', 22),\n",
       " ('relevant topic', 22),\n",
       " ('topic coherence', 22),\n",
       " ('much attention', 22),\n",
       " ('text mining technique', 22),\n",
       " ('clustering method', 22),\n",
       " ('high accuracy', 22),\n",
       " ('lda model', 22),\n",
       " ('classification performance', 22),\n",
       " ('scene classification', 22),\n",
       " ('topic structure', 21),\n",
       " ('wide range', 21),\n",
       " ('public opinion', 21),\n",
       " ('statistical model', 21),\n",
       " ('datum mining', 21),\n",
       " ('topic modeling method', 21),\n",
       " ('rapid growth', 21),\n",
       " ('conventional method', 21),\n",
       " ('dimensionality reduction', 21),\n",
       " ('naive baye', 20),\n",
       " ('empirical result', 20),\n",
       " ('main topic', 20),\n",
       " ('better understanding', 20),\n",
       " ('latent topic model', 20),\n",
       " ('social media data', 20),\n",
       " ('topic information', 20),\n",
       " ('class label', 20),\n",
       " ('topic proportion', 20),\n",
       " ('video sequence', 20),\n",
       " ('software system', 20),\n",
       " ('similarity measure', 19),\n",
       " ('speech recognition', 19),\n",
       " ('feature extraction', 19),\n",
       " ('statistical method', 19),\n",
       " ('posterior distribution', 19),\n",
       " ('document representation', 19),\n",
       " ('topic vector', 19),\n",
       " ('different level', 19),\n",
       " ('good performance', 19),\n",
       " ('opinion mining', 19),\n",
       " ('search engine', 19),\n",
       " ('text corpus', 19),\n",
       " ('web page', 19),\n",
       " ('urban area', 19),\n",
       " ('LDA algorithm', 19),\n",
       " ('first step', 18),\n",
       " ('textual information', 18),\n",
       " ('large set', 18),\n",
       " ('scientific literature', 18),\n",
       " ('different aspect', 18),\n",
       " ('Probabilistic topic model', 18),\n",
       " ('% improvement', 18),\n",
       " ('classification method', 18),\n",
       " ('main contribution', 18),\n",
       " ('important task', 18),\n",
       " ('systematic review', 18),\n",
       " ('promising result', 18),\n",
       " ('topic word', 18),\n",
       " ('feature space', 18),\n",
       " ('extract topic', 18),\n",
       " ('tv program', 18),\n",
       " ('e. g.', 18),\n",
       " ('previous research', 17),\n",
       " ('LDA topic model', 17),\n",
       " ('machine learning method', 17),\n",
       " ('Topic Model', 17),\n",
       " ('valuable information', 17),\n",
       " ('semantic relationship', 17),\n",
       " ('novel framework', 17),\n",
       " ('content analysis', 17),\n",
       " ('spatial information', 17),\n",
       " ('new model', 17),\n",
       " ('Social media', 17),\n",
       " ('dirichlet distribution', 17),\n",
       " ('image classification', 17),\n",
       " ('semantic relation', 17),\n",
       " ('document clustering', 17),\n",
       " ('better result', 17),\n",
       " ('tag recommendation', 17),\n",
       " ('topic trend', 17),\n",
       " ('multinomial distribution', 17),\n",
       " ('computer vision', 17),\n",
       " ('semantic feature', 17),\n",
       " ('effective method', 17),\n",
       " ('experimental evaluation', 17),\n",
       " ('meaningful topic', 17),\n",
       " ('lda method', 17),\n",
       " ('classification algorithm', 16),\n",
       " ('discrete datum', 16),\n",
       " ('textual document', 16),\n",
       " ('relevant document', 16),\n",
       " ('large dataset', 16),\n",
       " ('different domain', 16),\n",
       " ('state-of-the-art approach', 16),\n",
       " ('policy maker', 16),\n",
       " ('inference algorithm', 16),\n",
       " ('graphical model', 16),\n",
       " ('social network analysis', 16),\n",
       " ('daily life', 16),\n",
       " ('information need', 16),\n",
       " ('bayesian model', 16),\n",
       " ('multiple topic', 16),\n",
       " ('matrix factorization', 16),\n",
       " ('new topic model', 16),\n",
       " ('small set', 16),\n",
       " ('higher accuracy', 16),\n",
       " ('training set', 16),\n",
       " ('probabilistic topic modeling', 15),\n",
       " ('deep learning', 15),\n",
       " ('United States', 15),\n",
       " ('generative process', 15),\n",
       " ('novel model', 15),\n",
       " ('Stack Overflow', 15),\n",
       " ('visual feature', 15),\n",
       " ('new topic', 15),\n",
       " ('user-generated content', 15),\n",
       " ('textual content', 15),\n",
       " ('allocation model', 15),\n",
       " ('predictive performance', 15),\n",
       " ('semantic meaning', 15),\n",
       " ('word frequency', 15),\n",
       " ('data analysis', 15),\n",
       " ('benchmark dataset', 15),\n",
       " ('traditional approach', 15),\n",
       " ('search query', 15),\n",
       " ('community structure', 15),\n",
       " ('relevant information', 15),\n",
       " ('social interaction', 15),\n",
       " ('latent Semantic analysis', 15),\n",
       " ('feature selection', 15),\n",
       " ('text mining method', 14),\n",
       " ('traditional topic model', 14),\n",
       " ('latent Dirichlet allocation method', 14),\n",
       " ('vector space model', 14),\n",
       " ('other approach', 14),\n",
       " ('large scale', 14),\n",
       " ('behavior pattern', 14),\n",
       " ('real time', 14),\n",
       " ('past decade', 14),\n",
       " ('topic-word distribution', 14),\n",
       " ('topic correlation', 14),\n",
       " ('real-world datum', 14),\n",
       " ('hierarchical structure', 14),\n",
       " ('large corpora', 14),\n",
       " ('large corpus', 14),\n",
       " ('lda topic', 14),\n",
       " ('previous method', 14),\n",
       " ('important information', 14),\n",
       " ('best result', 14),\n",
       " ('blog post', 14),\n",
       " ('research papers', 13),\n",
       " ('lda analysis', 13),\n",
       " ('artificial intelligence', 13),\n",
       " ('coherent topic', 13),\n",
       " ('topic probability', 13),\n",
       " ('word vector', 13),\n",
       " ('research trend', 13),\n",
       " ('future research', 13),\n",
       " ('powerful tool', 13),\n",
       " ('research field', 13),\n",
       " ('more attention', 13),\n",
       " ('efficient method', 13),\n",
       " ('important issue', 13),\n",
       " ('computer science', 13),\n",
       " ('human activity', 13),\n",
       " ('word co-occurrence', 13),\n",
       " ('competitive performance', 13),\n",
       " ('human behavior', 13),\n",
       " ('novel topic model', 13),\n",
       " ('best performance', 13),\n",
       " ('latent Dirichlet allocation algorithm', 13),\n",
       " ('satellite image', 13),\n",
       " ('document level', 13),\n",
       " ('text collection', 13),\n",
       " ('topic representation', 13),\n",
       " ('user query', 13),\n",
       " ('review text', 12),\n",
       " ('term frequency', 12),\n",
       " ('research community', 12),\n",
       " ('such datum', 12),\n",
       " ('information technology', 12),\n",
       " ('customer satisfaction', 12),\n",
       " ('product review', 12),\n",
       " ('climate change', 12),\n",
       " ('heart disease', 12),\n",
       " ('learning method', 12),\n",
       " ('different language', 12),\n",
       " ('inference method', 12),\n",
       " ('mobile device', 12),\n",
       " ('similar topic', 12),\n",
       " ('context information', 12),\n",
       " ('social media post', 12),\n",
       " ('appropriate number', 12),\n",
       " ('online community', 12),\n",
       " ('topic hierarchy', 12),\n",
       " ('social event', 12),\n",
       " ('latent semantics', 12),\n",
       " ('vast amount', 12),\n",
       " ('latent structure', 12),\n",
       " ('Cohen d', 12),\n",
       " ('Delta u', 12),\n",
       " ('specific domain', 12),\n",
       " ('standard lda', 12),\n",
       " ('posterior inference', 12),\n",
       " ('topic number', 12),\n",
       " ('new feature', 12),\n",
       " ('semantic gap', 12),\n",
       " ('topic cluster', 12),\n",
       " ('feature word', 12),\n",
       " ('important problem', 12),\n",
       " ('general framework', 12),\n",
       " ('abnormal event', 12),\n",
       " ('hierarchical model', 12),\n",
       " ('color harmony model', 12),\n",
       " ('text information', 11),\n",
       " ('major challenge', 11),\n",
       " ('superior performance', 11),\n",
       " ('various aspect', 11),\n",
       " ('massive amount', 11),\n",
       " ('various topic', 11),\n",
       " ('unsupervised approach', 11),\n",
       " ('effective way', 11),\n",
       " ('temporal trend', 11),\n",
       " ('good result', 11),\n",
       " ('great challenge', 11),\n",
       " ('Markov chain Monte Carlo', 11),\n",
       " ('synthetic datum', 11),\n",
       " ('future work', 11),\n",
       " ('real world', 11),\n",
       " ('real world dataset', 11),\n",
       " ('statistical analysis', 11),\n",
       " ('other topic model', 11),\n",
       " ('dialogue system', 11),\n",
       " ('topic similarity', 11),\n",
       " ('long text', 11),\n",
       " ('semantic similarity', 11),\n",
       " ('vector space', 11),\n",
       " ('different method', 11),\n",
       " ('datum source', 11),\n",
       " ('search result', 11),\n",
       " ('clinical pathway', 11),\n",
       " ('evaluation result', 11),\n",
       " ('similar user', 11),\n",
       " ('bag-of-word assumption', 11),\n",
       " ('several topic', 11),\n",
       " ('time series', 11),\n",
       " ('user profile', 11),\n",
       " ('service recommendation', 11),\n",
       " ('unlabeled datum', 11),\n",
       " ('topic identification', 11),\n",
       " ('various type', 11),\n",
       " ('topic level', 11),\n",
       " ('image datum', 11),\n",
       " ('such model', 11),\n",
       " ('high resolution', 11),\n",
       " ('motion pattern', 11),\n",
       " ('image clustering', 11),\n",
       " ('singular value decomposition', 11),\n",
       " ('image annotation', 11),\n",
       " ('other user', 11),\n",
       " ('human action', 11),\n",
       " ('Topic model', 11),\n",
       " ('atomic activity', 11),\n",
       " ('social datum', 10),\n",
       " ('topic quality', 10),\n",
       " ('consumer review', 10),\n",
       " ('popular topic', 10),\n",
       " ('text feature', 10),\n",
       " ('user review', 10),\n",
       " ('such information', 10),\n",
       " ('non-negative Matrix factorization', 10),\n",
       " ('high dimensionality', 10),\n",
       " ('main idea', 10),\n",
       " ('efficient way', 10),\n",
       " ('dirichlet allocation model', 10),\n",
       " ('Practical implication', 10),\n",
       " ('word representation', 10),\n",
       " ('most case', 10),\n",
       " ('present study', 10),\n",
       " ('public dataset', 10),\n",
       " ('topic label', 10),\n",
       " ('many topic', 10),\n",
       " ('user experience', 10),\n",
       " ('electronic health record', 10),\n",
       " ('fault diagnosis', 10),\n",
       " ('individual user', 10),\n",
       " ('effective approach', 10),\n",
       " ('related topic', 10),\n",
       " ('real-world application', 10),\n",
       " ('test datum', 10),\n",
       " ('online datum', 10),\n",
       " ('word order', 10),\n",
       " ('fundamental problem', 10),\n",
       " ('random mixture', 10),\n",
       " ('online algorithm', 10),\n",
       " ('document corpus', 10),\n",
       " ('large document collection', 10),\n",
       " ('Gibbs sampling algorithm', 10),\n",
       " ('support Vector Machine', 10),\n",
       " ('baseline model', 10),\n",
       " ('mutual information', 10),\n",
       " ('few study', 10),\n",
       " ('domain knowledge', 10),\n",
       " ('point cloud', 10),\n",
       " ('k-means algorithm', 10),\n",
       " ('hierarchical clustering', 10),\n",
       " ('empirical evaluation', 10),\n",
       " ('image segmentation', 10),\n",
       " ('content information', 10),\n",
       " ('discriminative power', 10),\n",
       " ('temporal information', 10),\n",
       " ('multimodal datum', 10),\n",
       " ('retrieval performance', 10),\n",
       " ('hierarchical Dirichlet', 10),\n",
       " ('learning process', 10),\n",
       " ('certain topic', 10),\n",
       " ('evaluation experiment', 10),\n",
       " ('semantic space', 10),\n",
       " ('word image', 10),\n",
       " ('original LDA', 10),\n",
       " ('challenging issue', 10),\n",
       " ('customer review', 10),\n",
       " ('genetic algorithm', 10),\n",
       " ('image collection', 10),\n",
       " ('digital library', 10),\n",
       " ('low-level feature', 10),\n",
       " ('oov pn', 10),\n",
       " ('query photo', 10),\n",
       " ('topic assignment', 10),\n",
       " ('bayesian inference', 10),\n",
       " ('text stream', 10),\n",
       " ('word error rate', 10),\n",
       " ('conversational content', 10),\n",
       " ('large text corpora', 10),\n",
       " ('author-topic model', 10),\n",
       " ('super user', 10),\n",
       " ('trained model', 10),\n",
       " ('object recognition', 10),\n",
       " ('mobility pattern', 10),\n",
       " ('hybrid approach', 10),\n",
       " ('feature location', 10),\n",
       " ('product feature', 10),\n",
       " ('different word', 9),\n",
       " ('machine learning technique', 9),\n",
       " ('scientific publication', 9),\n",
       " ('other model', 9),\n",
       " ('conventional topic model', 9),\n",
       " ('sparsity problem', 9),\n",
       " ('negative sentiment', 9),\n",
       " ('research work', 9),\n",
       " ('such topic', 9),\n",
       " ('prediction accuracy', 9),\n",
       " ('image retrieval', 9),\n",
       " ('different way', 9),\n",
       " ('meaningful information', 9),\n",
       " ('recommendation list', 9),\n",
       " ('datum collection', 9),\n",
       " ('topic-based representation', 9),\n",
       " ('promising performance', 9),\n",
       " ('cosine similarity', 9),\n",
       " ('structured datum', 9),\n",
       " ('knowledge discovery', 9),\n",
       " ('variable model', 9),\n",
       " ('current study', 9),\n",
       " ('experimental study', 9),\n",
       " ('other word', 9),\n",
       " ('various domain', 9),\n",
       " ('different kind', 9),\n",
       " ('key factor', 9),\n",
       " ('qualitative analysis', 9),\n",
       " ('predictive model', 9),\n",
       " ('information gain', 9),\n",
       " ('textual feature', 9),\n",
       " ('social media platform', 9),\n",
       " ('generative topic model', 9),\n",
       " ('semantic relatedness', 9),\n",
       " ('corresponding topic', 9),\n",
       " ('prediction performance', 9),\n",
       " ('classification problem', 9),\n",
       " ('clinical report', 9),\n",
       " ('standard topic model', 9),\n",
       " ('latent Dirichlet Allocation Model', 9),\n",
       " ('long time', 9),\n",
       " ('promising approach', 9),\n",
       " ('comparative analysis', 9),\n",
       " ('weighting scheme', 9),\n",
       " ('semantic concept', 9),\n",
       " ('other type', 9),\n",
       " ('state-of-the-art performance', 9),\n",
       " ('recommendation approach', 9),\n",
       " ('Wikipedia article', 9),\n",
       " ('patent datum', 9),\n",
       " ('different number', 9),\n",
       " ('vector representation', 9),\n",
       " ('different dataset', 9),\n",
       " ('topic network', 9),\n",
       " ('training corpus', 9),\n",
       " ('multimodal information', 9),\n",
       " ('time complexity', 9),\n",
       " ('latent factor model', 9),\n",
       " ('document modeling', 9),\n",
       " ('semantic topic', 9),\n",
       " ('training dataset', 9),\n",
       " ('Vector Space Model', 9),\n",
       " ('Gibbs sampler', 9),\n",
       " ('efficient algorithm', 9),\n",
       " ('% accuracy', 9),\n",
       " ('vocabulary size', 9),\n",
       " ('proper name', 9),\n",
       " ('John Wiley', 9),\n",
       " ('variational method', 9),\n",
       " ('domain expert', 9),\n",
       " ('web document', 9),\n",
       " ('preliminary result', 9),\n",
       " ('second method', 9),\n",
       " ('scene recognition', 9),\n",
       " ('hsr imagery', 9),\n",
       " ('previous approach', 9),\n",
       " ('training image', 9),\n",
       " ('image feature', 9),\n",
       " ('traffic incident', 9),\n",
       " ('current research', 8),\n",
       " ('large quantity', 8),\n",
       " ('related word', 8),\n",
       " ('key topic', 8),\n",
       " ('regression model', 8),\n",
       " ('news topic', 8),\n",
       " ('explosive growth', 8),\n",
       " ('key feature', 8),\n",
       " ('recall rate', 8),\n",
       " ('sentiment word', 8),\n",
       " ('overall rating', 8),\n",
       " ('non-negative matrix factorization', 8),\n",
       " ('many study', 8),\n",
       " ('logistic regression', 8),\n",
       " ('service quality', 8),\n",
       " ('word cloud', 8),\n",
       " ('data-driven approach', 8),\n",
       " ('deep learning method', 8),\n",
       " ('software artifact', 8),\n",
       " ('personalized recommendation', 8),\n",
       " ('different region', 8),\n",
       " ('other topic', 8),\n",
       " ('core topic', 8),\n",
       " ('semantic structure', 8),\n",
       " ('text mining approach', 8),\n",
       " ('semantic coherence', 8),\n",
       " ('high level', 8),\n",
       " ('recent study', 8),\n",
       " ('further research', 8),\n",
       " ('quantitative analysis', 8),\n",
       " ('classification model', 8),\n",
       " ('new way', 8),\n",
       " ('clustering approach', 8),\n",
       " ('feature vector', 8),\n",
       " ('latent aspect', 8),\n",
       " ('feature set', 8),\n",
       " ('large datum set', 8),\n",
       " ('first study', 8),\n",
       " ('sparse coding', 8),\n",
       " ('document analysis', 8),\n",
       " ('topic-based approach', 8),\n",
       " ('Dirichlet Allocation model', 8),\n",
       " ('topic-based model', 8),\n",
       " ('difficult task', 8),\n",
       " ('important source', 8),\n",
       " ('several study', 8),\n",
       " ('text clustering', 8),\n",
       " ('different weight', 8),\n",
       " ('low-dimensional representation', 8),\n",
       " ('machine learning algorithm', 8),\n",
       " ('satisfactory result', 8),\n",
       " ('major topic', 8),\n",
       " ('input datum', 8),\n",
       " ('unsupervised machine', 8),\n",
       " ('weak hypothesis', 8),\n",
       " ('diabetes mellitus', 8),\n",
       " ('encouraging result', 8),\n",
       " ('different perspective', 8),\n",
       " ('topic classification', 8),\n",
       " ('new type', 8),\n",
       " ('huge number', 8),\n",
       " ('optimal number', 8),\n",
       " ('Dirichlet Allocation', 8),\n",
       " ('unseen datum', 8),\n",
       " ('Hidden Markov Model', 8),\n",
       " ('probabilistic distribution', 8),\n",
       " ('classification result', 8),\n",
       " ('human topic ranking', 8),\n",
       " ('Dirichlet prior', 8),\n",
       " ('challenging problem', 8),\n",
       " ('Markov model', 8),\n",
       " ('citation network', 8),\n",
       " ('program comprehension', 8),\n",
       " ('unstructured text', 8),\n",
       " ('community detection', 8),\n",
       " ('state-of-the-art baseline', 8),\n",
       " ('newspaper article', 8),\n",
       " ('clustering technique', 8),\n",
       " ('input image', 8),\n",
       " ('potential topic', 8),\n",
       " ('latent topic structure', 8),\n",
       " ('unsupervised topic model', 8),\n",
       " ('alternative method', 8),\n",
       " ('Dirichlet process', 8),\n",
       " ('international student', 8),\n",
       " ('semantic indexing', 8),\n",
       " ('different user', 8),\n",
       " ('self-driving car', 8),\n",
       " ('recommendation performance', 8),\n",
       " ('software maintenance', 8),\n",
       " ('topic modelling method', 8),\n",
       " ('valuable source', 8),\n",
       " ('latent factor', 8),\n",
       " ('Twitter user', 8),\n",
       " ('social tag', 8),\n",
       " ('web image', 8),\n",
       " ('Information Retrieval', 8),\n",
       " ('important part', 8),\n",
       " ('higher precision', 8),\n",
       " ('topic change', 8),\n",
       " ('Markov chain', 8),\n",
       " ('user context', 8),\n",
       " ('first method', 8),\n",
       " ('alternative approach', 8),\n",
       " ('active user', 8),\n",
       " ('variational baye', 8),\n",
       " ('common crawl', 8),\n",
       " ('classification approach', 8),\n",
       " ('similar interest', 8),\n",
       " ('software repository', 8),\n",
       " ('treatment pattern', 8),\n",
       " ('lda-based technique', 8),\n",
       " ('time point', 8),\n",
       " ('probability model', 8),\n",
       " ('natural language text', 8),\n",
       " ('document retrieval', 8),\n",
       " ('tv user', 8),\n",
       " ('datum sparseness', 8),\n",
       " ('optical flow', 8),\n",
       " ('Great East Japan Earthquake', 8),\n",
       " ('life style', 8),\n",
       " ('scene class', 8),\n",
       " ('activity routine', 8),\n",
       " ('reliable user', 8),\n",
       " ('clinical trial protocol', 8),\n",
       " ('tag-topic model', 8),\n",
       " ('cluster ensemble', 8),\n",
       " ('visual analytic', 7),\n",
       " ('tourist attraction', 7),\n",
       " ('common topic', 7),\n",
       " ('time period', 7),\n",
       " ('second step', 7),\n",
       " ('thematic structure', 7),\n",
       " ('future study', 7),\n",
       " ('user study', 7),\n",
       " ('information propagation', 7),\n",
       " ('co-occurrence pattern', 7),\n",
       " ('novel technique', 7),\n",
       " ('pattern recognition', 7),\n",
       " ('simulation study', 7),\n",
       " ('retrieval result', 7),\n",
       " ('urban planning', 7),\n",
       " ('machine translation', 7),\n",
       " ('software project', 7),\n",
       " ('significant challenge', 7),\n",
       " ('state-of-the-art model', 7),\n",
       " ('document frequency', 7),\n",
       " ('Linguistic Inquiry', 7),\n",
       " ('dominant topic', 7),\n",
       " ('cold topic', 7),\n",
       " ('research gap', 7),\n",
       " ('latent interest', 7),\n",
       " ('online user', 7),\n",
       " ('empirical analysis', 7),\n",
       " ('current state', 7),\n",
       " ('mixture distribution', 7),\n",
       " ('available online', 7),\n",
       " ('hidden knowledge', 7),\n",
       " ('human perception', 7),\n",
       " ('trend analysis', 7),\n",
       " ('mental health', 7),\n",
       " ('computational efficiency', 7),\n",
       " ('transfer learning', 7),\n",
       " ('relevant study', 7),\n",
       " ('practical implication', 7),\n",
       " ('current approach', 7),\n",
       " ('effective tool', 7),\n",
       " ('various method', 7),\n",
       " ('urgent care center', 7),\n",
       " ('Dirichlet allocation model', 7),\n",
       " ('predictive power', 7),\n",
       " ('service description', 7),\n",
       " ('problem list', 7),\n",
       " ('great deal', 7),\n",
       " ('patent document', 7),\n",
       " ('naive Bayes', 7),\n",
       " ('same event', 7),\n",
       " ('allocation topic model', 7),\n",
       " ('target user', 7),\n",
       " ('mining process', 7),\n",
       " ('many method', 7),\n",
       " ('visual information', 7),\n",
       " ('semantic level', 7),\n",
       " ('embedding vector', 7),\n",
       " ('spectral clustering', 7),\n",
       " ('multiple feature', 7),\n",
       " ('recent work', 7),\n",
       " ('last decade', 7),\n",
       " ('fraud detection', 7),\n",
       " ('sentiment polarity', 7),\n",
       " ('significant role', 7),\n",
       " ('input document', 7),\n",
       " ('limited number', 7),\n",
       " ('probabilistic modeling', 7),\n",
       " ('potential value', 7),\n",
       " ('text message', 7),\n",
       " ('supervised learning', 7),\n",
       " ('unsupervised fashion', 7),\n",
       " ('linguistic feature', 7),\n",
       " ('fake review', 7),\n",
       " ('other disease', 7),\n",
       " ('ground truth', 7),\n",
       " ('such method', 7),\n",
       " ('proposed approach', 7),\n",
       " ('supervised information', 7),\n",
       " ('useful insight', 7),\n",
       " ('many case', 7),\n",
       " ('literature review', 7),\n",
       " ('heterogeneous network', 7),\n",
       " ('document-topic distribution', 7),\n",
       " ('semantic representation', 7),\n",
       " ('overall accuracy', 7),\n",
       " ('new perspective', 7),\n",
       " ('full-text datum', 7),\n",
       " ('different size', 7),\n",
       " ('bow representation', 7),\n",
       " ('computational linguistics', 7),\n",
       " ('social image', 7),\n",
       " ('many researcher', 7),\n",
       " ('content-based method', 7),\n",
       " ('word level', 7),\n",
       " ('real world datum', 7),\n",
       " ('further analysis', 7),\n",
       " ('training sample', 7),\n",
       " ('new challenge', 7),\n",
       " ('receiver operating', 7),\n",
       " ('language modeling', 7),\n",
       " ('image patch', 7),\n",
       " ('research area', 7),\n",
       " ('different feature', 7),\n",
       " ('Big Data', 7),\n",
       " ('complementary information', 7),\n",
       " ('topic segmentation', 7),\n",
       " ('open problem', 7),\n",
       " ('new algorithm', 7),\n",
       " ('recent research', 7),\n",
       " ('further improvement', 7),\n",
       " ('space complexity', 7),\n",
       " ('software developer', 7),\n",
       " ('software evolution', 7),\n",
       " ('service composition', 7),\n",
       " ('available datum', 7),\n",
       " ('traditional LDA model', 7),\n",
       " ('topic clustering', 7),\n",
       " ('indonesian text', 7),\n",
       " ('web content', 7),\n",
       " ('Wall Street Journal', 7),\n",
       " ('various source', 7),\n",
       " ('textual review', 7),\n",
       " ('computational time', 7),\n",
       " ('hsr image', 7),\n",
       " ('computational model', 7),\n",
       " ('spatial pattern', 7),\n",
       " ('object concept', 7),\n",
       " ('recent advance', 7),\n",
       " ('sentiment lexicon', 7),\n",
       " ('activity recognition', 7),\n",
       " ('particular topic', 7),\n",
       " ('time dimension', 7),\n",
       " ('multi-document summarization', 7),\n",
       " ('expert finding system', 7),\n",
       " ('other domain', 7),\n",
       " ('keyword extraction', 7),\n",
       " ('automatic speech recognition', 7),\n",
       " ('mixture weight', 7),\n",
       " ('new question', 7),\n",
       " ('novel algorithm', 7),\n",
       " ('latent Dirichlet', 7),\n",
       " ('risk stratification', 7),\n",
       " ('mobile application', 7),\n",
       " ('eligibility criterion', 7),\n",
       " ('temporal feature', 7),\n",
       " ('wide variety', 7),\n",
       " ('posterior probability', 7),\n",
       " ('optimization problem', 7),\n",
       " ('automatic transcription', 7),\n",
       " ('mesh term', 7),\n",
       " ('patient trace', 7),\n",
       " ('lda-based method', 7),\n",
       " ('document cluster', 7),\n",
       " ('patient record', 7),\n",
       " ('risk type', 7),\n",
       " ('datum point', 7),\n",
       " ('web api', 7),\n",
       " ('same topic', 7),\n",
       " ('feature descriptor', 7),\n",
       " ('dimension reduction', 7),\n",
       " ('curriculum learning', 7),\n",
       " ('service ecosystem', 7),\n",
       " ('social circle', 7),\n",
       " ('target domain', 7),\n",
       " ('social tagging system', 7),\n",
       " ('human motion', 7),\n",
       " ('unigram scaling', 7),\n",
       " ('co-occurrence event', 7),\n",
       " ('drive topic', 7),\n",
       " ('Kullback-Leibler divergence', 7),\n",
       " ('unsupervised topic modeling', 6),\n",
       " ('research domain', 6),\n",
       " ('various application', 6),\n",
       " ('destination image', 6),\n",
       " ('overall performance', 6),\n",
       " ('local context', 6),\n",
       " ('real datum set', 6),\n",
       " ('CNN model', 6),\n",
       " ('news text', 6),\n",
       " ('precision rate', 6),\n",
       " ('digital document', 6),\n",
       " ('benchmark datum set', 6),\n",
       " ('unsupervised way', 6),\n",
       " ('learning model', 6),\n",
       " ('last year', 6),\n",
       " ('time frame', 6),\n",
       " ('research question', 6),\n",
       " ('higher performance', 6),\n",
       " ('temporal dynamics', 6),\n",
       " ('large-scale dataset', 6),\n",
       " ('k-means clustering algorithm', 6),\n",
       " ('Word Count', 6),\n",
       " ('supervised lda', 6),\n",
       " ('compact representation', 6),\n",
       " ('annotation performance', 6),\n",
       " ('intelligent system', 6),\n",
       " ('many people', 6),\n",
       " ('topic modeling process', 6),\n",
       " ('integral part', 6),\n",
       " ('% ci', 6),\n",
       " ('text representation', 6),\n",
       " ('acoustic feature', 6),\n",
       " ('sample size', 6),\n",
       " ('datum size', 6),\n",
       " ('patent analysis', 6),\n",
       " ('software engineering', 6),\n",
       " ('network analysis', 6),\n",
       " ('distinct topic', 6),\n",
       " ('similarity metric', 6),\n",
       " ('popular topic model', 6),\n",
       " ('public sentiment', 6),\n",
       " ('spatial distribution', 6),\n",
       " ('dynamic topic model', 6),\n",
       " ('latent topic distribution', 6),\n",
       " ('correlation analysis', 6),\n",
       " ('domain adaptation', 6),\n",
       " ('main challenge', 6),\n",
       " ('point cluster', 6),\n",
       " ('decision making', 6),\n",
       " ('word model', 6),\n",
       " ('significant change', 6),\n",
       " ('high quality', 6),\n",
       " ('latent pattern', 6),\n",
       " ('comprehensive experiment', 6),\n",
       " ('quantitative evaluation', 6),\n",
       " ('specific aspect', 6),\n",
       " ('service discovery', 6),\n",
       " ('relevant feature', 6),\n",
       " ('main goal', 6),\n",
       " ('keyword search', 6),\n",
       " ('important tool', 6),\n",
       " ('cryptocurrency investment', 6),\n",
       " ('nonnegative matrix factorization', 6),\n",
       " ('privacy policy', 6),\n",
       " ('feature representation', 6),\n",
       " ('recommendation model', 6),\n",
       " ('Gibbs sampling method', 6),\n",
       " ('news item', 6),\n",
       " ('great significance', 6),\n",
       " ('important component', 6),\n",
       " ('semantic dependency', 6),\n",
       " ('online course', 6),\n",
       " ('paragraph vector', 6),\n",
       " ('latent theme', 6),\n",
       " ('real-life dataset', 6),\n",
       " ('biomedical researcher', 6),\n",
       " ('research direction', 6),\n",
       " ('main research area', 6),\n",
       " ('New York City', 6),\n",
       " ('significant increase', 6),\n",
       " ('active learning', 6),\n",
       " ('online news website', 6),\n",
       " ('key component', 6),\n",
       " ('bag-of-word model', 6),\n",
       " ('information sharing', 6),\n",
       " ('background knowledge', 6),\n",
       " ('lda approach', 6),\n",
       " ('smart city', 6),\n",
       " ('different time period', 6),\n",
       " ('ad video', 6),\n",
       " ('synthetic aperture radar', 6),\n",
       " ('asymmetrical prior', 6),\n",
       " ('supervised learning method', 6),\n",
       " ('online product review', 6),\n",
       " ('content similarity', 6),\n",
       " ('spatial relation', 6),\n",
       " ('probabilistic approach', 6),\n",
       " ('full use', 6),\n",
       " ('complex network', 6),\n",
       " ('ranking function', 6),\n",
       " ('search space', 6),\n",
       " ('web service description', 6),\n",
       " ('high probability', 6),\n",
       " ('hidden structure', 6),\n",
       " ('multiple source', 6),\n",
       " ('topic stability', 6),\n",
       " ('small amount', 6),\n",
       " ('topic layer', 6),\n",
       " ('different scale', 6),\n",
       " ('new framework', 6),\n",
       " ('cold start problem', 6),\n",
       " ('great success', 6),\n",
       " ('open source project', 6),\n",
       " ('response variable', 6),\n",
       " ('variational approximation', 6),\n",
       " ('important factor', 6),\n",
       " ('sensor datum', 6),\n",
       " ('continuous datum', 6),\n",
       " ('hierarchical topic model', 6),\n",
       " ('model perplexity', 6),\n",
       " ('count matrix', 6),\n",
       " ('training process', 6),\n",
       " ('little attention', 6),\n",
       " ('sentiment-aware topic', 6),\n",
       " ('pseudo document', 6),\n",
       " ('automatic method', 6),\n",
       " ('target word', 6),\n",
       " ('support Vector Machines', 6),\n",
       " ('business process', 6),\n",
       " ('concurrency topic', 6),\n",
       " ('computation cost', 6),\n",
       " ('novel way', 6),\n",
       " ('single topic', 6),\n",
       " ('high-dimensional datum', 6),\n",
       " ('financial news', 6),\n",
       " ('popular method', 6),\n",
       " ('contextual feature', 6),\n",
       " ('single document', 6),\n",
       " ('other language', 6),\n",
       " ('user activity', 6),\n",
       " ('lda-based approach', 6),\n",
       " ('supervised method', 6),\n",
       " ('average accuracy', 6),\n",
       " ('crowded scene', 6),\n",
       " ('many domain', 6),\n",
       " ('high performance', 6),\n",
       " ('large archive', 6),\n",
       " ('media platform', 6),\n",
       " ('movie review', 6),\n",
       " ('co-occurrence relation', 6),\n",
       " ('many field', 6),\n",
       " ('difficult problem', 6),\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = relation.get_phrases_by_pattern(parse)\n",
    "sorted(dict(Counter([\" \".join(x) for x in ps])).items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hemes. (C) 2019 Elsevier Inc. All rights reserved. 【(C) 2019 Elsevier Inc.】\n",
      "earch. (C) 2019 Elsevier Inc. All rights reserved. 【(C) 2019 Elsevier Inc.】\n",
      "n CQA. (C) 2019 Elsevier B.V. All rights reserved. 【(C) 2019 Elsevier B.V.】\n",
      " behalf of International Institute of Forecasters. 【(C) 2018 The Authors. Published by Elsevier B.V.】\n",
      "tworks (DNNs). (C) 2019 Published by Elsevier Inc. 【(C) 2019 Published by Elsevier Inc.】\n",
      "-2005. (C) 2019 Elsevier Ltd. All rights reserved. 【(C) 2019 Elsevier Ltd.】\n",
      "ethod. (C) 2019 Elsevier B.V. All rights reserved. 【(C) 2019 Elsevier B.V.】\n",
      "tics, Inc. Production and hosting by Elsevier B.V. 【(C) 2019 The Korean Association of Shipping and Logistics, Inc. Production and hosting by Elsevier B.V.】\n",
      "antly. (C) 2019 Elsevier Inc. All rights reserved. 【(C) 2019 Elsevier Inc.】\n",
      "dents. (C) 2019 Elsevier Ltd. All rights reserved. 【(C) 2019 Elsevier Ltd.】\n",
      "tives. (C) 2019 Elsevier Ltd. All rights reserved. 【(C) 2019 Elsevier Ltd.】\n",
      "iants. (C) 2019 Elsevier B.V. All rights reserved. 【(C) 2019 Elsevier B.V.】\n",
      "ste treatment. (C) 2019 Published by Elsevier B.V. 【(C) 2019 Published by Elsevier B.V.】\n",
      "ation. (C) 2019 Elsevier B.V. All rights reserved. 【(C) 2019 Elsevier B.V.】\n",
      "(C) 2019 The Author(s). Published by Elsevier Inc. 【(C) 2019 The Author(s). Published by Elsevier Inc.】\n",
      "heory. (C) 2018 Elsevier B.V. All rights reserved. 【(C) 2018 Elsevier B.V.】\n",
      ". (C) 2019 The Authors. Published by Elsevier B.V. 【(C) 2019 The Authors. Published by Elsevier B.V.】\n",
      "asets. (C) 2018 Elsevier B.V. All rights reserved. 【(C) 2018 Elsevier B.V.】\n",
      "erits. (C) 2018 Elsevier B.V. All rights reserved. 【(C) 2018 Elsevier B.V.】\n",
      "rable. (C) 2019 Elsevier B.V. All rights reserved. 【(C) 2019 Elsevier B.V.】\n",
      "lters. (C) 2018 Elsevier B.V. All rights reserved. 【(C) 2018 Elsevier B.V.】\n",
      "ures). (C) 2019 Elsevier B.V. All rights reserved. 【(C) 2019 Elsevier B.V.】\n",
      "tions. (C) 2018 Elsevier Inc. All rights reserved. 【(C) 2018 Elsevier Inc.】\n",
      "arket. (C) 2018 Elsevier Ltd. All rights reserved. 【(C) 2018 Elsevier Ltd.】\n",
      "lines. (C) 2018 Elsevier B.V. All rights reserved. 【(C) 2018 Elsevier B.V.】\n",
      "digms. (C) 2018 Elsevier B.V. All rights reserved. 【(C) 2018 Elsevier B.V.】\n",
      "odels. (C) 2018 Elsevier B.V. All rights reserved. 【(C) 2018 Elsevier B.V.】\n",
      " data. (C) 2018 Elsevier B.V. All rights reserved. 【(C) 2018 Elsevier B.V.】\n",
      "y 28%. (C) 2018 Elsevier Ltd. All rights reserved. 【(C) 2018 Elsevier Ltd.】\n",
      "nment. (C) 2018 Elsevier Ltd. All rights reserved. 【(C) 2018 Elsevier Ltd.】\n",
      "stems. (C) 2018 Elsevier Ltd. All rights reserved. 【(C) 2018 Elsevier Ltd.】\n",
      "tions. (C) 2018 Elsevier B.V. All rights reserved. 【(C) 2018 Elsevier B.V.】\n",
      "mance. (C) 2018 Elsevier Inc. All rights reserved. 【(C) 2018 Elsevier Inc.】\n",
      "tions. (C) 2018 Elsevier Ltd. All rights reserved. 【(C) 2018 Elsevier Ltd.】\n",
      "rward. (C) 2018 Elsevier Ltd. All rights reserved. 【(C) 2018 Elsevier Ltd.】\n",
      "oposed models. (C) 2018 Published by Elsevier B.V. 【(C) 2018 Published by Elsevier B.V.】\n",
      "ction. (C) 2018 Elsevier Ltd. All rights reserved. 【(C) 2018 Elsevier Ltd.】\n",
      " EHRs. (C) 2018 Elsevier B.V. All rights reserved. 【(C) 2018 Elsevier B.V.】\n",
      " respectively. (C) 2017 Published by Elsevier B.V. 【(C) 2017 Published by Elsevier B.V.】\n",
      "thods. (C) 2018 Elsevier Ltd. All rights reserved. 【(C) 2018 Elsevier Ltd.】\n",
      "uages. (C) 2018 Elsevier Inc. All rights reserved. 【(C) 2018 Elsevier Inc.】\n",
      "iency. (C) 2018 Elsevier Ltd. All rights reserved. 【(C) 2018 Elsevier Ltd.】\n",
      "oblem. (C) 2018 Elsevier B.V. All rights reserved. 【(C) 2018 Elsevier B.V.】\n",
      "stems. (c) 2018 Elsevier B.V. All rights reserved. 【(c) 2018 Elsevier B.V.】\n",
      "al annotation. (C) 2018 Published by Elsevier B.V. 【(C) 2018 Published by Elsevier B.V.】\n",
      "thods. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      "ously. (c) 2018 Elsevier B.V. All rights reserved. 【(c) 2018 Elsevier B.V.】\n",
      "gging. (C) 2018 Elsevier Ltd. All rights reserved. 【(C) 2018 Elsevier Ltd.】\n",
      "India. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      "lysis. (C) 2017 Elsevier Inc. All rights reserved. 【(C) 2017 Elsevier Inc.】\n",
      "S-LDA. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      "ction. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      "perly. (c) 2017 Elsevier B.V. All rights reserved. 【(c) 2017 Elsevier B.V.】\n",
      "dates. (C) 2017 Elsevier Inc. All rights reserved. 【(C) 2017 Elsevier Inc.】\n",
      "shift. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      "e. Published by Elsevier Ltd. All rights reserved. 【(C) 2017 Fellowship of Postgraduate Medicine. Published by Elsevier Ltd.】\n",
      ". (C) 2017 The Authors. Published by Elsevier Ltd. 【(C) 2017 The Authors. Published by Elsevier Ltd.】\n",
      "stics. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      "poses. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      "areas. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      "ithms. (C) 2017 Elsevier Ltd. All rights reserved. 【(C) 2017 Elsevier Ltd.】\n",
      "). Published by Elsevier B.V. All rights reserved. 【(C) 2017 International Society for Photogrammetry and Remote Sensing, Inc. (ISPRS). Published by Elsevier B.V.】\n",
      "ation. (C) 2016 Elsevier B.V. All rights reserved. 【(C) 2016 Elsevier B.V.】\n",
      "sions. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      "iciently long, (C) 2017 Published by Elsevier B.V. 【(C) 2017 Published by Elsevier B.V.】\n",
      "asets. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      "e FSD. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      "ethod. (C) 2017 Elsevier Ltd. All rights reserved. 【(C) 2017 Elsevier Ltd.】\n",
      " user. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      "urism. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      " data. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      "texts. (C) 2017 Elsevier Ltd. All rights reserved. 【(C) 2017 Elsevier Ltd.】\n",
      "urces. (C) 2017 Elsevier Ltd. All rights reserved. 【(C) 2017 Elsevier Ltd.】\n",
      "ieval. (C) 2017 Elsevier Ltd. All rights reserved. 【(C) 2017 Elsevier Ltd.】\n",
      "oblem. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      "ising. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      "fairs. (C) 2017 Elsevier Ltd. All rights reserved. 【(C) 2017 Elsevier Ltd.】\n",
      "roach. (C) 2017 Elsevier Ltd. All rights reserved. 【(C) 2017 Elsevier Ltd.】\n",
      "ation. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      "ATLAB. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      "sfied. (C) 2017 Elsevier Ltd. All rights reserved. 【(C) 2017 Elsevier Ltd.】\n",
      ". (C) 2017 The Authors. Published by Elsevier B.V. 【(C) 2017 The Authors. Published by Elsevier B.V.】\n",
      "icons. (C) 2016 Elsevier B.V. All rights reserved. 【(C) 2016 Elsevier B.V.】\n",
      "o 15%. (C) 2017 Elsevier Inc. All rights reserved. 【(C) 2017 Elsevier Inc.】\n",
      "evels. (C) 2016 Elsevier B.V. All rights reserved. 【(C) 2016 Elsevier B.V.】\n",
      "overy. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      "uning. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      "iques. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      "earch. (C) 2017 Elsevier Ltd. All rights reserved. 【(C) 2017 Elsevier Ltd.】\n",
      "paign. (C) 2017 Elsevier Ltd. All rights reserved. 【(C) 2017 Elsevier Ltd.】\n",
      " memory usage. (C) 2017 Published by Elsevier Inc. 【(C) 2017 Published by Elsevier Inc.】\n",
      " data. (C) 2017 Elsevier Ltd. All rights reserved. 【(C) 2017 Elsevier Ltd.】\n",
      "ystem. (C) 2016 Elsevier Ltd. All rights reserved. 【(C) 2016 Elsevier Ltd.】\n",
      "roach. (C) 2017 Elsevier B.V. All rights reserved. 【(C) 2017 Elsevier B.V.】\n",
      "ctice. (C) 2016 Elsevier Ltd. All rights reserved. 【(C) 2016 Elsevier Ltd.】\n",
      "pment. (C) 2017 Elsevier Ltd. All rights reserved. 【(C) 2017 Elsevier Ltd.】\n",
      " eyes. (C) 2016 Elsevier B.V. All rights reserved. 【(C) 2016 Elsevier B.V.】\n",
      "ments. (C) 2016 Elsevier Ltd. All rights reserved. 【(C) 2016 Elsevier Ltd.】\n",
      "iques. (C) 2016 Elsevier B.V. All rights reserved. 【(C) 2016 Elsevier B.V.】\n",
      "is an open access article under the CC BY license. 【(C) 2016 The Authors. Published by Elsevier Inc.】\n",
      "e way. (C) 2016 Elsevier Ltd. All rights reserved. 【(C) 2016 Elsevier Ltd.】\n",
      ". (C) 2017 The Authors. Published by Elsevier B.V. 【(C) 2017 The Authors. Published by Elsevier B.V.】\n",
      "terns. (C) 2016 Elsevier Ltd. All rights reserved. 【(C) 2016 Elsevier Ltd.】\n",
      "antly. (C) 2015 Elsevier B.V. All rights reserved. 【(C) 2015 Elsevier B.V.】\n",
      "xtent. (C) 2016 Elsevier B.V. All rights reserved. 【(C) 2016 Elsevier B.V.】\n",
      ". (C) 2017 The Authors. Published by Elsevier B.V. 【(C) 2017 The Authors. Published by Elsevier B.V.】\n",
      ". (C) 2017 The Authors. Published by Elsevier B.V. 【(C) 2017 The Authors. Published by Elsevier B.V.】\n",
      ". (C) 2017 The Authors. Published by Elsevier B.V. 【(C) 2017 The Authors. Published by Elsevier B.V.】\n",
      "n conversations. (C) 2016 Elsevier B.V. All rights 【(C) 2016 Elsevier B.V.】\n",
      "ctive. (C) 2015 Elsevier B.V. All rights reserved. 【(C) 2015 Elsevier B.V.】\n",
      "ation. (C) 2016 Elsevier Inc. All rights reserved. 【(C) 2016 Elsevier Inc.】\n",
      "jects. (C) 2016 Elsevier B.V. All rights reserved. 【(C) 2016 Elsevier B.V.】\n",
      " task. (C) 2016 Elsevier Inc. All rights reserved. 【(C) 2016 Elsevier Inc.】\n",
      "(C) 2016 The Author(s). Published by Elsevier Inc. 【(C) 2016 The Author(s). Published by Elsevier Inc.】\n",
      "odels. (C) 2016 Elsevier Ltd. All rights reserved. 【(C) 2016 Elsevier Ltd.】\n",
      "cepts. (C) 2016 Elsevier Ltd. All rights reserved. 【(C) 2016 Elsevier Ltd.】\n",
      "twork. (C) 2016 Elsevier B.V. All rights reserved. 【(C) 2016 Elsevier B.V.】\n",
      "thods. (C) 2016 Elsevier B.V. All rights reserved. 【(C) 2016 Elsevier B.V.】\n",
      "iques. (C) 2016 Elsevier Ltd. All rights reserved. 【(C) 2016 Elsevier Ltd.】\n",
      "ality. (C) 2016 Elsevier B.V. All rights reserved. 【(C) 2016 Elsevier B.V.】\n",
      "tents. (C) 2016 Elsevier Ltd. All rights reserved. 【(C) 2016 Elsevier Ltd.】\n",
      ". (C) 2016 The Authors. Published by Elsevier Inc. 【(C) 2016 The Authors. Published by Elsevier Inc.】\n",
      "ction. (C) 2016 Elsevier B.V. All rights reserved. 【(C) 2016 Elsevier B.V.】\n",
      ". (C) 2016 The Authors. Published by Elsevier Ltd. 【(C) 2016 The Authors. Published by Elsevier Ltd.】\n",
      "Boost. (C) 2016 Elsevier B.V. All rights reserved. 【(C) 2016 Elsevier B.V.】\n",
      "3.3%). (C) 2016 Elsevier Ltd. All rights reserved. 【(C) 2016 Elsevier Ltd.】\n",
      "tions. (C) 2016 Elsevier B.V. All rights reserved. 【(C) 2016 Elsevier B.V.】\n",
      " data. (C) 2016 Elsevier Inc. All rights reserved. 【(C) 2016 Elsevier Inc.】\n",
      "ports. (C) 2016 Elsevier Inc. All rights reserved. 【(C) 2016 Elsevier Inc.】\n",
      "ation. (C) 2016 Elsevier B.V. All rights reserved. 【(C) 2016 Elsevier B.V.】\n",
      "aches. (C) 2016 Elsevier Inc. All rights reserved. 【(C) 2016 Elsevier Inc.】\n",
      "ethod. (C) 2016 Elsevier B.V. All rights reserved. 【(C) 2016 Elsevier B.V.】\n",
      "aking. (C) 2016 Elsevier B.V. All rights reserved. 【(C) 2016 Elsevier B.V.】\n",
      "nions. (C) 2015 Elsevier Ltd. All rights reserved. 【(C) 2015 Elsevier Ltd.】\n",
      "trees. (C) 2016 Elsevier Ltd. All rights reserved. 【(C) 2016 Elsevier Ltd.】\n",
      "uracy. (C) 2016 Elsevier B.V. All rights reserved. 【(C) 2016 Elsevier B.V.】\n",
      " task. (C) 2016 Elsevier B.V. All rights reserved. 【(C) 2016 Elsevier B.V.】\n",
      "ively. (C) 2015 Elsevier B.V. All rights reserved. 【(C) 2015 Elsevier B.V.】\n",
      "pport. (C) 2016 Elsevier Inc. All rights reserved. 【(C) 2016 Elsevier Inc.】\n",
      "ctors. (C) 2015 Elsevier B.V. All rights reserved. 【(C) 2015 Elsevier B.V.】\n",
      "sment. (C) 2014 Elsevier B.V. All rights reserved. 【(C) 2014 Elsevier B.V.】\n",
      "model. (C) 2015 Elsevier Ltd. All rights reserved. 【(C) 2015 Elsevier Ltd.】\n",
      "ially. (C) 2015 Elsevier Inc. All rights reserved. 【(C) 2015 Elsevier Inc.】\n",
      "tails. (C) 2016 Elsevier Ltd. All rights reserved. 【(C) 2016 Elsevier Ltd.】\n",
      "marks. (c) 2015 Elsevier B.V. All rights reserved. 【(c) 2015 Elsevier B.V.】\n",
      "egies. (C) 2015 Elsevier B.V. All rights reserved. 【(C) 2015 Elsevier B.V.】\n",
      "roach. (C) 2015 Elsevier B.V. All rights reserved. 【(C) 2015 Elsevier B.V.】\n",
      "ol) Hosting By Elsevier Ltd. All rights reserverd. 【(C) 2016, IFAC (International Federation Control) Hosting By Elsevier Ltd.】\n",
      ". (C) 2016 The Authors. Published by Elsevier B.V. 【(C) 2016 The Authors. Published by Elsevier B.V.】\n",
      "combining DJs. (C) 2016 Published by Elsevier B.V. 【(C) 2016 Published by Elsevier B.V.】\n",
      "eriod. (C) 2015 Elsevier Ltd. All rights reserved. 【(C) 2015 Elsevier Ltd.】\n",
      "y. Published by Elsevier Inc. All rights reserved. 【(C) 2015 Kelley School of Business, Indiana University. Published by Elsevier Inc.】\n",
      "sment. (C) 2015 Elsevier Ltd. All rights reserved. 【(C) 2015 Elsevier Ltd.】\n",
      "shion. (C) 2015 Elsevier Inc. All rights reserved. 【(C) 2015 Elsevier Inc.】\n",
      "sment. (C) 2015 Elsevier B.V. All rights reserved. 【(C) 2015 Elsevier B.V.】\n",
      "nse (http://creativecommons.org/licenses/by/4.0/). 【(C) 2015 The Authors. Published by Elsevier B.V.】\n",
      "eline. (C) 2015 Elsevier Ltd. All rights reserved. 【(C) 2015 Elsevier Ltd.】\n",
      "ctory. (C) 2014 Elsevier Ltd. All rights reserved. 【(C) 2014 Elsevier Ltd.】\n",
      "ro-F1. (C) 2015 Elsevier B.V. All rights reserved. 【(C) 2015 Elsevier B.V.】\n",
      "ation. (C) 2015 Elsevier Ltd. All rights reserved. 【(C) 2015 Elsevier Ltd.】\n",
      " data. (C) 2014 Elsevier Ltd. All rights reserved. 【(C) 2014 Elsevier Ltd.】\n",
      "odels. (C) 2015 Elsevier Inc. All rights reserved. 【(C) 2015 Elsevier Inc.】\n",
      "works. (C) 2015 Elsevier Inc. All rights reserved. 【(C) 2015 Elsevier Inc.】\n",
      "mains. (C) 2015 Elsevier Ltd. All rights reserved. 【(C) 2015 Elsevier Ltd.】\n",
      "poses. (C) 2015 Elsevier Inc. All rights reserved. 【(C) 2015 Elsevier Inc.】\n",
      "opics. (C) 2015 Elsevier B.V. All rights reserved. 【(C) 2015 Elsevier B.V.】\n",
      "odels. (C) 2015 Elsevier Inc. All rights reserved. 【(C) 2015 Elsevier Inc.】\n",
      "tions. (C) 2015 Elsevier Ltd. All rights reserved. 【(C) 2015 Elsevier Ltd.】\n",
      "ences. (C) 2015 Elsevier B.V. All rights reserved. 【(C) 2015 Elsevier B.V.】\n",
      "sults. (C) 2014 Elsevier Ltd. All rights reserved. 【(C) 2014 Elsevier Ltd.】\n",
      "forts. (C) 2015 Elsevier B.V. All rights reserved. 【(C) 2015 Elsevier B.V.】\n",
      "works. (C) 2015 Elsevier Inc. All rights reserved. 【(C) 2015 Elsevier Inc.】\n",
      "better.(C) 2014 Elsevier B.V. All rights reserved. 【(C) 2014 Elsevier B.V.】\n",
      "sults. (C) 2014 Elsevier B.V. All rights reserved. 【(C) 2014 Elsevier B.V.】\n",
      "earch. (C) 2014 Elsevier Ltd. All rights reserved. 【(C) 2014 Elsevier Ltd.】\n",
      "ently. (C) 2014 Elsevier Ltd. All rights reserved. 【(C) 2014 Elsevier Ltd.】\n",
      "aches. (C) 2014 Elsevier B.V. All rights reserved. 【(C) 2014 Elsevier B.V.】\n",
      "forms. (C) 2014 Elsevier Inc. All rights reserved. 【(C) 2014 Elsevier Inc.】\n",
      "ining. (C) 2014 Elsevier B.V. All rights reserved. 【(C) 2014 Elsevier B.V.】\n",
      "uracy. (C) 2014 Elsevier B.V. All rights reserved. 【(C) 2014 Elsevier B.V.】\n",
      "ssing. (C) 2014 Elsevier B.V. All rights reserved. 【(C) 2014 Elsevier B.V.】\n",
      "sages. (C) 2014 Elsevier B.V. All rights reserved. 【(C) 2014 Elsevier B.V.】\n",
      " pass. (C) 2014 Elsevier Ltd. All rights reserved. 【(C) 2014 Elsevier Ltd.】\n",
      "enome. (C) 2014 Elsevier Ltd. All rights reserved. 【(C) 2014 Elsevier Ltd.】\n",
      "roach. (C) 2014 Elsevier Ltd. All rights reserved. 【(C) 2014 Elsevier Ltd.】\n",
      " best. (C) 2014 Elsevier B.V. All rights reserved. 【(C) 2014 Elsevier B.V.】\n",
      "means. (C) 2014 Elsevier B.V. All rights reserved. 【(C) 2014 Elsevier B.V.】\n",
      "ering. (C) 2014 Elsevier Ltd. All rights reserved. 【(C) 2014 Elsevier Ltd.】\n",
      "cheme. (C) 2014 Elsevier B.V. All rights reserved. 【(C) 2014 Elsevier B.V.】\n",
      "iques. (C) 2014 Elsevier Ltd. All rights reserved. 【(C) 2014 Elsevier Ltd.】\n",
      "aches. (C) 2014 Elsevier Ltd. All rights reserved. 【(C) 2014 Elsevier Ltd.】\n",
      "ation. (C) 2014 Elsevier Ltd. All rights reserved. 【(C) 2014 Elsevier Ltd.】\n",
      "nse (http://creativecommons.org/licenses/by/3.0/). 【(C) 2014 The Authors. Published by Elsevier Inc.】\n",
      "rders. (C) 2014 Elsevier B.V. All rights reserved. 【(C) 2014 Elsevier B.V.】\n",
      "nking. (C) 2014 Elsevier Ltd. All rights reserved. 【(C) 2014 Elsevier Ltd.】\n",
      "ively. (C) 2014 Elsevier Ltd. All rights reserved. 【(C) 2014 Elsevier Ltd.】\n",
      "ethod. (C) 2014 Elsevier Inc. All rights reserved. 【(C) 2014 Elsevier Inc.】\n",
      "thods. (C) 2014 Elsevier Ltd. All rights reserved. 【(C) 2014 Elsevier Ltd.】\n",
      "tions. (C) 2013 Elsevier Ltd. All rights reserved. 【(C) 2013 Elsevier Ltd.】\n",
      "words. (C) 2014 Elsevier B.V. All rights reserved. 【(C) 2014 Elsevier B.V.】\n",
      "mains. (C) 2013 Elsevier B.V. All rights reserved. 【(C) 2013 Elsevier B.V.】\n",
      "rties. (C) 2013 Elsevier B.V. All rights reserved. 【(C) 2013 Elsevier B.V.】\n",
      "ively. (C) 2013 Elsevier B.V. All rights reserved. 【(C) 2013 Elsevier B.V.】\n",
      "e HCP. (C) 2013 Elsevier Inc. All rights reserved. 【(C) 2013 Elsevier Inc.】\n",
      "thods. (C) 2013 Elsevier Ltd. All rights reserved. 【(C) 2013 Elsevier Ltd.】\n",
      "ation. (C) 2013 Elsevier Inc. All rights reserved. 【(C) 2013 Elsevier Inc.】\n",
      "ystem. (C) 2012 Elsevier B.V. All rights reserved. 【(C) 2012 Elsevier B.V.】\n",
      "oblem. (C) 2013 Elsevier B.V. All rights reserved. 【(C) 2013 Elsevier B.V.】\n",
      "sevier B.V. Open access under CC BY-NC-ND license. 【(C) 2014 Published by Elsevier B.V.】\n",
      "ories. (C) 2013 Elsevier Ltd. All rights reserved. 【(C) 2013 Elsevier Ltd.】\n",
      "aning. (C) 2013 Elsevier B.V. All rights reserved. 【(C) 2013 Elsevier B.V.】\n",
      "mages. (c) 2013 Elsevier B.V. All rights reserved. 【(c) 2013 Elsevier B.V.】\n",
      "small. (C) 2012 Elsevier B.V. All rights reserved. 【(C) 2012 Elsevier B.V.】\n",
      "stems. (C) 2013 Elsevier B.V. All rights reserved. 【(C) 2013 Elsevier B.V.】\n",
      "study. (C) 2013 Elsevier B.V. All rights reserved. 【(C) 2013 Elsevier B.V.】\n",
      "omers. (C) 2012 Elsevier Ltd. All rights reserved. 【(C) 2012 Elsevier Ltd.】\n",
      "data. (C) 2012 Elsevier B.V. All rights reserved.` 【(C) 2012 Elsevier B.V.】\n",
      "roach. (C) 2012 Elsevier Inc. All rights reserved. 【(C) 2012 Elsevier Inc.】\n",
      "iment. (C) 2012 Elsevier B.V. All rights reserved. 【(C) 2012 Elsevier B.V.】\n",
      "terms. (C) 2012 Elsevier Ltd. All rights reserved. 【(C) 2012 Elsevier Ltd.】\n",
      ". (C) 2013 The Authors. Published by Elsevier B.V. 【(C) 2013 The Authors. Published by Elsevier B.V.】\n",
      "tasks. (C) 2012 Elsevier B.V. All rights reserved. 【(C) 2012 Elsevier B.V.】\n",
      "tions. (c) 2012 Elsevier B.V. All rights reserved. 【(c) 2012 Elsevier B.V.】\n",
      "ethod. (C) 2012 Elsevier B.V. All rights reserved. 【(C) 2012 Elsevier B.V.】\n",
      "rises. (C) 2012 Elsevier Ltd. All rights reserved. 【(C) 2012 Elsevier Ltd.】\n",
      "nting. (C) 2012 Elsevier Ltd. All rights reserved. 【(C) 2012 Elsevier Ltd.】\n",
      "orted. (C) 2011 Elsevier B.V. All rights reserved. 【(C) 2011 Elsevier B.V.】\n",
      "odels. (C) 2011 Elsevier B.V. All rights reserved. 【(C) 2011 Elsevier B.V.】\n",
      "aches. (C) 2011 Elsevier B.V. All rights reserved. 【(C) 2011 Elsevier B.V.】\n",
      "bility of Noorul Islam Centre for Higher Education 【(C) 2012 Published by Elsevier Ltd.】\n",
      "ell performed. (C) 2011 Published by Elsevier Ltd. 【(C) 2011 Published by Elsevier Ltd.】\n",
      "ities. (C) 2011 Elsevier Ltd. All rights reserved. 【(C) 2011 Elsevier Ltd.】\n",
      "roach. (C) 2011 Elsevier Ltd. All rights reserved. 【(C) 2011 Elsevier Ltd.】\n",
      "tion., (C) 2010 Elsevier Ltd. All rights reserved. 【(C) 2010 Elsevier Ltd.】\n",
      " data. (C) 2010 Elsevier Ltd. All rights reserved. 【(C) 2010 Elsevier Ltd.】\n",
      "the MRF model. (C) 2011 Published by Elsevier Ltd. 【(C) 2011 Published by Elsevier Ltd.】\n",
      "tions. (C) 2010 Elsevier B.V. All rights reserved. 【(C) 2010 Elsevier B.V.】\n",
      " responsibility of COINs 2010 Organizing Committee 【(C) 2011 Published by Elsevier Ltd.】\n",
      "cable. (C) 2010 Elsevier B.V. All rights reserved. 【(C) 2010 Elsevier B.V.】\n",
      "gnals. (C) 2009 Elsevier B.V. All rights reserved. 【(C) 2009 Elsevier B.V.】\n",
      "opics. (C) 2008 Elsevier B.V. All rights reserved. 【(C) 2008 Elsevier B.V.】\n",
      "ction. (c) 2007 Elsevier B.V. All rights reserved. 【(c) 2007 Elsevier B.V.】\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "242"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = 0\n",
    "s = set()\n",
    "for r in _raw:\n",
    "    m = re.findall(pp.Elsevier, r)\n",
    "    if m:\n",
    "        print(r[-50::], \"【{}】\".format(m[0]))\n",
    "        c+=1\n",
    "c"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lda362",
   "language": "python",
   "name": "lda362"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
