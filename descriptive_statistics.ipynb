{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not use stanford CoreNLP client!\n"
     ]
    }
   ],
   "source": [
    "import preprocess as pp\n",
    "import configs\n",
    "import persister\n",
    "import relation\n",
    "import lda\n",
    "import vis\n",
    "import evaluate\n",
    "from utils import *\n",
    "\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from functools import reduce\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tree import Tree\n",
    "from nltk import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load abs\n"
     ]
    }
   ],
   "source": [
    "# is_news = 1\n",
    "\n",
    "# l = 7\n",
    "# t = 4\n",
    "# m = \"c_v\"\n",
    "# i = 200\n",
    "# min_df = 1\n",
    "\n",
    "is_news = 0\n",
    "l = 6\n",
    "t = 14\n",
    "m = \"c_v\"\n",
    "i = 200\n",
    "min_df = 1\n",
    "\n",
    "size = 100\n",
    "\n",
    "# load\n",
    "if is_news:\n",
    "    _raw = persister.load_json(configs.RAWNEWS)\n",
    "    parse = persister.read_parse()\n",
    "    _input = persister.read_input(configs.NEWSINPUT)\n",
    "    model_name = configs.NEWSMODEL.format(l, t, m, i, min_df)\n",
    "    terms, doc_topic, topic_word = persister.read_lda(configs.NEWSLDA.format(model_name))\n",
    "    model = persister.load_model(model_name)\n",
    "    vec = persister.load_model(configs.NEWSVEC.format(min_df))\n",
    "    w2vmodel = persister.load_wv(configs.NEWSWV.format(size))\n",
    "    print(\"load news\")\n",
    "else:\n",
    "    _raw = persister.load_json(configs.RAWABSTRACT)\n",
    "    _input = persister.read_input(configs.ABSTRACTINPUT)\n",
    "    model_name = configs.ABSTRACTMODEL.format(l, t, m, i, min_df)\n",
    "    terms, doc_topic, topic_word = persister.read_lda(configs.ABSTRACTLDA.format(model_name))\n",
    "    parse = persister.read_parse(configs.ABSTRACTPARSE)\n",
    "    model = persister.load_model(model_name)\n",
    "    vec = persister.load_model(configs.ABSVEC.format(min_df))\n",
    "    w2vmodel = persister.load_wv(configs.ABSWV.format(size))\n",
    "    print(\"load abs\")\n",
    "tf = vec.fit_transform(_input)\n",
    "top_terms, top_docs = lda.get_topics(topic_word, terms, doc_topic)\n",
    "df_top_words, df_top_docs = lda.pd_topics_vis(top_terms, top_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'self driving emerge technology several benefit improved quality life crash reduction fuel efficiency however concern regard utilization self driving technology affordability safety control liability increase effort research center academia industry advance every sphere science technology yet get harder find innovative idea however untapped potential analyze increase research result use visual analytic scientometric machine learning paper use scientific literature database scopus collect relevant dataset apply visual analytic tool citespace conduct co citation clustering term burst detection time series analysis identify emerge trend analysis global impact collaboration also apply unsupervised topic modeling latent dirichlet allocation identify hidden topic gain insight topic regard self driving technology result show emerge trend relevant self driving technology global regional collaboration country moreover result form lda show standard topic modeling reveal hidden topic without trend information believe result study indicate key technological area research domain hot spot technology future plan include dynamic topic modeling identify trend'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_input[0].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9933"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs distribution: {3: 395, 12: 766, 13: 5, 0: 27, 11: 83, 6: 1176, 4: 140, 7: 1, 1: 2, 10: 1}\n"
     ]
    }
   ],
   "source": [
    "distr = lda.get_dominant_topic(doc_topic)\n",
    "print(\"docs distribution:\",dict(Counter(distr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top terms info:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "      <th>Word 11</th>\n",
       "      <th>Word 12</th>\n",
       "      <th>Word 13</th>\n",
       "      <th>Word 14</th>\n",
       "      <th>Word 15</th>\n",
       "      <th>Word 16</th>\n",
       "      <th>Word 17</th>\n",
       "      <th>Word 18</th>\n",
       "      <th>Word 19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>(6396, 221.18)</td>\n",
       "      <td>(3861, 156.45)</td>\n",
       "      <td>(2415, 154.5)</td>\n",
       "      <td>(5330, 143.69)</td>\n",
       "      <td>(1357, 137.79)</td>\n",
       "      <td>(3565, 113.72)</td>\n",
       "      <td>(9138, 106.83)</td>\n",
       "      <td>(2071, 94.07)</td>\n",
       "      <td>(2292, 83.0)</td>\n",
       "      <td>(1118, 68.83)</td>\n",
       "      <td>(8493, 63.52)</td>\n",
       "      <td>(3505, 62.15)</td>\n",
       "      <td>(9049, 57.71)</td>\n",
       "      <td>(7267, 54.26)</td>\n",
       "      <td>(9048, 54.06)</td>\n",
       "      <td>(3504, 50.48)</td>\n",
       "      <td>(565, 48.94)</td>\n",
       "      <td>(1616, 46.28)</td>\n",
       "      <td>(7527, 44.88)</td>\n",
       "      <td>(6789, 44.08)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>(5871, 259.54)</td>\n",
       "      <td>(8496, 86.19)</td>\n",
       "      <td>(2483, 63.1)</td>\n",
       "      <td>(5050, 50.88)</td>\n",
       "      <td>(511, 37.19)</td>\n",
       "      <td>(5027, 36.26)</td>\n",
       "      <td>(515, 27.07)</td>\n",
       "      <td>(8188, 26.52)</td>\n",
       "      <td>(7943, 26.22)</td>\n",
       "      <td>(8439, 25.47)</td>\n",
       "      <td>(8445, 25.25)</td>\n",
       "      <td>(4775, 22.62)</td>\n",
       "      <td>(4800, 21.37)</td>\n",
       "      <td>(2206, 21.02)</td>\n",
       "      <td>(1476, 20.41)</td>\n",
       "      <td>(4926, 20.37)</td>\n",
       "      <td>(1352, 20.19)</td>\n",
       "      <td>(6613, 18.96)</td>\n",
       "      <td>(4122, 16.53)</td>\n",
       "      <td>(3860, 16.33)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>(1269, 19.85)</td>\n",
       "      <td>(4089, 11.69)</td>\n",
       "      <td>(4649, 9.79)</td>\n",
       "      <td>(3988, 7.93)</td>\n",
       "      <td>(9041, 7.11)</td>\n",
       "      <td>(6333, 7.01)</td>\n",
       "      <td>(5817, 6.85)</td>\n",
       "      <td>(5133, 6.78)</td>\n",
       "      <td>(5646, 6.69)</td>\n",
       "      <td>(9905, 6.67)</td>\n",
       "      <td>(6583, 6.67)</td>\n",
       "      <td>(3599, 6.22)</td>\n",
       "      <td>(4830, 5.93)</td>\n",
       "      <td>(3489, 5.9)</td>\n",
       "      <td>(8613, 5.87)</td>\n",
       "      <td>(4893, 5.12)</td>\n",
       "      <td>(4886, 5.06)</td>\n",
       "      <td>(9598, 5.03)</td>\n",
       "      <td>(888, 4.99)</td>\n",
       "      <td>(1166, 4.86)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>(9489, 774.45)</td>\n",
       "      <td>(2071, 691.96)</td>\n",
       "      <td>(9023, 635.56)</td>\n",
       "      <td>(2994, 447.82)</td>\n",
       "      <td>(95, 416.09)</td>\n",
       "      <td>(334, 402.85)</td>\n",
       "      <td>(8969, 393.15)</td>\n",
       "      <td>(8493, 386.16)</td>\n",
       "      <td>(4133, 340.06)</td>\n",
       "      <td>(4856, 334.16)</td>\n",
       "      <td>(8175, 330.52)</td>\n",
       "      <td>(6401, 316.37)</td>\n",
       "      <td>(279, 311.09)</td>\n",
       "      <td>(2360, 293.64)</td>\n",
       "      <td>(452, 289.6)</td>\n",
       "      <td>(342, 275.95)</td>\n",
       "      <td>(1234, 271.08)</td>\n",
       "      <td>(8188, 241.73)</td>\n",
       "      <td>(5843, 237.56)</td>\n",
       "      <td>(1506, 237.21)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>(4179, 910.26)</td>\n",
       "      <td>(3233, 421.58)</td>\n",
       "      <td>(6030, 376.84)</td>\n",
       "      <td>(6993, 348.52)</td>\n",
       "      <td>(9644, 296.13)</td>\n",
       "      <td>(5403, 295.57)</td>\n",
       "      <td>(9613, 294.0)</td>\n",
       "      <td>(7753, 289.52)</td>\n",
       "      <td>(4943, 214.63)</td>\n",
       "      <td>(4055, 195.09)</td>\n",
       "      <td>(7527, 176.11)</td>\n",
       "      <td>(9489, 171.6)</td>\n",
       "      <td>(8249, 170.43)</td>\n",
       "      <td>(380, 167.65)</td>\n",
       "      <td>(5558, 167.43)</td>\n",
       "      <td>(7253, 166.41)</td>\n",
       "      <td>(1335, 163.3)</td>\n",
       "      <td>(7869, 151.41)</td>\n",
       "      <td>(4856, 142.66)</td>\n",
       "      <td>(7329, 141.64)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 5</th>\n",
       "      <td>(647, 92.33)</td>\n",
       "      <td>(8208, 79.19)</td>\n",
       "      <td>(3592, 39.85)</td>\n",
       "      <td>(4289, 37.56)</td>\n",
       "      <td>(9865, 34.77)</td>\n",
       "      <td>(5118, 27.65)</td>\n",
       "      <td>(6639, 27.24)</td>\n",
       "      <td>(2334, 23.5)</td>\n",
       "      <td>(6126, 20.37)</td>\n",
       "      <td>(4313, 19.81)</td>\n",
       "      <td>(5135, 17.49)</td>\n",
       "      <td>(5741, 16.78)</td>\n",
       "      <td>(5742, 16.56)</td>\n",
       "      <td>(3, 15.75)</td>\n",
       "      <td>(1009, 14.92)</td>\n",
       "      <td>(1502, 14.53)</td>\n",
       "      <td>(9866, 13.97)</td>\n",
       "      <td>(4576, 13.41)</td>\n",
       "      <td>(657, 13.14)</td>\n",
       "      <td>(3389, 12.59)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 6</th>\n",
       "      <td>(5558, 4572.2)</td>\n",
       "      <td>(9023, 3802.4)</td>\n",
       "      <td>(4856, 1869.53)</td>\n",
       "      <td>(2526, 1816.82)</td>\n",
       "      <td>(9489, 1707.65)</td>\n",
       "      <td>(4879, 1596.94)</td>\n",
       "      <td>(5403, 1559.28)</td>\n",
       "      <td>(9833, 1384.37)</td>\n",
       "      <td>(2360, 1360.57)</td>\n",
       "      <td>(6993, 1312.33)</td>\n",
       "      <td>(279, 1144.06)</td>\n",
       "      <td>(8866, 1130.34)</td>\n",
       "      <td>(2071, 1093.51)</td>\n",
       "      <td>(266, 958.69)</td>\n",
       "      <td>(7527, 891.73)</td>\n",
       "      <td>(452, 873.17)</td>\n",
       "      <td>(731, 804.38)</td>\n",
       "      <td>(6310, 791.13)</td>\n",
       "      <td>(3233, 742.09)</td>\n",
       "      <td>(2474, 661.89)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 7</th>\n",
       "      <td>(7125, 166.56)</td>\n",
       "      <td>(8117, 24.96)</td>\n",
       "      <td>(7248, 21.27)</td>\n",
       "      <td>(1544, 19.62)</td>\n",
       "      <td>(8937, 18.53)</td>\n",
       "      <td>(396, 18.36)</td>\n",
       "      <td>(3280, 17.69)</td>\n",
       "      <td>(2024, 16.29)</td>\n",
       "      <td>(6447, 16.03)</td>\n",
       "      <td>(150, 15.48)</td>\n",
       "      <td>(1629, 14.81)</td>\n",
       "      <td>(164, 14.53)</td>\n",
       "      <td>(6932, 13.78)</td>\n",
       "      <td>(5594, 13.64)</td>\n",
       "      <td>(7365, 13.16)</td>\n",
       "      <td>(5768, 13.02)</td>\n",
       "      <td>(2906, 12.84)</td>\n",
       "      <td>(1908, 10.49)</td>\n",
       "      <td>(5399, 9.63)</td>\n",
       "      <td>(4118, 9.55)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 8</th>\n",
       "      <td>(5741, 39.4)</td>\n",
       "      <td>(8160, 12.77)</td>\n",
       "      <td>(7588, 11.87)</td>\n",
       "      <td>(3350, 10.13)</td>\n",
       "      <td>(8649, 10.05)</td>\n",
       "      <td>(3388, 10.05)</td>\n",
       "      <td>(5337, 10.02)</td>\n",
       "      <td>(4829, 8.72)</td>\n",
       "      <td>(8888, 8.69)</td>\n",
       "      <td>(9664, 8.43)</td>\n",
       "      <td>(9519, 7.84)</td>\n",
       "      <td>(5196, 7.74)</td>\n",
       "      <td>(924, 7.15)</td>\n",
       "      <td>(1241, 6.65)</td>\n",
       "      <td>(9916, 6.65)</td>\n",
       "      <td>(8577, 5.78)</td>\n",
       "      <td>(8226, 5.1)</td>\n",
       "      <td>(1129, 5.05)</td>\n",
       "      <td>(4867, 4.97)</td>\n",
       "      <td>(1526, 4.89)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 9</th>\n",
       "      <td>(6474, 25.17)</td>\n",
       "      <td>(7136, 13.82)</td>\n",
       "      <td>(3385, 10.77)</td>\n",
       "      <td>(3979, 9.76)</td>\n",
       "      <td>(7946, 8.92)</td>\n",
       "      <td>(246, 8.0)</td>\n",
       "      <td>(8690, 7.25)</td>\n",
       "      <td>(5435, 6.95)</td>\n",
       "      <td>(6885, 5.86)</td>\n",
       "      <td>(7838, 5.78)</td>\n",
       "      <td>(4152, 5.13)</td>\n",
       "      <td>(1650, 5.01)</td>\n",
       "      <td>(2160, 4.99)</td>\n",
       "      <td>(2492, 4.93)</td>\n",
       "      <td>(6023, 4.04)</td>\n",
       "      <td>(6288, 4.0)</td>\n",
       "      <td>(5297, 3.96)</td>\n",
       "      <td>(543, 3.95)</td>\n",
       "      <td>(8100, 3.94)</td>\n",
       "      <td>(6412, 3.13)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 10</th>\n",
       "      <td>(6393, 58.3)</td>\n",
       "      <td>(3827, 20.21)</td>\n",
       "      <td>(6533, 17.08)</td>\n",
       "      <td>(1902, 15.77)</td>\n",
       "      <td>(1828, 13.61)</td>\n",
       "      <td>(2880, 12.87)</td>\n",
       "      <td>(541, 9.98)</td>\n",
       "      <td>(2549, 7.95)</td>\n",
       "      <td>(5763, 7.87)</td>\n",
       "      <td>(7027, 7.74)</td>\n",
       "      <td>(183, 7.71)</td>\n",
       "      <td>(919, 6.87)</td>\n",
       "      <td>(9658, 6.02)</td>\n",
       "      <td>(9055, 5.97)</td>\n",
       "      <td>(2691, 5.86)</td>\n",
       "      <td>(2221, 5.71)</td>\n",
       "      <td>(8502, 5.16)</td>\n",
       "      <td>(6532, 5.07)</td>\n",
       "      <td>(9417, 5.06)</td>\n",
       "      <td>(4205, 5.06)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 11</th>\n",
       "      <td>(9023, 657.02)</td>\n",
       "      <td>(7468, 567.26)</td>\n",
       "      <td>(8493, 269.02)</td>\n",
       "      <td>(4133, 174.58)</td>\n",
       "      <td>(334, 163.05)</td>\n",
       "      <td>(511, 160.33)</td>\n",
       "      <td>(3262, 151.73)</td>\n",
       "      <td>(5037, 151.58)</td>\n",
       "      <td>(9489, 149.54)</td>\n",
       "      <td>(8488, 144.44)</td>\n",
       "      <td>(1031, 138.47)</td>\n",
       "      <td>(6311, 130.88)</td>\n",
       "      <td>(9146, 127.19)</td>\n",
       "      <td>(7778, 121.33)</td>\n",
       "      <td>(5560, 116.29)</td>\n",
       "      <td>(7470, 116.19)</td>\n",
       "      <td>(19, 115.47)</td>\n",
       "      <td>(7775, 109.89)</td>\n",
       "      <td>(4715, 98.13)</td>\n",
       "      <td>(8866, 96.69)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 12</th>\n",
       "      <td>(9023, 2043.67)</td>\n",
       "      <td>(9495, 1712.41)</td>\n",
       "      <td>(9489, 1117.94)</td>\n",
       "      <td>(5403, 964.93)</td>\n",
       "      <td>(4365, 923.47)</td>\n",
       "      <td>(5558, 914.17)</td>\n",
       "      <td>(6993, 871.51)</td>\n",
       "      <td>(4856, 752.54)</td>\n",
       "      <td>(731, 711.78)</td>\n",
       "      <td>(7527, 603.38)</td>\n",
       "      <td>(452, 600.32)</td>\n",
       "      <td>(7933, 591.26)</td>\n",
       "      <td>(279, 590.26)</td>\n",
       "      <td>(2360, 569.91)</td>\n",
       "      <td>(8175, 557.0)</td>\n",
       "      <td>(7561, 541.1)</td>\n",
       "      <td>(6310, 489.1)</td>\n",
       "      <td>(7903, 471.44)</td>\n",
       "      <td>(8866, 459.64)</td>\n",
       "      <td>(6120, 456.35)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 13</th>\n",
       "      <td>(8811, 172.33)</td>\n",
       "      <td>(6380, 128.24)</td>\n",
       "      <td>(7260, 83.85)</td>\n",
       "      <td>(4970, 75.7)</td>\n",
       "      <td>(8808, 60.24)</td>\n",
       "      <td>(2274, 56.0)</td>\n",
       "      <td>(5546, 53.44)</td>\n",
       "      <td>(426, 49.44)</td>\n",
       "      <td>(444, 48.07)</td>\n",
       "      <td>(7259, 47.87)</td>\n",
       "      <td>(3466, 44.76)</td>\n",
       "      <td>(5260, 31.72)</td>\n",
       "      <td>(5213, 30.65)</td>\n",
       "      <td>(2243, 25.04)</td>\n",
       "      <td>(1935, 24.86)</td>\n",
       "      <td>(7788, 23.65)</td>\n",
       "      <td>(4513, 23.64)</td>\n",
       "      <td>(9151, 23.28)</td>\n",
       "      <td>(5809, 21.05)</td>\n",
       "      <td>(5986, 20.25)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Word 0           Word 1           Word 2           Word 3  \\\n",
       "Topic 0    (6396, 221.18)   (3861, 156.45)    (2415, 154.5)   (5330, 143.69)   \n",
       "Topic 1    (5871, 259.54)    (8496, 86.19)     (2483, 63.1)    (5050, 50.88)   \n",
       "Topic 2     (1269, 19.85)    (4089, 11.69)     (4649, 9.79)     (3988, 7.93)   \n",
       "Topic 3    (9489, 774.45)   (2071, 691.96)   (9023, 635.56)   (2994, 447.82)   \n",
       "Topic 4    (4179, 910.26)   (3233, 421.58)   (6030, 376.84)   (6993, 348.52)   \n",
       "Topic 5      (647, 92.33)    (8208, 79.19)    (3592, 39.85)    (4289, 37.56)   \n",
       "Topic 6    (5558, 4572.2)   (9023, 3802.4)  (4856, 1869.53)  (2526, 1816.82)   \n",
       "Topic 7    (7125, 166.56)    (8117, 24.96)    (7248, 21.27)    (1544, 19.62)   \n",
       "Topic 8      (5741, 39.4)    (8160, 12.77)    (7588, 11.87)    (3350, 10.13)   \n",
       "Topic 9     (6474, 25.17)    (7136, 13.82)    (3385, 10.77)     (3979, 9.76)   \n",
       "Topic 10     (6393, 58.3)    (3827, 20.21)    (6533, 17.08)    (1902, 15.77)   \n",
       "Topic 11   (9023, 657.02)   (7468, 567.26)   (8493, 269.02)   (4133, 174.58)   \n",
       "Topic 12  (9023, 2043.67)  (9495, 1712.41)  (9489, 1117.94)   (5403, 964.93)   \n",
       "Topic 13   (8811, 172.33)   (6380, 128.24)    (7260, 83.85)     (4970, 75.7)   \n",
       "\n",
       "                   Word 4           Word 5           Word 6           Word 7  \\\n",
       "Topic 0    (1357, 137.79)   (3565, 113.72)   (9138, 106.83)    (2071, 94.07)   \n",
       "Topic 1      (511, 37.19)    (5027, 36.26)     (515, 27.07)    (8188, 26.52)   \n",
       "Topic 2      (9041, 7.11)     (6333, 7.01)     (5817, 6.85)     (5133, 6.78)   \n",
       "Topic 3      (95, 416.09)    (334, 402.85)   (8969, 393.15)   (8493, 386.16)   \n",
       "Topic 4    (9644, 296.13)   (5403, 295.57)    (9613, 294.0)   (7753, 289.52)   \n",
       "Topic 5     (9865, 34.77)    (5118, 27.65)    (6639, 27.24)     (2334, 23.5)   \n",
       "Topic 6   (9489, 1707.65)  (4879, 1596.94)  (5403, 1559.28)  (9833, 1384.37)   \n",
       "Topic 7     (8937, 18.53)     (396, 18.36)    (3280, 17.69)    (2024, 16.29)   \n",
       "Topic 8     (8649, 10.05)    (3388, 10.05)    (5337, 10.02)     (4829, 8.72)   \n",
       "Topic 9      (7946, 8.92)       (246, 8.0)     (8690, 7.25)     (5435, 6.95)   \n",
       "Topic 10    (1828, 13.61)    (2880, 12.87)      (541, 9.98)     (2549, 7.95)   \n",
       "Topic 11    (334, 163.05)    (511, 160.33)   (3262, 151.73)   (5037, 151.58)   \n",
       "Topic 12   (4365, 923.47)   (5558, 914.17)   (6993, 871.51)   (4856, 752.54)   \n",
       "Topic 13    (8808, 60.24)     (2274, 56.0)    (5546, 53.44)     (426, 49.44)   \n",
       "\n",
       "                   Word 8           Word 9         Word 10          Word 11  \\\n",
       "Topic 0      (2292, 83.0)    (1118, 68.83)   (8493, 63.52)    (3505, 62.15)   \n",
       "Topic 1     (7943, 26.22)    (8439, 25.47)   (8445, 25.25)    (4775, 22.62)   \n",
       "Topic 2      (5646, 6.69)     (9905, 6.67)    (6583, 6.67)     (3599, 6.22)   \n",
       "Topic 3    (4133, 340.06)   (4856, 334.16)  (8175, 330.52)   (6401, 316.37)   \n",
       "Topic 4    (4943, 214.63)   (4055, 195.09)  (7527, 176.11)    (9489, 171.6)   \n",
       "Topic 5     (6126, 20.37)    (4313, 19.81)   (5135, 17.49)    (5741, 16.78)   \n",
       "Topic 6   (2360, 1360.57)  (6993, 1312.33)  (279, 1144.06)  (8866, 1130.34)   \n",
       "Topic 7     (6447, 16.03)     (150, 15.48)   (1629, 14.81)     (164, 14.53)   \n",
       "Topic 8      (8888, 8.69)     (9664, 8.43)    (9519, 7.84)     (5196, 7.74)   \n",
       "Topic 9      (6885, 5.86)     (7838, 5.78)    (4152, 5.13)     (1650, 5.01)   \n",
       "Topic 10     (5763, 7.87)     (7027, 7.74)     (183, 7.71)      (919, 6.87)   \n",
       "Topic 11   (9489, 149.54)   (8488, 144.44)  (1031, 138.47)   (6311, 130.88)   \n",
       "Topic 12    (731, 711.78)   (7527, 603.38)   (452, 600.32)   (7933, 591.26)   \n",
       "Topic 13     (444, 48.07)    (7259, 47.87)   (3466, 44.76)    (5260, 31.72)   \n",
       "\n",
       "                  Word 12         Word 13         Word 14         Word 15  \\\n",
       "Topic 0     (9049, 57.71)   (7267, 54.26)   (9048, 54.06)   (3504, 50.48)   \n",
       "Topic 1     (4800, 21.37)   (2206, 21.02)   (1476, 20.41)   (4926, 20.37)   \n",
       "Topic 2      (4830, 5.93)     (3489, 5.9)    (8613, 5.87)    (4893, 5.12)   \n",
       "Topic 3     (279, 311.09)  (2360, 293.64)    (452, 289.6)   (342, 275.95)   \n",
       "Topic 4    (8249, 170.43)   (380, 167.65)  (5558, 167.43)  (7253, 166.41)   \n",
       "Topic 5     (5742, 16.56)      (3, 15.75)   (1009, 14.92)   (1502, 14.53)   \n",
       "Topic 6   (2071, 1093.51)   (266, 958.69)  (7527, 891.73)   (452, 873.17)   \n",
       "Topic 7     (6932, 13.78)   (5594, 13.64)   (7365, 13.16)   (5768, 13.02)   \n",
       "Topic 8       (924, 7.15)    (1241, 6.65)    (9916, 6.65)    (8577, 5.78)   \n",
       "Topic 9      (2160, 4.99)    (2492, 4.93)    (6023, 4.04)     (6288, 4.0)   \n",
       "Topic 10     (9658, 6.02)    (9055, 5.97)    (2691, 5.86)    (2221, 5.71)   \n",
       "Topic 11   (9146, 127.19)  (7778, 121.33)  (5560, 116.29)  (7470, 116.19)   \n",
       "Topic 12    (279, 590.26)  (2360, 569.91)   (8175, 557.0)   (7561, 541.1)   \n",
       "Topic 13    (5213, 30.65)   (2243, 25.04)   (1935, 24.86)   (7788, 23.65)   \n",
       "\n",
       "                 Word 16         Word 17         Word 18         Word 19  \n",
       "Topic 0     (565, 48.94)   (1616, 46.28)   (7527, 44.88)   (6789, 44.08)  \n",
       "Topic 1    (1352, 20.19)   (6613, 18.96)   (4122, 16.53)   (3860, 16.33)  \n",
       "Topic 2     (4886, 5.06)    (9598, 5.03)     (888, 4.99)    (1166, 4.86)  \n",
       "Topic 3   (1234, 271.08)  (8188, 241.73)  (5843, 237.56)  (1506, 237.21)  \n",
       "Topic 4    (1335, 163.3)  (7869, 151.41)  (4856, 142.66)  (7329, 141.64)  \n",
       "Topic 5    (9866, 13.97)   (4576, 13.41)    (657, 13.14)   (3389, 12.59)  \n",
       "Topic 6    (731, 804.38)  (6310, 791.13)  (3233, 742.09)  (2474, 661.89)  \n",
       "Topic 7    (2906, 12.84)   (1908, 10.49)    (5399, 9.63)    (4118, 9.55)  \n",
       "Topic 8      (8226, 5.1)    (1129, 5.05)    (4867, 4.97)    (1526, 4.89)  \n",
       "Topic 9     (5297, 3.96)     (543, 3.95)    (8100, 3.94)    (6412, 3.13)  \n",
       "Topic 10    (8502, 5.16)    (6532, 5.07)    (9417, 5.06)    (4205, 5.06)  \n",
       "Topic 11    (19, 115.47)  (7775, 109.89)   (4715, 98.13)   (8866, 96.69)  \n",
       "Topic 12   (6310, 489.1)  (7903, 471.44)  (8866, 459.64)  (6120, 456.35)  \n",
       "Topic 13   (4513, 23.64)   (9151, 23.28)   (5809, 21.05)   (5986, 20.25)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"top terms info:\")\n",
    "df_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top docs info:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>(562, 0.636)</td>\n",
       "      <td>(377, 0.593)</td>\n",
       "      <td>(61, 0.571)</td>\n",
       "      <td>(1350, 0.553)</td>\n",
       "      <td>(263, 0.516)</td>\n",
       "      <td>(894, 0.49)</td>\n",
       "      <td>(722, 0.481)</td>\n",
       "      <td>(179, 0.477)</td>\n",
       "      <td>(344, 0.473)</td>\n",
       "      <td>(1855, 0.465)</td>\n",
       "      <td>(2386, 0.447)</td>\n",
       "      <td>(1897, 0.405)</td>\n",
       "      <td>(1764, 0.4)</td>\n",
       "      <td>(1069, 0.388)</td>\n",
       "      <td>(1533, 0.387)</td>\n",
       "      <td>(2304, 0.385)</td>\n",
       "      <td>(1566, 0.382)</td>\n",
       "      <td>(415, 0.38)</td>\n",
       "      <td>(5, 0.378)</td>\n",
       "      <td>(1037, 0.371)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>(716, 0.367)</td>\n",
       "      <td>(142, 0.329)</td>\n",
       "      <td>(123, 0.315)</td>\n",
       "      <td>(1312, 0.287)</td>\n",
       "      <td>(87, 0.286)</td>\n",
       "      <td>(48, 0.252)</td>\n",
       "      <td>(1378, 0.236)</td>\n",
       "      <td>(1239, 0.227)</td>\n",
       "      <td>(989, 0.227)</td>\n",
       "      <td>(465, 0.22)</td>\n",
       "      <td>(971, 0.209)</td>\n",
       "      <td>(133, 0.195)</td>\n",
       "      <td>(1274, 0.193)</td>\n",
       "      <td>(9, 0.186)</td>\n",
       "      <td>(357, 0.183)</td>\n",
       "      <td>(1447, 0.178)</td>\n",
       "      <td>(234, 0.175)</td>\n",
       "      <td>(866, 0.173)</td>\n",
       "      <td>(355, 0.171)</td>\n",
       "      <td>(406, 0.17)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>(47, 0.22)</td>\n",
       "      <td>(854, 0.152)</td>\n",
       "      <td>(2526, 0.15)</td>\n",
       "      <td>(796, 0.15)</td>\n",
       "      <td>(142, 0.147)</td>\n",
       "      <td>(1178, 0.135)</td>\n",
       "      <td>(926, 0.13)</td>\n",
       "      <td>(2057, 0.12)</td>\n",
       "      <td>(920, 0.116)</td>\n",
       "      <td>(1266, 0.116)</td>\n",
       "      <td>(1414, 0.112)</td>\n",
       "      <td>(994, 0.099)</td>\n",
       "      <td>(1785, 0.099)</td>\n",
       "      <td>(1638, 0.098)</td>\n",
       "      <td>(1449, 0.098)</td>\n",
       "      <td>(1440, 0.098)</td>\n",
       "      <td>(1242, 0.097)</td>\n",
       "      <td>(2121, 0.093)</td>\n",
       "      <td>(1406, 0.091)</td>\n",
       "      <td>(2188, 0.091)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>(580, 0.964)</td>\n",
       "      <td>(483, 0.943)</td>\n",
       "      <td>(953, 0.942)</td>\n",
       "      <td>(127, 0.937)</td>\n",
       "      <td>(171, 0.909)</td>\n",
       "      <td>(1154, 0.894)</td>\n",
       "      <td>(150, 0.892)</td>\n",
       "      <td>(68, 0.879)</td>\n",
       "      <td>(909, 0.856)</td>\n",
       "      <td>(994, 0.846)</td>\n",
       "      <td>(650, 0.842)</td>\n",
       "      <td>(1046, 0.832)</td>\n",
       "      <td>(214, 0.821)</td>\n",
       "      <td>(1594, 0.819)</td>\n",
       "      <td>(704, 0.81)</td>\n",
       "      <td>(122, 0.807)</td>\n",
       "      <td>(229, 0.807)</td>\n",
       "      <td>(294, 0.805)</td>\n",
       "      <td>(1193, 0.803)</td>\n",
       "      <td>(543, 0.796)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>(1222, 0.785)</td>\n",
       "      <td>(416, 0.781)</td>\n",
       "      <td>(1223, 0.779)</td>\n",
       "      <td>(1550, 0.775)</td>\n",
       "      <td>(745, 0.764)</td>\n",
       "      <td>(1811, 0.746)</td>\n",
       "      <td>(2520, 0.737)</td>\n",
       "      <td>(695, 0.732)</td>\n",
       "      <td>(662, 0.726)</td>\n",
       "      <td>(291, 0.726)</td>\n",
       "      <td>(1111, 0.725)</td>\n",
       "      <td>(166, 0.718)</td>\n",
       "      <td>(399, 0.716)</td>\n",
       "      <td>(2339, 0.716)</td>\n",
       "      <td>(1410, 0.71)</td>\n",
       "      <td>(2580, 0.706)</td>\n",
       "      <td>(1020, 0.704)</td>\n",
       "      <td>(393, 0.699)</td>\n",
       "      <td>(2026, 0.698)</td>\n",
       "      <td>(1694, 0.692)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 5</th>\n",
       "      <td>(1570, 0.376)</td>\n",
       "      <td>(158, 0.35)</td>\n",
       "      <td>(477, 0.339)</td>\n",
       "      <td>(1117, 0.323)</td>\n",
       "      <td>(1606, 0.311)</td>\n",
       "      <td>(1874, 0.247)</td>\n",
       "      <td>(757, 0.243)</td>\n",
       "      <td>(737, 0.215)</td>\n",
       "      <td>(30, 0.214)</td>\n",
       "      <td>(2004, 0.212)</td>\n",
       "      <td>(1306, 0.207)</td>\n",
       "      <td>(778, 0.201)</td>\n",
       "      <td>(126, 0.199)</td>\n",
       "      <td>(1167, 0.18)</td>\n",
       "      <td>(2219, 0.172)</td>\n",
       "      <td>(2433, 0.165)</td>\n",
       "      <td>(1175, 0.164)</td>\n",
       "      <td>(581, 0.153)</td>\n",
       "      <td>(2118, 0.151)</td>\n",
       "      <td>(2325, 0.15)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 6</th>\n",
       "      <td>(809, 0.992)</td>\n",
       "      <td>(535, 0.992)</td>\n",
       "      <td>(98, 0.991)</td>\n",
       "      <td>(742, 0.991)</td>\n",
       "      <td>(438, 0.991)</td>\n",
       "      <td>(1901, 0.991)</td>\n",
       "      <td>(173, 0.99)</td>\n",
       "      <td>(2240, 0.99)</td>\n",
       "      <td>(95, 0.99)</td>\n",
       "      <td>(481, 0.989)</td>\n",
       "      <td>(480, 0.989)</td>\n",
       "      <td>(2365, 0.989)</td>\n",
       "      <td>(1157, 0.988)</td>\n",
       "      <td>(991, 0.987)</td>\n",
       "      <td>(2237, 0.987)</td>\n",
       "      <td>(2593, 0.987)</td>\n",
       "      <td>(2579, 0.987)</td>\n",
       "      <td>(332, 0.987)</td>\n",
       "      <td>(2141, 0.986)</td>\n",
       "      <td>(2550, 0.986)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 7</th>\n",
       "      <td>(81, 0.439)</td>\n",
       "      <td>(75, 0.327)</td>\n",
       "      <td>(996, 0.304)</td>\n",
       "      <td>(1928, 0.23)</td>\n",
       "      <td>(42, 0.224)</td>\n",
       "      <td>(935, 0.206)</td>\n",
       "      <td>(1273, 0.201)</td>\n",
       "      <td>(830, 0.199)</td>\n",
       "      <td>(2453, 0.195)</td>\n",
       "      <td>(413, 0.184)</td>\n",
       "      <td>(453, 0.176)</td>\n",
       "      <td>(802, 0.168)</td>\n",
       "      <td>(882, 0.158)</td>\n",
       "      <td>(1243, 0.146)</td>\n",
       "      <td>(1761, 0.145)</td>\n",
       "      <td>(1567, 0.142)</td>\n",
       "      <td>(2114, 0.142)</td>\n",
       "      <td>(2040, 0.139)</td>\n",
       "      <td>(1272, 0.139)</td>\n",
       "      <td>(1548, 0.134)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 8</th>\n",
       "      <td>(1319, 0.225)</td>\n",
       "      <td>(872, 0.199)</td>\n",
       "      <td>(1024, 0.177)</td>\n",
       "      <td>(1958, 0.176)</td>\n",
       "      <td>(124, 0.156)</td>\n",
       "      <td>(1625, 0.155)</td>\n",
       "      <td>(711, 0.152)</td>\n",
       "      <td>(1278, 0.129)</td>\n",
       "      <td>(2195, 0.122)</td>\n",
       "      <td>(856, 0.121)</td>\n",
       "      <td>(1287, 0.116)</td>\n",
       "      <td>(1450, 0.112)</td>\n",
       "      <td>(2233, 0.107)</td>\n",
       "      <td>(1199, 0.105)</td>\n",
       "      <td>(1206, 0.103)</td>\n",
       "      <td>(1543, 0.093)</td>\n",
       "      <td>(2420, 0.091)</td>\n",
       "      <td>(1925, 0.085)</td>\n",
       "      <td>(2194, 0.082)</td>\n",
       "      <td>(981, 0.081)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 9</th>\n",
       "      <td>(1309, 0.196)</td>\n",
       "      <td>(1079, 0.196)</td>\n",
       "      <td>(1400, 0.189)</td>\n",
       "      <td>(1813, 0.159)</td>\n",
       "      <td>(1310, 0.146)</td>\n",
       "      <td>(1744, 0.141)</td>\n",
       "      <td>(2516, 0.129)</td>\n",
       "      <td>(1822, 0.122)</td>\n",
       "      <td>(1762, 0.121)</td>\n",
       "      <td>(2158, 0.115)</td>\n",
       "      <td>(1415, 0.113)</td>\n",
       "      <td>(1133, 0.11)</td>\n",
       "      <td>(1868, 0.105)</td>\n",
       "      <td>(1479, 0.101)</td>\n",
       "      <td>(2440, 0.098)</td>\n",
       "      <td>(2422, 0.094)</td>\n",
       "      <td>(1542, 0.091)</td>\n",
       "      <td>(897, 0.076)</td>\n",
       "      <td>(1719, 0.071)</td>\n",
       "      <td>(2199, 0.068)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 10</th>\n",
       "      <td>(970, 0.258)</td>\n",
       "      <td>(1442, 0.224)</td>\n",
       "      <td>(1499, 0.213)</td>\n",
       "      <td>(2248, 0.192)</td>\n",
       "      <td>(2029, 0.19)</td>\n",
       "      <td>(1770, 0.151)</td>\n",
       "      <td>(1164, 0.142)</td>\n",
       "      <td>(1668, 0.132)</td>\n",
       "      <td>(1340, 0.128)</td>\n",
       "      <td>(1407, 0.122)</td>\n",
       "      <td>(1127, 0.114)</td>\n",
       "      <td>(1343, 0.112)</td>\n",
       "      <td>(2009, 0.108)</td>\n",
       "      <td>(768, 0.107)</td>\n",
       "      <td>(1740, 0.105)</td>\n",
       "      <td>(1141, 0.104)</td>\n",
       "      <td>(1817, 0.102)</td>\n",
       "      <td>(1129, 0.1)</td>\n",
       "      <td>(1658, 0.1)</td>\n",
       "      <td>(112, 0.099)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 11</th>\n",
       "      <td>(41, 0.992)</td>\n",
       "      <td>(97, 0.713)</td>\n",
       "      <td>(1044, 0.695)</td>\n",
       "      <td>(308, 0.646)</td>\n",
       "      <td>(106, 0.643)</td>\n",
       "      <td>(656, 0.633)</td>\n",
       "      <td>(1305, 0.621)</td>\n",
       "      <td>(317, 0.611)</td>\n",
       "      <td>(85, 0.61)</td>\n",
       "      <td>(53, 0.605)</td>\n",
       "      <td>(2280, 0.598)</td>\n",
       "      <td>(617, 0.593)</td>\n",
       "      <td>(266, 0.589)</td>\n",
       "      <td>(198, 0.576)</td>\n",
       "      <td>(2023, 0.566)</td>\n",
       "      <td>(73, 0.561)</td>\n",
       "      <td>(653, 0.553)</td>\n",
       "      <td>(117, 0.551)</td>\n",
       "      <td>(205, 0.551)</td>\n",
       "      <td>(6, 0.55)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 12</th>\n",
       "      <td>(783, 0.993)</td>\n",
       "      <td>(144, 0.993)</td>\n",
       "      <td>(76, 0.992)</td>\n",
       "      <td>(280, 0.992)</td>\n",
       "      <td>(1896, 0.991)</td>\n",
       "      <td>(642, 0.991)</td>\n",
       "      <td>(129, 0.991)</td>\n",
       "      <td>(348, 0.991)</td>\n",
       "      <td>(371, 0.99)</td>\n",
       "      <td>(379, 0.99)</td>\n",
       "      <td>(946, 0.99)</td>\n",
       "      <td>(463, 0.99)</td>\n",
       "      <td>(1528, 0.99)</td>\n",
       "      <td>(623, 0.99)</td>\n",
       "      <td>(827, 0.989)</td>\n",
       "      <td>(967, 0.989)</td>\n",
       "      <td>(575, 0.989)</td>\n",
       "      <td>(1382, 0.988)</td>\n",
       "      <td>(1270, 0.987)</td>\n",
       "      <td>(439, 0.987)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 13</th>\n",
       "      <td>(2, 0.599)</td>\n",
       "      <td>(489, 0.456)</td>\n",
       "      <td>(273, 0.398)</td>\n",
       "      <td>(1013, 0.379)</td>\n",
       "      <td>(115, 0.378)</td>\n",
       "      <td>(49, 0.364)</td>\n",
       "      <td>(181, 0.337)</td>\n",
       "      <td>(82, 0.335)</td>\n",
       "      <td>(335, 0.332)</td>\n",
       "      <td>(8, 0.329)</td>\n",
       "      <td>(540, 0.315)</td>\n",
       "      <td>(528, 0.296)</td>\n",
       "      <td>(105, 0.292)</td>\n",
       "      <td>(1602, 0.283)</td>\n",
       "      <td>(1298, 0.282)</td>\n",
       "      <td>(1054, 0.278)</td>\n",
       "      <td>(913, 0.273)</td>\n",
       "      <td>(564, 0.265)</td>\n",
       "      <td>(2045, 0.255)</td>\n",
       "      <td>(70, 0.243)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0              1              2              3  \\\n",
       "Topic 0    (562, 0.636)   (377, 0.593)    (61, 0.571)  (1350, 0.553)   \n",
       "Topic 1    (716, 0.367)   (142, 0.329)   (123, 0.315)  (1312, 0.287)   \n",
       "Topic 2      (47, 0.22)   (854, 0.152)   (2526, 0.15)    (796, 0.15)   \n",
       "Topic 3    (580, 0.964)   (483, 0.943)   (953, 0.942)   (127, 0.937)   \n",
       "Topic 4   (1222, 0.785)   (416, 0.781)  (1223, 0.779)  (1550, 0.775)   \n",
       "Topic 5   (1570, 0.376)    (158, 0.35)   (477, 0.339)  (1117, 0.323)   \n",
       "Topic 6    (809, 0.992)   (535, 0.992)    (98, 0.991)   (742, 0.991)   \n",
       "Topic 7     (81, 0.439)    (75, 0.327)   (996, 0.304)   (1928, 0.23)   \n",
       "Topic 8   (1319, 0.225)   (872, 0.199)  (1024, 0.177)  (1958, 0.176)   \n",
       "Topic 9   (1309, 0.196)  (1079, 0.196)  (1400, 0.189)  (1813, 0.159)   \n",
       "Topic 10   (970, 0.258)  (1442, 0.224)  (1499, 0.213)  (2248, 0.192)   \n",
       "Topic 11    (41, 0.992)    (97, 0.713)  (1044, 0.695)   (308, 0.646)   \n",
       "Topic 12   (783, 0.993)   (144, 0.993)    (76, 0.992)   (280, 0.992)   \n",
       "Topic 13     (2, 0.599)   (489, 0.456)   (273, 0.398)  (1013, 0.379)   \n",
       "\n",
       "                      4              5              6              7  \\\n",
       "Topic 0    (263, 0.516)    (894, 0.49)   (722, 0.481)   (179, 0.477)   \n",
       "Topic 1     (87, 0.286)    (48, 0.252)  (1378, 0.236)  (1239, 0.227)   \n",
       "Topic 2    (142, 0.147)  (1178, 0.135)    (926, 0.13)   (2057, 0.12)   \n",
       "Topic 3    (171, 0.909)  (1154, 0.894)   (150, 0.892)    (68, 0.879)   \n",
       "Topic 4    (745, 0.764)  (1811, 0.746)  (2520, 0.737)   (695, 0.732)   \n",
       "Topic 5   (1606, 0.311)  (1874, 0.247)   (757, 0.243)   (737, 0.215)   \n",
       "Topic 6    (438, 0.991)  (1901, 0.991)    (173, 0.99)   (2240, 0.99)   \n",
       "Topic 7     (42, 0.224)   (935, 0.206)  (1273, 0.201)   (830, 0.199)   \n",
       "Topic 8    (124, 0.156)  (1625, 0.155)   (711, 0.152)  (1278, 0.129)   \n",
       "Topic 9   (1310, 0.146)  (1744, 0.141)  (2516, 0.129)  (1822, 0.122)   \n",
       "Topic 10   (2029, 0.19)  (1770, 0.151)  (1164, 0.142)  (1668, 0.132)   \n",
       "Topic 11   (106, 0.643)   (656, 0.633)  (1305, 0.621)   (317, 0.611)   \n",
       "Topic 12  (1896, 0.991)   (642, 0.991)   (129, 0.991)   (348, 0.991)   \n",
       "Topic 13   (115, 0.378)    (49, 0.364)   (181, 0.337)    (82, 0.335)   \n",
       "\n",
       "                      8              9             10             11  \\\n",
       "Topic 0    (344, 0.473)  (1855, 0.465)  (2386, 0.447)  (1897, 0.405)   \n",
       "Topic 1    (989, 0.227)    (465, 0.22)   (971, 0.209)   (133, 0.195)   \n",
       "Topic 2    (920, 0.116)  (1266, 0.116)  (1414, 0.112)   (994, 0.099)   \n",
       "Topic 3    (909, 0.856)   (994, 0.846)   (650, 0.842)  (1046, 0.832)   \n",
       "Topic 4    (662, 0.726)   (291, 0.726)  (1111, 0.725)   (166, 0.718)   \n",
       "Topic 5     (30, 0.214)  (2004, 0.212)  (1306, 0.207)   (778, 0.201)   \n",
       "Topic 6      (95, 0.99)   (481, 0.989)   (480, 0.989)  (2365, 0.989)   \n",
       "Topic 7   (2453, 0.195)   (413, 0.184)   (453, 0.176)   (802, 0.168)   \n",
       "Topic 8   (2195, 0.122)   (856, 0.121)  (1287, 0.116)  (1450, 0.112)   \n",
       "Topic 9   (1762, 0.121)  (2158, 0.115)  (1415, 0.113)   (1133, 0.11)   \n",
       "Topic 10  (1340, 0.128)  (1407, 0.122)  (1127, 0.114)  (1343, 0.112)   \n",
       "Topic 11     (85, 0.61)    (53, 0.605)  (2280, 0.598)   (617, 0.593)   \n",
       "Topic 12    (371, 0.99)    (379, 0.99)    (946, 0.99)    (463, 0.99)   \n",
       "Topic 13   (335, 0.332)     (8, 0.329)   (540, 0.315)   (528, 0.296)   \n",
       "\n",
       "                     12             13             14             15  \\\n",
       "Topic 0     (1764, 0.4)  (1069, 0.388)  (1533, 0.387)  (2304, 0.385)   \n",
       "Topic 1   (1274, 0.193)     (9, 0.186)   (357, 0.183)  (1447, 0.178)   \n",
       "Topic 2   (1785, 0.099)  (1638, 0.098)  (1449, 0.098)  (1440, 0.098)   \n",
       "Topic 3    (214, 0.821)  (1594, 0.819)    (704, 0.81)   (122, 0.807)   \n",
       "Topic 4    (399, 0.716)  (2339, 0.716)   (1410, 0.71)  (2580, 0.706)   \n",
       "Topic 5    (126, 0.199)   (1167, 0.18)  (2219, 0.172)  (2433, 0.165)   \n",
       "Topic 6   (1157, 0.988)   (991, 0.987)  (2237, 0.987)  (2593, 0.987)   \n",
       "Topic 7    (882, 0.158)  (1243, 0.146)  (1761, 0.145)  (1567, 0.142)   \n",
       "Topic 8   (2233, 0.107)  (1199, 0.105)  (1206, 0.103)  (1543, 0.093)   \n",
       "Topic 9   (1868, 0.105)  (1479, 0.101)  (2440, 0.098)  (2422, 0.094)   \n",
       "Topic 10  (2009, 0.108)   (768, 0.107)  (1740, 0.105)  (1141, 0.104)   \n",
       "Topic 11   (266, 0.589)   (198, 0.576)  (2023, 0.566)    (73, 0.561)   \n",
       "Topic 12   (1528, 0.99)    (623, 0.99)   (827, 0.989)   (967, 0.989)   \n",
       "Topic 13   (105, 0.292)  (1602, 0.283)  (1298, 0.282)  (1054, 0.278)   \n",
       "\n",
       "                     16             17             18             19  \n",
       "Topic 0   (1566, 0.382)    (415, 0.38)     (5, 0.378)  (1037, 0.371)  \n",
       "Topic 1    (234, 0.175)   (866, 0.173)   (355, 0.171)    (406, 0.17)  \n",
       "Topic 2   (1242, 0.097)  (2121, 0.093)  (1406, 0.091)  (2188, 0.091)  \n",
       "Topic 3    (229, 0.807)   (294, 0.805)  (1193, 0.803)   (543, 0.796)  \n",
       "Topic 4   (1020, 0.704)   (393, 0.699)  (2026, 0.698)  (1694, 0.692)  \n",
       "Topic 5   (1175, 0.164)   (581, 0.153)  (2118, 0.151)   (2325, 0.15)  \n",
       "Topic 6   (2579, 0.987)   (332, 0.987)  (2141, 0.986)  (2550, 0.986)  \n",
       "Topic 7   (2114, 0.142)  (2040, 0.139)  (1272, 0.139)  (1548, 0.134)  \n",
       "Topic 8   (2420, 0.091)  (1925, 0.085)  (2194, 0.082)   (981, 0.081)  \n",
       "Topic 9   (1542, 0.091)   (897, 0.076)  (1719, 0.071)  (2199, 0.068)  \n",
       "Topic 10  (1817, 0.102)    (1129, 0.1)    (1658, 0.1)   (112, 0.099)  \n",
       "Topic 11   (653, 0.553)   (117, 0.551)   (205, 0.551)      (6, 0.55)  \n",
       "Topic 12   (575, 0.989)  (1382, 0.988)  (1270, 0.987)   (439, 0.987)  \n",
       "Topic 13   (913, 0.273)   (564, 0.265)  (2045, 0.255)    (70, 0.243)  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"top docs info:\")\n",
    "df_top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis.pyLDA(topic_word, doc_topic, [len(s) for s in [word_tokenize(corp) for corp in _input]], vec.get_feature_names(), np.array(sum(tf).todense())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.33 2 716 2\n",
      "representational state transfer style be become a de facto standard adopt by software organization to build they web application .\n",
      "REpresentational State Transfer (REST) style is becoming a de facto standard adopted by software organizations to build their Web applications.\n",
      "[(0, 1, 2, 3), (5,), None]\n",
      "[(0, 1, 2, 3), (9,), None]\n",
      "========================================\n",
      "0.04 1 716 1\n",
      "understandability and reusability be two important characteristic of software quality .\n",
      "Understandability and reusability are two important characteristics of software quality.\n",
      "['Understandability', 'and', 'reusability']-['are']-['important', 'characteristics', 'of', 'quality']\n",
      "========================================\n",
      "0.04 1 716 0\n",
      "Identifier lexicon may have a direct impact on software understandability and reusability and , thus , on the quality of the final software product .\n",
      "Identifier lexicon may have a direct impact on software understandability and reusability and, thus, on the quality of the final software product.\n",
      "['Identifier', 'lexicon']-['have']-['direct', 'impact', 'on', 'understandability']\n",
      "['Identifier', 'lexicon']-['have']-['quality', 'of', 'product']\n",
      "========================================\n",
      "0.02 1 716 3\n",
      "understandable and reusable Uniform Resource Identifers be important to attract client developer of restful api because good uri support the client developer to understand and reuse the api .\n",
      "Understandable and reusable Uniform Resource Identifers (URIs) are important to attract client developers of RESTful APIs because good URIs support the client developers to understand and reuse the APIs.\n",
      "['Understandable', 'reusable', 'Uniform', 'Resource', 'Identifers']-['are']-['important']\n",
      "['Understandable', 'reusable', 'Uniform', 'Resource', 'Identifers']-['attract']-['client', 'developers', 'of', 'APIs']\n",
      "['good', 'URIs']-['support']-['client', 'developers']\n",
      "['good', 'URIs']-['understand']-['APIs']\n",
      "[(16, 17), (25,), None]\n",
      "========================================\n",
      "1.04 2 123 5\n",
      "as a application , we embed delegate news selection into a simple beauty-contest model to demonstrate how it affect action in a setting with strategic interaction .\n",
      "As an application, we embed delegated news selection into a simple beauty-contest model to demonstrate how it affects actions in a setting with strategic interactions.\n",
      "['we']-['embed']-['delegated', 'news', 'selection']\n",
      "========================================\n",
      "1.04 2 123 4\n",
      "we prove that agent can always reduce the entropy of they posterior belief by delegate they information choice , state-dependent reporting convey information not only via the contents of a story , but also via the decision of what to report , and a event that be report by all news provider be common knowledge among agent only if it be also consider maximally newsworthy by all provider .\n",
      "We prove that (i) agents can always reduce the entropy of their posterior beliefs by delegating their information choice, (ii) state-dependent reporting conveys information not only via the contents of a story, but also via the decision of what to report, and (iii) an event that is reported by all news providers is common knowledge among agents only if it is also considered maximally newsworthy by all providers.\n",
      "[(59,), (62,), None]\n",
      "[(59,), (64,), None]\n",
      "========================================\n",
      "1.0 1 123 3\n",
      "we propose a theoretical framework that formalize this type of state-dependent editorial behavior by introduce news selection function .\n",
      "We propose a theoretical framework that formalizes this type of state-dependent editorial behavior by introducing news selection functions.\n",
      "['We']-['propose']-['theoretical', 'framework']\n",
      "========================================\n",
      "1.0 1 123 2\n",
      "we document empirically that , while different outlet typically emphasize different topic , major event shift the general news focus and make coverage more homogeneous .\n",
      "We document empirically that, while different outlets typically emphasize different topics, major events shift the general news focus and make coverage more homogeneous.\n",
      "[(0,), (1,), None]\n",
      "['major', 'events']-['shift']-['general', 'news', 'focus']\n",
      "[(13, 14), (21,), None]\n",
      "['different', 'outlets']-['emphasize']-['different', 'topics']\n",
      "[(22,), (24,), None]\n",
      "========================================\n",
      "1.0 1 123 1\n",
      "therefore , many delegate they information choice to specialize news provider that monitor the world on they behalf and report only a curated selection of event .\n",
      "Therefore, many delegate their information choice to specialized news providers that monitor the world on their behalf and report only a curated selection of events.\n",
      "['many']-['delegate']-['information', 'choice']\n",
      "['many']-['delegate']-['specialized', 'news', 'providers']\n",
      "========================================\n",
      "0.3 2 1312 6\n",
      "we train a player recognition system on the PGA model 's output to verify that its discovery about gameplay be in fact independent of each player 's play style .\n",
      "We train a player recognition system on the PGA model's output to verify that its discoveries about gameplay are in fact independent of each player's play style.\n",
      "['We']-['train']-['player', 'recognition', 'system', 'on', 'output']\n",
      "[(0,), (13,), None]\n",
      "['discoveries', 'about', 'gameplay']-['are']-['in', 'fact', 'independent', 'of', 'style']\n",
      "========================================\n",
      "0.3 2 1312 5\n",
      "we begin with latent dirichlet allocation , the simplest topic model , then develop the player-gameplay action model to make the same type of discovery about gameplay in a way that be independent of each player 's play style .\n",
      "We begin with latent Dirichlet allocation (LDA), the simplest topic model, then develop the player-gameplay action (PGA) model to make the same types of discoveries about gameplay in a way that is independent of each player's play style.\n",
      "['We']-['begin', 'with']-['latent', 'Dirichlet', 'allocation']\n",
      "========================================\n",
      "0.3 2 1312 4\n",
      "they can use this information to verify that they level feature the appropriate style of gameplay and to recommend level with gameplay that be similar to level that player like .\n",
      "They can use this information to verify that their levels feature the appropriate style of gameplay and to recommend levels with gameplay that is similar to levels that players like.\n",
      "['They']-['use']-['information']\n",
      "[(0,), (6,), None]\n",
      "['levels']-['feature']-['appropriate', 'style', 'of', 'gameplay']\n",
      "['levels']-['recommend']-['levels', 'with', 'gameplay']\n",
      "========================================\n",
      "0.01 1 1312 7\n",
      "the system recognize player with over 90 % accuracy in about 20 seconds of playtime .\n",
      "The system recognizes players with over 90% accuracy in about 20 seconds of playtime.\n",
      "['system']-['recognizes']-['players']\n",
      "['%']-['accuracy', 'in']-['seconds', 'of', 'playtime']\n",
      "========================================\n",
      "0.01 1 1312 2\n",
      "in this paper , we show that analyze player ' controller input use probabilistic topic model allow game developer to describe the type of gameplay or action - in game in a quantitative way .\n",
      "In this paper, we show that analyzing players' controller inputs using probabilistic topic models allows game developers to describe the types of gameplay or action - in games in a quantitative way.\n",
      "['we']-['show']-['paper']\n",
      "['we']-['allows']-['game', 'developers']\n",
      "['game', 'developers']-['describe']-['types', 'of', 'gameplay', 'in', 'games']\n",
      "========================================\n",
      "0.01 1 1312 0\n",
      "today 's game analytic system be power by event log , which reveal information about what player be do but offer little insight about the type of gameplay that game foster .\n",
      "Today's game analytics systems are powered by event logs, which reveal information about what players are doing but offer little insight about the types of gameplay that games foster.\n",
      "[(2, 3, 4), (6,), None]\n",
      "========================================\n",
      "0.21 4 48 5\n",
      "although many approach be use to generate effective stop word , the list be outdate or too general to apply to mining software artifact .\n",
      "Although many approaches are used to generate effective stop words, the lists are outdated or too general to apply to mining software artifacts.\n",
      "[(12,), (14,), None]\n",
      "[(1, 2), (4,), None]\n",
      "['lists']-['apply', 'to']-['mining', 'software', 'artifacts']\n",
      "['many', 'approaches']-['generate']-['effective', 'stop', 'words']\n",
      "========================================\n",
      "0.12 3 48 7\n",
      "to resolve these problem , we propose a automatic stop word generation approach for topic model of software artifact .\n",
      "To resolve these problems, we propose an automatic stop word generation approach for topic models of software artifacts.\n",
      "['we']-['propose']-['automatic', 'stop', 'word', 'generation', 'approach', 'for', 'models']\n",
      "['we']-['resolve']-['problems']\n",
      "========================================\n",
      "0.12 2 48 9\n",
      "through we experiment , we prove that we stop word list result in a higher performance of the topic model than list from other approach .\n",
      "Through our experiment, we proved that our stop words list results in a higher performance of the topic model than lists from other approaches.\n",
      "['we']-['proved']-['experiment']\n",
      "['stop', 'words', 'list']-['results', 'in']-['higher', 'performance', 'of', 'model']\n",
      "========================================\n",
      "0.12 2 48 8\n",
      "by measure topic coherence among word in the topic use Pointwise Mutual Information , we add word with a low pmi score to we stop word list for every topic modeling loop .\n",
      "By measuring topic coherence among words in the topic using Pointwise Mutual Information (PMI), we added words with a low PMI score to our stop words list for every topic modeling loop.\n",
      "['we']-['added']-['words', 'with', 'score']\n",
      "['we']-['measuring']-['topic', 'coherence']\n",
      "========================================\n",
      "0.09 2 48 3\n",
      "these software artifact characteristic worsen the performance of topic modeling .\n",
      "These software artifact characteristics worsen the performance of topic modeling.\n",
      "['software', 'artifact', 'characteristics']-['worsen']-['performance', 'of', 'modeling']\n",
      "========================================\n",
      "0.09 2 48 2\n",
      "however , software artifact be unstructured and contain a mix of textual type within the natural text .\n",
      "However, software artifacts are unstructured and contain a mix of textual types within the natural text.\n",
      "['software', 'artifacts']-['are']-['unstructured', 'and', 'contain']\n",
      "========================================\n",
      "0.09 2 48 1\n",
      "topic modeling in particular have be widely use to discover meaningful information from software artifact .\n",
      "Topic modeling in particular has been widely used to discover meaningful information from software artifacts.\n",
      "[(0, 1, 2, 3), (7,), None]\n",
      "['Topic', 'modeling', 'in', 'particular']-['discover']-['meaningful', 'information']\n",
      "['Topic', 'modeling', 'in', 'particular']-['discover']-['software', 'artifacts']\n",
      "========================================\n",
      "0.09 2 48 0\n",
      "mining software artifact be a useful way to understand the source code of software project .\n",
      "Mining software artifacts is a useful way to understand the source code of software projects.\n",
      "['Mining', 'software', 'artifacts']-['is']-['useful', 'way']\n",
      "========================================\n",
      "0.04 1 48 4\n",
      "among several natural language preprocess task , remove stop word to reduce meaningless and uninteresting term be a efficient way to improve the quality of topic model .\n",
      "Among several natural language preprocessing tasks, removing stop words to reduce meaningless and uninteresting terms is an efficient way to improve the quality of topic models.\n",
      "['preprocessing', 'tasks']-['is']-['efficient', 'way']\n",
      "========================================\n",
      "1.09 3 1378 8\n",
      "the first task , full headline , be to retrieve full headline use for a news article give the body of news article .\n",
      "The first task, full headline, is to retrieve full headline used for a news article given the body of news article.\n",
      "[(1, 2), (7,), None]\n",
      "['first', 'task']-['retrieve']-['full', 'headline']\n",
      "========================================\n",
      "1.09 2 1378 2\n",
      "in this paper , we conduct a large-scale analysis and modeling of 150k news article publish over a period of four month on the Yahoo home page .\n",
      "In this paper, we conduct a large-scale analysis and modeling of 150K news articles published over a period of four months on the Yahoo home page.\n",
      "['we']-['conduct']-['large-scale', 'analysis', 'and', 'modeling']\n",
      "['we']-['conduct']-['modeling', 'of', 'articles']\n",
      "========================================\n",
      "1.09 3 1378 1\n",
      "traditionally , journalist have follow rules-of-thumb and experience to master the art of craft catchy headline , but with the valuable resource of large-scale click-through datum of online news article , we can apply quantitative analysis and text mining technique to acquire a in-depth understanding of headline .\n",
      "Traditionally, journalists have followed rules-of-thumb and experience to master the art of crafting catchy headlines, but with the valuable resource of large-scale click-through data of online news articles, we can apply quantitative analysis and text mining techniques to acquire an in-depth understanding of headlines.\n",
      "['we']-['apply']-['quantitative', 'analysis', 'and', 'techniques']\n",
      "['we']-['apply']-['text', 'mining', 'techniques']\n",
      "['journalists']-['followed']-['rules-of-thumb', 'and', 'experience']\n",
      "['we']-['acquire']-['in-depth', 'understanding', 'of', 'headlines']\n",
      "['rules-of-thumb', 'and', 'experience']-['master']-['art']\n",
      "['journalists']-['resource', 'of']-['large-scale', 'click-through', 'data', 'of', 'articles']\n",
      "========================================\n",
      "1.04 3 1378 0\n",
      "headline be particularly important for online news outlet where there be many similar news story compete for user ' attention .\n",
      "Headlines are particularly important for online news outlets where there are many similar news stories competing for users' attention.\n",
      "['Headlines']-['are']-['important', 'for', 'outlets']\n",
      "========================================\n",
      "1.0 2 1378 11\n",
      "for good headline task , which be of more practical importance to both individual journalist and online news outlet , we model significantly outperform all other comparative method .\n",
      "For good headline task, which is of more practical importance to both individual journalists and online news outlets, our model significantly outperforms all other comparative methods.\n",
      "['model']-['outperforms']-['good', 'headline', 'task']\n",
      "['model']-['outperforms']-['other', 'comparative', 'methods']\n",
      "========================================\n",
      "1.0 2 1378 9\n",
      "the second task , good headline , be to specifically identify word in the headline that have high click value for current news audience .\n",
      "The second task, good headline, is to specifically identify words in the headline that have high click values for current news audience.\n",
      "[(1, 2), (7,), None]\n",
      "['second', 'task']-['identify']-['words', 'in', 'headline']\n",
      "========================================\n",
      "0.04 1 1378 7\n",
      "we evaluate hctm in two different experimental setting and compare its performance with ALDA , LDA , and TextRank .\n",
      "We evaluate HCTM in two different experimental settings and compare its performance with ALDA (adapted LDA), LDA, and TextRank.\n",
      "['We']-['evaluate']-['different', 'experimental', 'settings']\n",
      "========================================\n",
      "0.09 1 1239 1\n",
      "the document in a corpus that consist of summary of conference and journal article classify by naive baye , support Vector Machines and Random forest method and they performer have be compair .\n",
      "The documents in a corpus that consists of summaries of conference and journal articles classified by Naive Bayes, Support Vector Machines and Random Forests methods and their performers have been compaired.\n",
      "[(1, 2, 4, 21, 25, 26, 28), (31,), None]\n",
      "[(19, 20, 21, 22, 25), (31,), None]\n",
      "========================================\n",
      "0.04 1 1239 2\n",
      "all the model that have employ preprocess with stem and stop word elimination have yield between 2.26 % and 4.94 % improvement in performance to the model that have not employ such preprocessing .\n",
      "All the models that have employed preprocessing with stemming and stop words elimination have yielded between 2.26% and 4.94% improvement in performance to the models that have not employed such preprocessing.\n",
      "['models']-['yielded']-['performance', 'to', 'models']\n",
      "========================================\n",
      "0.04 1 1239 0\n",
      "in this study , the effect of the application of stop word filter and stem method on the classification of turkish text .\n",
      "In this study, the effects of the application of stop words filtering and stemming methods on the classification of Turkish Texts.\n",
      "========================================\n",
      "0.02 1 989 4\n",
      "first , we find evidence for density dependence , a inverse-u-shaped relationship between the density of a petition 's niche and the number of signature the petition obtain .\n",
      "First, we find evidence for density dependence, an inverse-U-shaped relationship between the density of a petition's niche and the number of signatures the petition obtains.\n",
      "['we']-['find']-['evidence', 'for', 'dependence']\n",
      "========================================\n",
      "0.04 1 465 2\n",
      "specifically , we first propose Relational Rules , a novel representation scheme for cooperative game with overlap coalition , which encode the aforementioned relation , and which extend the well-known MC net representation to this setting .\n",
      "Specifically, we first propose Relational Rules, a novel representation scheme for cooperative games with overlapping coalitions, which encodes the aforementioned relations, and which extends the well-known MC nets representation to this setting.\n",
      "['we']-['propose']-['Relational', 'Rules']\n",
      "========================================\n",
      "0.0 1 971 7\n",
      "practically , we demonstrate how we research can be use to automatically identify promising idea and recommend word to user on the fly to help they improve they idea .\n",
      "Practically, we demonstrate how our research can be used to automatically identify promising ideas and recommend words to users on the fly to help them improve their ideas.\n",
      "[(2,), (3,), None]\n",
      "[(6,), (9,), None]\n",
      "['research']-['identify']-['promising', 'ideas']\n",
      "['research']-['recommend']-['words']\n",
      "['research']-['recommend']-['users']\n",
      "['research']-['recommend']-['fly']\n",
      "[(17,), (24,), None]\n",
      "['them']-['improve']-['ideas']\n",
      "========================================\n",
      "0.0 1 971 6\n",
      "we show this effect in eight study involve over 4,000 idea across multiple domain .\n",
      "We show this effect in eight studies involving over 4,000 ideas across multiple domains.\n",
      "['We']-['show']-['effect']\n",
      "['We']-['show']-['studies']\n",
      "========================================\n",
      "0.0 1 971 5\n",
      "base on the `` beauty in averageness '' effect , we hypothesize that idea with semantic subnetwork that have a more prototypical edge weight distribution be judge as more creative .\n",
      "Based on the \"beauty in averageness\" effect, we hypothesize that ideas with semantic subnetworks that have a more prototypical edge weight distribution are judged as more creative.\n",
      "[(10,), (11,), None]\n",
      "['we']-['beauty', 'in']-['averageness', 'effect']\n",
      "[(13, 14, 16), (26,), None]\n",
      "[(13, 14, 16), (29,), None]\n",
      "========================================\n",
      "0.0 1 971 4\n",
      "the edge weight distribution in that subnetwork reflect how the idea balance novelty with familiarity .\n",
      "The edge weight distribution in that subnetwork reflects how the idea balances novelty with familiarity.\n",
      "[(1, 2, 3, 4, 6), (7,), None]\n",
      "['idea']-['balances']-['novelty', 'with', 'familiarity']\n",
      "========================================\n",
      "0.0 1 971 3\n",
      "each idea contain a set of word stem , which form a semantic subnetwork .\n",
      "Each idea contains a set of word stems, which form a semantic subnetwork.\n",
      "[(1,), (2,), None]\n",
      "[(4, 5, 6), (7,), None]\n",
      "['which']-['form']-['semantic', 'subnetwork']\n",
      "========================================\n",
      "0.0 1 971 2\n",
      "we build semantic network where node represent word stem in a particular idea generation topic , and edge weight capture the degree of novelty versus familiarity of word stem combination .\n",
      "We build semantic networks where nodes represent word stems in a particular idea generation topic, and edge weights capture the degree of novelty versus familiarity of word stem combinations (i.e., the weight of an edge that connects two word stems measures their scaled co-occurrence in the relevant language).\n",
      "['We']-['build']-['semantic', 'networks']\n",
      "========================================\n",
      "0.0 1 971 1\n",
      "the literature suggest that creativity result from the optimal balance between novelty and familiarity , which can be measure base on the combination of word in a idea .\n",
      "The literature suggests that creativity results from the optimal balance between novelty and familiarity, which can be measured based on the combinations of words in an idea.\n",
      "[(1,), (2,), None]\n",
      "['creativity']-['results', 'from']-['optimal', 'balance']\n",
      "========================================\n",
      "0.0 1 971 0\n",
      "we explore the use of big datum tool to shed new light on the idea generation process , automatically `` read '' idea to identify promising one , and help people be more creative .\n",
      "We explore the use of big data tools to shed new light on the idea generation process, automatically \"read\" ideas to identify promising ones, and help people be more creative.\n",
      "['We']-['explore']-['use', 'of', 'tools']\n",
      "========================================\n",
      "0.12 2 133 9\n",
      "in the future , we plan to investigate how to provide more tailor care by utilize the association between problem list and practice setting reveal in this study .\n",
      "sent token differ\n",
      "['we']-['plan']-['future']\n",
      "[(4,), (7,), None]\n",
      "['we']-['provide']-['tailored', 'care']\n",
      "['association']-['revealed', 'in']-['study']\n",
      "========================================\n",
      "0.12 2 133 8\n",
      "ConclusionTo we best knowledge , we study be the first attempt to discover the association between the problem list and hospital practice setting .\n",
      "In the future, we plan to investigate how to provide more tailored care by utilizing the association between problem list and practice setting revealed in this study.\n",
      "========================================\n",
      "0.12 2 133 6\n",
      "evaluation be conduct through 5-fold cross validation and recall @k , precision @k and f1 @k be calculated.resultsour method can generate prioritize and meaningful problem list correspond to specific practice setting .\n",
      "Evaluation was conducted through 5-fold cross validation and Recall@k, Precision@k and F1@k were calculated.ResultsOur method can generate prioritized and meaningful problem lists corresponding to specific practice settings.\n",
      "['Evaluation']-['conducted', 'through']-['5-fold', 'cross', 'validation']\n",
      "========================================\n",
      "0.12 2 133 4\n",
      "specifically , after practice setting and problem list be normalize , statistical test , term frequency-inverse document frequency and enrichment analysis be use to choose representative concept for each setting .\n",
      "Specifically, after practice settings and problem lists were normalized, statistical (2) test, term frequency-inverse document frequency (TF-IDF) and enrichment analysis were used to choose representative concepts for each setting.\n",
      "========================================\n",
      "0.12 2 133 3\n",
      "however , it remain a open question on how to leverage problem list in different practice setting to provide tailor care , of which the bottleneck lie in the association between problem list and practice setting.methodsin this study , use sample clinical document associate with a cohort of patient who receive they primary care at Mayo Clinic , we investigate the association between problem list and practice setting through natural language processing and topic modeling technique .\n",
      "However, it remains an open question on how to leverage problem lists in different practice settings to provide tailored care, of which the bottleneck lies in the associations between problem list and practice setting.MethodsIn this study, using sampled clinical documents associated with a cohort of patients who received their primary care at Mayo Clinic, we investigated the associations between problem list and practice setting through natural language processing (NLP) and topic modeling techniques.\n",
      "['it']-['remains']-['open', 'question']\n",
      "========================================\n",
      "0.08 1 133 2\n",
      "the implementation of problem list in ehr have a potential to help practitioner to provide customize care to patient .\n",
      "The implementation of problem lists in EHRs has a potential to help practitioners to provide customized care to patients.\n",
      "['implementation', 'of', 'lists', 'in', 'EHRs']-['has']-['potential']\n",
      "========================================\n",
      "0.08 1 133 1\n",
      "one of the core criterion for meaningful use of ehr be to have a problem list that show the most important health problem face by a patient .\n",
      "One of the core criteria for Meaningful Use of EHRs is to have a problem list that shows the most important health problems faced by a patient.\n",
      "[(3, 4, 5, 7), (10,), None]\n",
      "['core', 'criteria', 'for', 'Use']-['have']-['problem', 'list']\n",
      "========================================\n",
      "0.04 1 133 7\n",
      "for practice setting prediction , recall increase from 0.719 to 0.931 , precision increase from 0.882 to 0.931 and f1 increase from 0.790 to 0.931 .\n",
      "For practice setting prediction, recall increases from 0.719 (k=2) to 0.931 (k=10), precision increases from 0.882 (k=2) to 0.931 (k=10) and F1 increases from 0.790 (k=2) to 0.931 (k=10).ConclusionTo our best knowledge, our study is the first attempting to discover the association between the problem lists and hospital practice settings.\n",
      "========================================\n",
      "0.04 1 133 5\n",
      "then latent Dirichlet Allocations be use to train topic model and predict potential practice setting use similarity metric base on the problem concept representative of practice setting .\n",
      "Then Latent Dirichlet Allocations (LDA) were used to train topic models and predict potential practice settings using similarity metrics based on the problem concepts representative of practice settings.\n",
      "[(1, 2, 3), (5,), None]\n",
      "['Latent', 'Dirichlet', 'Allocations']-['train']-['topic', 'models']\n",
      "['Latent', 'Dirichlet', 'Allocations']-['predict']-['potential', 'practice', 'settings']\n",
      "========================================\n",
      "0.08 1 1274 1\n",
      "list datum be collect and interpret use Latent-Dirichlet Allocation , to determine common topic in the listing .\n",
      "Listing data was collected and interpreted using Latent-Dirichlet Allocation (LDA), to determine common topics in the listings.\n",
      "[(0, 1), (3,), None]\n",
      "['Listing', 'data']-['using']-['Latent-Dirichlet', 'Allocation']\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========================================\n",
      "0.02 1 9 6\n",
      "we result highlight potential area with high density of affected individual and infrastructure damage throughout the temporal progression of the disaster .\n",
      "Our results highlight potential areas with high density of affected individuals and infrastructure damage throughout the temporal progression of the disaster.\n",
      "['results']-['highlight']-['potential', 'areas', 'with', 'density']\n",
      "========================================\n",
      "0.09 1 357 3\n",
      "by utilize latent dirichlet allocation as a generative probabilistic topic model , we analyse a unique dataset consist of 46,582 full-text article publish in the last 26 year in 21 specialize scientific fishery journal .\n",
      "By utilizing latent Dirichlet allocation as a generative probabilistic topic model, we analyse a unique dataset consisting of 46,582 full-text articles published in the last 26 years in 21 specialized scientific fisheries journals.\n",
      "['we']-['analyse']-['unique', 'dataset']\n",
      "['we']-['utilizing']-['latent', 'Dirichlet', 'allocation']\n",
      "['we']-['utilizing']-['generative', 'probabilistic', 'topic', 'model']\n",
      "========================================\n",
      "0.0 1 1447 3\n",
      "draw on one aspect of Stephen Ramsay 's idea of algorithmic criticism , the essay emphasize the continuity between `` big datum '' method and technique and longer-standing method of literary study .\n",
      "Drawing on one aspect of Stephen Ramsay's idea of algorithmic criticism, the essay emphasizes the continuities between \"big data\" methods and techniques and longer-standing methods of literary study.\n",
      "['essay']-['emphasizes']-['continuities']\n",
      "['essay']-['Drawing', 'on']-['aspect', 'of', 'idea']\n",
      "========================================\n",
      "1.09 2 234 14\n",
      "conclusion : data analysis and the visualization of news article can generate useful information .\n",
      "Conclusions: Data analysis and the visualization of news articles can generate useful information.\n",
      "========================================\n",
      "1.04 2 234 12\n",
      "give we analysis of the percentage of news related to ths in China , Topic 1 be the most popular among the topic and be mention in 31.9 % of all news story .\n",
      "Given our analysis of the percentage of news related to THS in China, Topic 1 (Cancer) was the most popular among the topics and was mentioned in 31.9% of all news stories.\n",
      "['Topic']-['was']-['popular', 'and', 'mentioned']\n",
      "========================================\n",
      "1.0 1 234 18\n",
      "we recommend that additional study be conduct related to sentiment analysis of news datum to verify and measure the influence of ths-related topic .\n",
      "We recommend that additional studies be conducted related to sentiment analysis of news data to verify and measure the influence of THS-related topics.\n",
      "[(0,), (1,), None]\n",
      "[(3, 4), (6,), None]\n",
      "['additional', 'studies']-['related', 'to']-['sentiment', 'analysis', 'of', 'data']\n",
      "========================================\n",
      "1.0 1 234 4\n",
      "method : the datum be retrieve from the Wiser and Factiva news database .\n",
      "Methods: The data were retrieved from the Wiser and Factiva news databases.\n",
      "========================================\n",
      "1.0 1 234 15\n",
      "we study show that topic modeling can offer insight into understand news report relate to THS .\n",
      "Our study shows that topic modeling can offer insights into understanding news reports related to THS.\n",
      "[(1,), (2,), None]\n",
      "['topic', 'modeling']-['offer']-['insights']\n",
      "['topic', 'modeling']-['understanding']-['news', 'reports']\n",
      "========================================\n",
      "1.0 1 234 13\n",
      "topic 2 be relate to roughly 15 % of news item on ths .\n",
      "Topic 2 (Control of quitting smoking) was related to roughly 15% of news items on THS.\n",
      "['Topic']-['related', 'to']-['%', 'of', 'items']\n",
      "========================================\n",
      "1.0 1 234 11\n",
      "we find 279 news report about ths from the chinese media over the same period and 363 news report from the United States .\n",
      "We found 279 news reports about THS from the Chinese media over the same period and 363 news reports from the United States.\n",
      "['We']-['found']-['news', 'reports', 'about', 'THS', 'over', 'period', 'and', 'reports']\n",
      "['We']-['found']-['news', 'reports', 'from', 'States']\n",
      "========================================\n",
      "0.09 1 234 5\n",
      "a preliminary investigation focus on article date between January 1 , 2013 , and December 31 , 2017 .\n",
      "A preliminary investigation focused on articles dated between January 1, 2013, and December 31, 2017.\n",
      "['preliminary', 'investigation']-['focused', 'on']-['articles']\n",
      "========================================\n",
      "0.09 1 234 9\n",
      "result : we find 745 article date between January 1 , 2013 , and December 31 , 2017 .\n",
      "Results: We found 745 articles dated between January 1, 2013, and December 31, 2017.\n",
      "========================================\n",
      "0.19 1 866 3\n",
      "the objective of this paper be to propose a method to detect fraud suspect on IDD call service which combine the advantage of hybrid nbtree and Kullback Leibler divergence .\n",
      "The objective of this paper is to propose a method to detect fraud suspects on IDD call services which combines the advantages of hybrid NBTree and Kullback Leibler divergence (KL-divergence or KLD).\n",
      "[(1, 2, 4), (5,), None]\n",
      "['objective', 'of', 'paper']-['propose']-['method']\n",
      "========================================\n",
      "0.3 2 406 5\n",
      "to these , we apply latent dirichlet allocation to summarize the player 's motion style as a probabilistic mixture of different style discover from datum .\n",
      "To these, we apply latent Dirichlet allocation to summarize the player's motion style as a probabilistic mixture of different styles discovered from data.\n",
      "['we']-['apply']-['latent', 'Dirichlet', 'allocation']\n",
      "['we']-['summarize']-['motion', 'style']\n",
      "['we']-['summarize']-['probabilistic', 'mixture', 'of', 'styles']\n",
      "========================================\n",
      "0.04 1 406 3\n",
      "this be do by deal both with the intrinsic uncertainty associate with the setting and with the agent necessity to act in real time to support the game interaction .\n",
      "This is done by dealing both with the intrinsic uncertainty associated with the setting and with the agent necessity to act in real time to support the game interaction.\n",
      "[(0,), (2,), None]\n",
      "['This']-['dealing']-['intrinsic', 'uncertainty']\n",
      "========================================\n",
      "0.01 1 406 7\n",
      "the obtain result suggest that the proposed system be able to provide a robust description for the player interaction .\n",
      "The obtained results suggest that the proposed system is able to provide a robust description for the player interaction.\n",
      "[(1, 2), (3,), None]\n",
      "['proposed', 'system']-['is']-['able']\n",
      "['proposed', 'system']-['provide']-['robust', 'description', 'for', 'interaction']\n",
      "========================================\n",
      "0.01 1 406 2\n",
      "follow such direction and give the lack of quantitative method for player modeling in PIRG , we propose a methodology for represent player as a mixture of exist player 's type uncover from datum .\n",
      "Following such directions and given the lack of quantitative methods for player modeling in PIRG, we propose a methodology for representing players as a mixture of existing player's types uncovered from data.\n",
      "['we']-['propose']-['methodology']\n",
      "['we']-['propose']-['methodology']\n",
      "========================================\n",
      "0.01 1 406 1\n",
      "in this framework , learn a model of player ' activity be relevant both to understand they engagement , as well as to understand specific strategy they adopt , which in turn can foster game adaptation .\n",
      "In this framework, learning a model of players' activity is relevant both to understand their engagement, as well as to understand specific strategies they adopted, which in turn can foster game adaptation.\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "t_idx = 1\n",
    "for top_doc in top_docs[t_idx]:\n",
    "    _idx = top_doc[0]\n",
    "    if type(parse[_idx]) is str:\n",
    "        print(_idx, \"parse\", \"err\")\n",
    "        continue\n",
    "    sents = relation.convert_parse2lemma_sents(parse[_idx])\n",
    "    sort_idxs, importance, counts = relation.extract_important_sents(sents, [terms[x[0]] for x in top_terms[t_idx]], [x[1] for x in top_terms[t_idx]])\n",
    "    for i in sort_idxs:\n",
    "        if importance[i]>0:\n",
    "            print(round(importance[i], 2), counts[i], _idx, i)\n",
    "            sent_tokens = parse[_idx][\"sentences\"][i][\"tokens\"]\n",
    "            sent_deps = parse[_idx][\"sentences\"][i][\"enhancedPlusPlusDependencies\"]\n",
    "            print(\" \".join([s[\"lemma\"] for s in sent_tokens]))\n",
    "            sents = sent_tokenize(_raw[_idx])\n",
    "            if len(sents) > i:\n",
    "                print(sents[i])\n",
    "            else:\n",
    "                print(\"sent token differ\")\n",
    "            triples = relation.extract_triples_from_sent(sent_deps, sent_tokens, True)\n",
    "            for triple in triples:\n",
    "                if None in triple:\n",
    "                    print(triple)\n",
    "                    continue\n",
    "                s = [sent_tokens[i][\"originalText\"] for i in triple[0]]\n",
    "                p = [sent_tokens[i][\"originalText\"] for i in triple[1]]\n",
    "                o = [sent_tokens[i][\"originalText\"] for i in triple[2]]\n",
    "                print(\"{}-{}-{}\".format(s,p,o))\n",
    "            print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[142, 101, 47, 171, 198, 120, 153, 104, 61, 64, 110, 128, 226, 168]\n",
      "113 topic modeling-[understand]-news report\n",
      "101 topic modeling in particular-[discover]-software artifact\n",
      "100 topic modeling in particular-[discover]-meaningful information\n",
      "99 latent Dirichlet Allocations-[train]-topic model\n",
      "93 we-[utilize]-generative probabilistic topic model\n",
      "87 stop word list-[result in]-higher performance of model\n",
      "81 topic modeling-[offer]-insight\n",
      "77 different outlet-[emphasize]-different topic\n",
      "70 we-[propose]-automatic stop word generation approach for model\n",
      "65 topic-[relate to]-% of item\n",
      "64 we-[measure]-topic coherence\n",
      "64 core criterion for use-[have]-problem list\n",
      "61 list datum-[use]-Latent-Dirichlet Allocation\n",
      "59 model-[yield]-performance to model\n",
      "55 second task-[identify]-word in headline\n",
      "53 we-[find]-news report about ths over period and report\n",
      "51 model-[outperform]-good headline task\n",
      "49 latent Dirichlet Allocations-[predict]-potential practice setting\n",
      "48 model-[outperform]-other comparative method\n",
      "48 they-[use]-information\n",
      "45 additional study-[related to]-sentiment analysis of datum\n",
      "45 we-[apply]-latent dirichlet allocation\n",
      "45 game developer-[describe]-type of gameplay in game\n",
      "43 software artifact characteristic-[worsen]-performance of modeling\n",
      "43 we-[utilize]-latent dirichlet allocation\n",
      "42 major event-[shift]-general news focus\n",
      "41 we-[explore]-use of tool\n",
      "41 many approach-[generate]-effective stop word\n",
      "40 list-[apply to]-mining software artifact\n",
      "40 we-[begin with]-latent dirichlet allocation\n",
      "33 objective of paper-[propose]-method\n",
      "32 many-[delegate]-specialize news provider\n",
      "32 research-[identify]-promising idea\n",
      "31 we-[find]-news report from States\n",
      "31 research-[recommend]-word\n",
      "30 we-[embed]-delegate news selection\n",
      "29 we-[conduct]-modeling of article\n",
      "29 we-[conduct]-large-scale analysis and modeling\n",
      "28 journalist-[resource of]-large-scale click-through datum of article\n",
      "27 mining software artifact-[be]-useful way\n",
      "27 we-[add]-word with score\n",
      "24 result-[highlight]-potential area with density\n",
      "24 we-[evaluate]-different experimental setting\n",
      "23 headline-[be]-important for outlet\n",
      "23 association-[reveal in]-study\n",
      "22 we-[apply]-quantitative analysis and technique\n",
      "22 level-[feature]-appropriate style of gameplay\n",
      "22 first task-[retrieve]-full headline\n",
      "21 many-[delegate]-information choice\n",
      "21 software artifact-[be]-unstructured and contain\n",
      "20 good uri-[understand]-api\n",
      "20 implementation of list in ehr-[have]-potential\n",
      "20 we-[train]-player recognition system on output\n",
      "19 discovery about gameplay-[be]-in fact independent of style\n",
      "19 we-[apply]-text mining technique\n",
      "19 level-[recommend]-level with gameplay\n",
      "19 idea-[balance]-novelty with familiarity\n",
      "19 understandable reusable Uniform Resource Identifers-[attract]-client developer of api\n",
      "18 we-[acquire]-in-depth understanding of headline\n",
      "18 we-[allow]-game developer\n",
      "17 preliminary investigation-[focus on]-article\n",
      "17 creativity-[result from]-optimal balance\n",
      "17 essay-[draw on]-aspect of idea\n",
      "16 proposed system-[provide]-robust description for interaction\n",
      "16 system-[recognize]-player\n",
      "16 research-[recommend]-user\n",
      "15 preprocess task-[be]-efficient way\n",
      "15 we-[provide]-tailor care\n",
      "14 good uri-[support]-client developer\n",
      "14 we-[resolve]-problem\n",
      "14 understandability and reusability-[be]-important characteristic of quality\n",
      "14 which-[form]-semantic subnetwork\n",
      "14 we-[show]-study\n",
      "13 they-[improve]-idea\n",
      "13 we-[summarize]-probabilistic mixture of style\n",
      "12 Identifier lexicon-[have]-direct impact on understandability\n",
      "12 research-[recommend]-fly\n",
      "11 we-[build]-semantic network\n",
      "11 we-[propose]-theoretical framework\n",
      "9 we-[propose]-methodology\n",
      "9 we-[propose]-methodology\n",
      "8 we-[summarize]-motion style\n",
      "7 we-[beauty in]-averageness effect\n",
      "7 we-[analyse]-unique dataset\n",
      "7 we-[propose]-Relational Rules\n",
      "7 Identifier lexicon-[have]-quality of product\n",
      "6 understandable reusable Uniform Resource Identifers-[be]-important\n",
      "6 we-[prove]-experiment\n",
      "6 evaluation-[conduct through]-5-fold cross validation\n",
      "6 %-[accuracy in]-seconds of playtime\n",
      "5 journalist-[follow]-rules-of-thumb and experience\n",
      "5 essay-[emphasize]-continuity\n",
      "5 rules-of-thumb and experience-[master]-art\n",
      "4 it-[remain]-open question\n",
      "4 we-[show]-effect\n",
      "4 we-[show]-paper\n",
      "4 we-[find]-evidence for dependence\n",
      "4 we-[plan]-future\n",
      "3 this-[deal]-intrinsic uncertainty\n",
      "3 proposed system-[be]-able\n",
      "2 Topic-[be]-popular and mention\n"
     ]
    }
   ],
   "source": [
    "extended = relation.extend_lda_results(parse, _input, top_terms, top_docs, terms, \"lemma\", top_n=-1, score_method=\"tf\")\n",
    "print([len(t) for t in extended])\n",
    "for triple in extended[t_idx]:\n",
    "    print(round(triple[1],4),\"{}-[{}]-{}\".format(\" \".join(triple[0][0]),\" \".join(triple[0][1]),\" \".join(triple[0][2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4261430405292502, 0.468703245855919, 0.6300999908306748, 0.38015286768970613, 0.47686543550368227, 0.33041919447047324, 0.36195040713496035, 0.5260061361395432, 0.6344010739418413, 0.5610000625155974, 0.560750760014041, 0.5049310494344786, 0.36906548999486494, 0.306203278729893]\n",
      "[0.2535263358435659, 0.23527037872437412, 0.237734854167385, 0.31172286004436633, 0.46165065592589577, 0.2581540708686481, 0.363709806927763, 0.3563169467840928, 0.3018421979196038, 0.23419465944476442, 0.2753598893493359, 0.37568608698688, 0.37868540393783356, 0.3750643612616595]\n",
      "0.46690657377035183 0.315637036299012\n"
     ]
    }
   ],
   "source": [
    "ws = 110\n",
    "measure = \"c_v\"\n",
    "old = lda.get_coherence(tf, terms, topic_word, _input, measure, top_n=20, window_size=ws)\n",
    "extended = [e[:20] for e in extended]\n",
    "new = evaluate.evaluate_triples_by_coherence(extended,vec,terms,topic_word,_input,top_docs,measure, window_size=ws)\n",
    "print(old, new, sep=\"\\n\")\n",
    "print(np.mean(old), np.mean(new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func get_phrases_by_pattern exec time: 0.8318648338317871\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('latent Dirichlet allocation', 705),\n",
       " ('topic model', 646),\n",
       " ('experimental result', 431),\n",
       " ('topic modeling', 320),\n",
       " ('latent topic', 202),\n",
       " ('latent Dirichlet Allocation', 199),\n",
       " ('LDA model', 178),\n",
       " ('latent Dirichlet allocation model', 155),\n",
       " ('short text', 124),\n",
       " ('sentiment analysis', 103),\n",
       " ('social media', 99),\n",
       " ('topic distribution', 92),\n",
       " ('social network', 89),\n",
       " ('recent year', 86),\n",
       " ('Dirichlet allocation', 86),\n",
       " ('large number', 82),\n",
       " ('text mining', 80),\n",
       " ('latent Dirichlet Allocation model', 78),\n",
       " ('Gibbs sampling', 74),\n",
       " ('case study', 73),\n",
       " ('dirichlet allocation', 68),\n",
       " ('probabilistic topic model', 68),\n",
       " ('information retrieval', 65),\n",
       " ('previous work', 65),\n",
       " ('large amount', 61),\n",
       " ('natural language processing', 60),\n",
       " ('visual word', 59),\n",
       " ('latent Dirichlet allocation topic model', 57),\n",
       " ('novel approach', 57),\n",
       " ('extensive experiment', 54),\n",
       " ('better performance', 54),\n",
       " ('big datum', 53),\n",
       " ('machine learning', 52),\n",
       " ('new method', 52),\n",
       " ('online review', 51),\n",
       " ('important role', 51),\n",
       " ('different type', 51),\n",
       " ('textual datum', 50),\n",
       " ('text document', 49),\n",
       " ('semantic analysis', 49),\n",
       " ('different topic', 48),\n",
       " ('generative model', 48),\n",
       " ('datum set', 46),\n",
       " ('new approach', 45),\n",
       " ('text datum', 45),\n",
       " ('language model', 45),\n",
       " ('probability distribution', 45),\n",
       " ('training datum', 44),\n",
       " ('source code', 44),\n",
       " ('topic detection', 44),\n",
       " ('latent variable', 42),\n",
       " ('word embedding', 41),\n",
       " ('large collection', 41),\n",
       " ('hidden topic', 40),\n",
       " ('text classification', 40),\n",
       " ('large volume', 40),\n",
       " ('allocation model', 40),\n",
       " ('state-of-the-art method', 39),\n",
       " ('topic modelling', 38),\n",
       " ('natural language', 38),\n",
       " ('user interest', 38),\n",
       " ('other hand', 37),\n",
       " ('significant improvement', 37),\n",
       " ('novel method', 37),\n",
       " ('challenging task', 36),\n",
       " ('probabilistic model', 35),\n",
       " ('model parameter', 34),\n",
       " ('research topic', 33),\n",
       " ('text corpora', 33),\n",
       " ('proposed system', 33),\n",
       " ('document collection', 33),\n",
       " ('real-world dataset', 32),\n",
       " ('latent Dirichlet allocation algorithm', 32),\n",
       " ('text categorization', 32),\n",
       " ('previous study', 32),\n",
       " ('prior knowledge', 32),\n",
       " ('same time', 31),\n",
       " ('topic space', 31),\n",
       " ('news article', 31),\n",
       " ('dirichlet allocation model', 30),\n",
       " ('user preference', 30),\n",
       " ('text analysis', 30),\n",
       " ('unsupervised manner', 30),\n",
       " ('semantic information', 29),\n",
       " ('sentiment classification', 29),\n",
       " ('Twitter datum', 29),\n",
       " ('recommender system', 29),\n",
       " ('topic analysis', 29),\n",
       " ('classification accuracy', 28),\n",
       " ('support vector machine', 28),\n",
       " ('bug report', 28),\n",
       " ('latent Dirichlet allocation method', 27),\n",
       " ('empirical study', 27),\n",
       " ('neural network', 27),\n",
       " ('hot topic', 27),\n",
       " ('topic modeling technique', 27),\n",
       " ('recommendation system', 27),\n",
       " ('other method', 26),\n",
       " ('Dirichlet Allocation model', 26),\n",
       " ('many application', 26),\n",
       " ('rapid development', 26),\n",
       " ('traditional method', 26),\n",
       " ('baseline method', 26),\n",
       " ('small number', 26),\n",
       " ('experiment result', 26),\n",
       " ('real dataset', 26),\n",
       " ('huge amount', 26),\n",
       " ('web service', 26),\n",
       " ('topic discovery', 26),\n",
       " ('parameter estimation', 26),\n",
       " ('topic modeling approach', 25),\n",
       " ('contextual information', 25),\n",
       " ('k-means clustering', 25),\n",
       " ('topic extraction', 25),\n",
       " ('lda model', 25),\n",
       " ('real datum', 25),\n",
       " ('support Vector Machine', 25),\n",
       " ('user behavior', 25),\n",
       " ('classification task', 24),\n",
       " ('unsupervised learning', 24),\n",
       " ('word distribution', 24),\n",
       " ('Semantic analysis', 24),\n",
       " ('useful information', 24),\n",
       " ('clustering algorithm', 24),\n",
       " ('document classification', 24),\n",
       " ('topic evolution', 24),\n",
       " ('Dirichlet Allocation', 24),\n",
       " ('LDA topic model', 23),\n",
       " ('topic feature', 23),\n",
       " ('variational inference', 23),\n",
       " ('specific topic', 22),\n",
       " ('unsupervised method', 22),\n",
       " ('unstructured datum', 22),\n",
       " ('relevant topic', 22),\n",
       " ('topic coherence', 22),\n",
       " ('much attention', 22),\n",
       " ('text mining technique', 22),\n",
       " ('Dirichlet allocation model', 22),\n",
       " ('high accuracy', 22),\n",
       " ('LDA algorithm', 22),\n",
       " ('classification performance', 22),\n",
       " ('scene classification', 22),\n",
       " ('topic structure', 21),\n",
       " ('wide range', 21),\n",
       " ('public opinion', 21),\n",
       " ('statistical model', 21),\n",
       " ('clustering method', 21),\n",
       " ('datum mining', 21),\n",
       " ('topic modeling method', 21),\n",
       " ('data set', 21),\n",
       " ('conventional method', 21),\n",
       " ('dimensionality reduction', 21),\n",
       " ('mixture model', 21),\n",
       " ('software system', 21),\n",
       " ('empirical result', 20),\n",
       " ('main topic', 20),\n",
       " ('better understanding', 20),\n",
       " ('latent topic model', 20),\n",
       " ('social media data', 20),\n",
       " ('topic modeling algorithm', 20),\n",
       " ('rapid growth', 20),\n",
       " ('topic information', 20),\n",
       " ('class label', 20),\n",
       " ('topic proportion', 20),\n",
       " ('similarity measure', 19),\n",
       " ('feature extraction', 19),\n",
       " ('statistical method', 19),\n",
       " ('author-topic model', 19),\n",
       " ('posterior distribution', 19),\n",
       " ('document representation', 19),\n",
       " ('different level', 19),\n",
       " ('good performance', 19),\n",
       " ('opinion mining', 19),\n",
       " ('search engine', 19),\n",
       " ('text corpus', 19),\n",
       " ('web page', 19),\n",
       " ('urban area', 19),\n",
       " ('video sequence', 19),\n",
       " ('first step', 18),\n",
       " ('textual information', 18),\n",
       " ('large set', 18),\n",
       " ('scientific literature', 18),\n",
       " ('different aspect', 18),\n",
       " ('Probabilistic topic model', 18),\n",
       " ('% improvement', 18),\n",
       " ('main contribution', 18),\n",
       " ('important task', 18),\n",
       " ('systematic review', 18),\n",
       " ('promising result', 18),\n",
       " ('feature space', 18),\n",
       " ('extract topic', 18),\n",
       " ('tv program', 18),\n",
       " ('lda method', 18),\n",
       " ('previous research', 17),\n",
       " ('machine learning method', 17),\n",
       " ('speech recognition', 17),\n",
       " ('valuable information', 17),\n",
       " ('semantic relationship', 17),\n",
       " ('novel framework', 17),\n",
       " ('content analysis', 17),\n",
       " ('relevant document', 17),\n",
       " ('spatial information', 17),\n",
       " ('new model', 17),\n",
       " ('topic vector', 17),\n",
       " ('Social media', 17),\n",
       " ('dirichlet distribution', 17),\n",
       " ('image classification', 17),\n",
       " ('semantic relation', 17),\n",
       " ('topic word', 17),\n",
       " ('document clustering', 17),\n",
       " ('better result', 17),\n",
       " ('topic trend', 17),\n",
       " ('multinomial distribution', 17),\n",
       " ('computer vision', 17),\n",
       " ('effective method', 17),\n",
       " ('experimental evaluation', 17),\n",
       " ('meaningful topic', 17),\n",
       " ('Topic Model', 16),\n",
       " ('naive baye', 16),\n",
       " ('discrete datum', 16),\n",
       " ('textual document', 16),\n",
       " ('classification method', 16),\n",
       " ('state-of-the-art approach', 16),\n",
       " ('tag recommendation', 16),\n",
       " ('graphical model', 16),\n",
       " ('social network analysis', 16),\n",
       " ('daily life', 16),\n",
       " ('information need', 16),\n",
       " ('semantic feature', 16),\n",
       " ('bayesian model', 16),\n",
       " ('multiple topic', 16),\n",
       " ('matrix factorization', 16),\n",
       " ('new topic model', 16),\n",
       " ('small set', 16),\n",
       " ('higher accuracy', 16),\n",
       " ('feature selection', 16),\n",
       " ('probabilistic topic modeling', 15),\n",
       " ('generative process', 15),\n",
       " ('novel model', 15),\n",
       " ('Stack Overflow', 15),\n",
       " ('visual feature', 15),\n",
       " ('new topic', 15),\n",
       " ('textual content', 15),\n",
       " ('large dataset', 15),\n",
       " ('predictive performance', 15),\n",
       " ('semantic meaning', 15),\n",
       " ('different domain', 15),\n",
       " ('word frequency', 15),\n",
       " ('past decade', 15),\n",
       " ('data analysis', 15),\n",
       " ('benchmark dataset', 15),\n",
       " ('traditional approach', 15),\n",
       " ('policy maker', 15),\n",
       " ('inference algorithm', 15),\n",
       " ('community structure', 15),\n",
       " ('relevant information', 15),\n",
       " ('training set', 15),\n",
       " ('text mining method', 14),\n",
       " ('deep learning', 14),\n",
       " ('traditional topic model', 14),\n",
       " ('classification algorithm', 14),\n",
       " ('vector space model', 14),\n",
       " ('other approach', 14),\n",
       " ('powerful tool', 14),\n",
       " ('user-generated content', 14),\n",
       " ('large scale', 14),\n",
       " ('behavior pattern', 14),\n",
       " ('real time', 14),\n",
       " ('topic-word distribution', 14),\n",
       " ('real-world datum', 14),\n",
       " ('hierarchical structure', 14),\n",
       " ('latent Dirichlet allocation topic modeling', 14),\n",
       " ('large corpora', 14),\n",
       " ('large corpus', 14),\n",
       " ('search query', 14),\n",
       " ('previous method', 14),\n",
       " ('social interaction', 14),\n",
       " ('important information', 14),\n",
       " ('latent Semantic analysis', 14),\n",
       " ('best result', 14),\n",
       " ('research papers', 13),\n",
       " ('lda analysis', 13),\n",
       " ('artificial intelligence', 13),\n",
       " ('United States', 13),\n",
       " ('coherent topic', 13),\n",
       " ('topic probability', 13),\n",
       " ('word vector', 13),\n",
       " ('research trend', 13),\n",
       " ('future research', 13),\n",
       " ('research field', 13),\n",
       " ('more attention', 13),\n",
       " ('efficient method', 13),\n",
       " ('important issue', 13),\n",
       " ('human activity', 13),\n",
       " ('topic correlation', 13),\n",
       " ('word co-occurrence', 13),\n",
       " ('competitive performance', 13),\n",
       " ('human behavior', 13),\n",
       " ('novel topic model', 13),\n",
       " ('lda topic', 13),\n",
       " ('best performance', 13),\n",
       " ('vast amount', 13),\n",
       " ('blog post', 13),\n",
       " ('text collection', 13),\n",
       " ('topic representation', 13),\n",
       " ('user query', 13),\n",
       " ('review text', 12),\n",
       " ('term frequency', 12),\n",
       " ('research community', 12),\n",
       " ('text information', 12),\n",
       " ('major challenge', 12),\n",
       " ('such datum', 12),\n",
       " ('information technology', 12),\n",
       " ('customer satisfaction', 12),\n",
       " ('word representation', 12),\n",
       " ('product review', 12),\n",
       " ('learning method', 12),\n",
       " ('inference method', 12),\n",
       " ('mobile device', 12),\n",
       " ('latent Dirichlet allocation approach', 12),\n",
       " ('similar topic', 12),\n",
       " ('context information', 12),\n",
       " ('social media post', 12),\n",
       " ('appropriate number', 12),\n",
       " ('online community', 12),\n",
       " ('long text', 12),\n",
       " ('social event', 12),\n",
       " ('latent semantics', 12),\n",
       " ('specific domain', 12),\n",
       " ('document level', 12),\n",
       " ('posterior inference', 12),\n",
       " ('topic number', 12),\n",
       " ('new feature', 12),\n",
       " ('semantic gap', 12),\n",
       " ('topic cluster', 12),\n",
       " ('feature word', 12),\n",
       " ('important problem', 12),\n",
       " ('general framework', 12),\n",
       " ('abnormal event', 12),\n",
       " ('hierarchical model', 12),\n",
       " ('superior performance', 11),\n",
       " ('various aspect', 11),\n",
       " ('massive amount', 11),\n",
       " ('various topic', 11),\n",
       " ('unsupervised approach', 11),\n",
       " ('effective way', 11),\n",
       " ('temporal trend', 11),\n",
       " ('good result', 11),\n",
       " ('great challenge', 11),\n",
       " ('climate change', 11),\n",
       " ('heart disease', 11),\n",
       " ('synthetic datum', 11),\n",
       " ('different language', 11),\n",
       " ('future work', 11),\n",
       " ('real world', 11),\n",
       " ('real world dataset', 11),\n",
       " ('allocation topic model', 11),\n",
       " ('statistical analysis', 11),\n",
       " ('other topic model', 11),\n",
       " ('dialogue system', 11),\n",
       " ('topic similarity', 11),\n",
       " ('computer science', 11),\n",
       " ('topic hierarchy', 11),\n",
       " ('semantic similarity', 11),\n",
       " ('vector space', 11),\n",
       " ('different method', 11),\n",
       " ('Gibbs sampling algorithm', 11),\n",
       " ('latent Dirichlet Allocation Model', 11),\n",
       " ('evaluation result', 11),\n",
       " ('similar user', 11),\n",
       " ('latent structure', 11),\n",
       " ('bag-of-word assumption', 11),\n",
       " ('several topic', 11),\n",
       " ('time series', 11),\n",
       " ('user profile', 11),\n",
       " ('standard lda', 11),\n",
       " ('unlabeled datum', 11),\n",
       " ('topic identification', 11),\n",
       " ('various type', 11),\n",
       " ('topic level', 11),\n",
       " ('image datum', 11),\n",
       " ('such model', 11),\n",
       " ('motion pattern', 11),\n",
       " ('image clustering', 11),\n",
       " ('bayesian inference', 11),\n",
       " ('image annotation', 11),\n",
       " ('other user', 11),\n",
       " ('color harmony model', 11),\n",
       " ('human action', 11),\n",
       " ('social datum', 10),\n",
       " ('machine learning technique', 10),\n",
       " ('topic quality', 10),\n",
       " ('consumer review', 10),\n",
       " ('popular topic', 10),\n",
       " ('user review', 10),\n",
       " ('such information', 10),\n",
       " ('high dimensionality', 10),\n",
       " ('main idea', 10),\n",
       " ('efficient way', 10),\n",
       " ('Practical implication', 10),\n",
       " ('most case', 10),\n",
       " ('present study', 10),\n",
       " ('public dataset', 10),\n",
       " ('topic label', 10),\n",
       " ('user experience', 10),\n",
       " ('electronic health record', 10),\n",
       " ('fault diagnosis', 10),\n",
       " ('individual user', 10),\n",
       " ('effective approach', 10),\n",
       " ('related topic', 10),\n",
       " ('real-world application', 10),\n",
       " ('test datum', 10),\n",
       " ('online datum', 10),\n",
       " ('word order', 10),\n",
       " ('fundamental problem', 10),\n",
       " ('random mixture', 10),\n",
       " ('document corpus', 10),\n",
       " ('large document collection', 10),\n",
       " ('baseline model', 10),\n",
       " ('few study', 10),\n",
       " ('datum source', 10),\n",
       " ('search result', 10),\n",
       " ('domain knowledge', 10),\n",
       " ('k-means algorithm', 10),\n",
       " ('hierarchical clustering', 10),\n",
       " ('% accuracy', 10),\n",
       " ('empirical evaluation', 10),\n",
       " ('image segmentation', 10),\n",
       " ('content information', 10),\n",
       " ('service recommendation', 10),\n",
       " ('discriminative power', 10),\n",
       " ('temporal information', 10),\n",
       " ('multimodal datum', 10),\n",
       " ('retrieval performance', 10),\n",
       " ('hierarchical Dirichlet', 10),\n",
       " ('learning process', 10),\n",
       " ('certain topic', 10),\n",
       " ('evaluation experiment', 10),\n",
       " ('semantic space', 10),\n",
       " ('word image', 10),\n",
       " ('original LDA', 10),\n",
       " ('challenging issue', 10),\n",
       " ('customer review', 10),\n",
       " ('image collection', 10),\n",
       " ('low-level feature', 10),\n",
       " ('satellite image', 10),\n",
       " ('query photo', 10),\n",
       " ('topic assignment', 10),\n",
       " ('text stream', 10),\n",
       " ('conversational content', 10),\n",
       " ('large text corpora', 10),\n",
       " ('trained model', 10),\n",
       " ('object recognition', 10),\n",
       " ('mobility pattern', 10),\n",
       " ('hybrid approach', 10),\n",
       " ('feature location', 10),\n",
       " ('product feature', 10),\n",
       " ('Topic model', 10),\n",
       " ('different word', 9),\n",
       " ('other model', 9),\n",
       " ('conventional topic model', 9),\n",
       " ('sparsity problem', 9),\n",
       " ('negative sentiment', 9),\n",
       " ('text feature', 9),\n",
       " ('research work', 9),\n",
       " ('such topic', 9),\n",
       " ('prediction accuracy', 9),\n",
       " ('image retrieval', 9),\n",
       " ('different way', 9),\n",
       " ('meaningful information', 9),\n",
       " ('recommendation list', 9),\n",
       " ('topic-based representation', 9),\n",
       " ('promising performance', 9),\n",
       " ('cosine similarity', 9),\n",
       " ('structured datum', 9),\n",
       " ('knowledge discovery', 9),\n",
       " ('variable model', 9),\n",
       " ('current study', 9),\n",
       " ('experimental study', 9),\n",
       " ('many topic', 9),\n",
       " ('naive Bayes', 9),\n",
       " ('other word', 9),\n",
       " ('various domain', 9),\n",
       " ('different kind', 9),\n",
       " ('key factor', 9),\n",
       " ('qualitative analysis', 9),\n",
       " ('predictive model', 9),\n",
       " ('social media platform', 9),\n",
       " ('generative topic model', 9),\n",
       " ('semantic relatedness', 9),\n",
       " ('non-negative Matrix factorization', 9),\n",
       " ('online algorithm', 9),\n",
       " ('corresponding topic', 9),\n",
       " ('prediction performance', 9),\n",
       " ('classification problem', 9),\n",
       " ('clinical report', 9),\n",
       " ('standard topic model', 9),\n",
       " ('long time', 9),\n",
       " ('promising approach', 9),\n",
       " ('comparative analysis', 9),\n",
       " ('weighting scheme', 9),\n",
       " ('clinical pathway', 9),\n",
       " ('semantic concept', 9),\n",
       " ('other type', 9),\n",
       " ('state-of-the-art performance', 9),\n",
       " ('recommendation approach', 9),\n",
       " ('patent datum', 9),\n",
       " ('different number', 9),\n",
       " ('vector representation', 9),\n",
       " ('different dataset', 9),\n",
       " ('training corpus', 9),\n",
       " ('multimodal information', 9),\n",
       " ('document modeling', 9),\n",
       " ('semantic topic', 9),\n",
       " ('training dataset', 9),\n",
       " ('Gibbs sampler', 9),\n",
       " ('efficient algorithm', 9),\n",
       " ('topic change', 9),\n",
       " ('high resolution', 9),\n",
       " ('oov pn', 9),\n",
       " ('John Wiley', 9),\n",
       " ('genetic algorithm', 9),\n",
       " ('variational method', 9),\n",
       " ('domain expert', 9),\n",
       " ('singular value decomposition', 9),\n",
       " ('web document', 9),\n",
       " ('digital library', 9),\n",
       " ('super user', 9),\n",
       " ('preliminary result', 9),\n",
       " ('second method', 9),\n",
       " ('Kullback-Leibler divergence', 9),\n",
       " ('scene recognition', 9),\n",
       " ('hsr imagery', 9),\n",
       " ('previous approach', 9),\n",
       " ('training image', 9),\n",
       " ('image feature', 9),\n",
       " ('traffic incident', 9),\n",
       " ('tag-topic model', 9),\n",
       " ('current research', 8),\n",
       " ('scientific publication', 8),\n",
       " ('large quantity', 8),\n",
       " ('related word', 8),\n",
       " ('key topic', 8),\n",
       " ('regression model', 8),\n",
       " ('news topic', 8),\n",
       " ('explosive growth', 8),\n",
       " ('key feature', 8),\n",
       " ('recall rate', 8),\n",
       " ('sentiment word', 8),\n",
       " ('overall rating', 8),\n",
       " ('non-negative matrix factorization', 8),\n",
       " ('unsupervised machine', 8),\n",
       " ('many study', 8),\n",
       " ('logistic regression', 8),\n",
       " ('service quality', 8),\n",
       " ('word cloud', 8),\n",
       " ('data-driven approach', 8),\n",
       " ('deep learning method', 8),\n",
       " ('software artifact', 8),\n",
       " ('personalized recommendation', 8),\n",
       " ('different region', 8),\n",
       " ('core topic', 8),\n",
       " ('semantic structure', 8),\n",
       " ('semantic coherence', 8),\n",
       " ('high level', 8),\n",
       " ('recent study', 8),\n",
       " ('further research', 8),\n",
       " ('quantitative analysis', 8),\n",
       " ('classification model', 8),\n",
       " ('new way', 8),\n",
       " ('clustering approach', 8),\n",
       " ('feature vector', 8),\n",
       " ('latent aspect', 8),\n",
       " ('feature set', 8),\n",
       " ('large datum set', 8),\n",
       " ('first study', 8),\n",
       " ('natural language processing technique', 8),\n",
       " ('document analysis', 8),\n",
       " ('textual feature', 8),\n",
       " ('topic-based approach', 8),\n",
       " ('topic-based model', 8),\n",
       " ('difficult task', 8),\n",
       " ('important source', 8),\n",
       " ('several study', 8),\n",
       " ('text clustering', 8),\n",
       " ('different weight', 8),\n",
       " ('low-dimensional representation', 8),\n",
       " ('machine learning algorithm', 8),\n",
       " ('satisfactory result', 8),\n",
       " ('major topic', 8),\n",
       " ('input datum', 8),\n",
       " ('weak hypothesis', 8),\n",
       " ('mutual information', 8),\n",
       " ('diabetes mellitus', 8),\n",
       " ('encouraging result', 8),\n",
       " ('different perspective', 8),\n",
       " ('new type', 8),\n",
       " ('optimal number', 8),\n",
       " ('unseen datum', 8),\n",
       " ('probabilistic distribution', 8),\n",
       " ('semantic representation', 8),\n",
       " ('human topic ranking', 8),\n",
       " ('Dirichlet prior', 8),\n",
       " ('challenging problem', 8),\n",
       " ('Wikipedia article', 8),\n",
       " ('program comprehension', 8),\n",
       " ('unstructured text', 8),\n",
       " ('community detection', 8),\n",
       " ('state-of-the-art baseline', 8),\n",
       " ('clustering technique', 8),\n",
       " ('input image', 8),\n",
       " ('topic network', 8),\n",
       " ('potential topic', 8),\n",
       " ('latent topic structure', 8),\n",
       " ('unsupervised topic model', 8),\n",
       " ('alternative method', 8),\n",
       " ('latent factor model', 8),\n",
       " ('international student', 8),\n",
       " ('different user', 8),\n",
       " ('self-driving car', 8),\n",
       " ('software maintenance', 8),\n",
       " ('topic modelling method', 8),\n",
       " ('valuable source', 8),\n",
       " ('latent factor', 8),\n",
       " ('Twitter user', 8),\n",
       " ('social tag', 8),\n",
       " ('web image', 8),\n",
       " ('important part', 8),\n",
       " ('hsr image', 8),\n",
       " ('higher precision', 8),\n",
       " ('Markov chain', 8),\n",
       " ('user context', 8),\n",
       " ('first method', 8),\n",
       " ('vocabulary size', 8),\n",
       " ('proper name', 8),\n",
       " ('alternative approach', 8),\n",
       " ('active user', 8),\n",
       " ('e. g.', 8),\n",
       " ('common crawl', 8),\n",
       " ('classification approach', 8),\n",
       " ('similar interest', 8),\n",
       " ('software repository', 8),\n",
       " ('treatment pattern', 8),\n",
       " ('lda-based technique', 8),\n",
       " ('probability model', 8),\n",
       " ('natural language text', 8),\n",
       " ('document retrieval', 8),\n",
       " ('tv user', 8),\n",
       " ('datum sparseness', 8),\n",
       " ('Great East Japan Earthquake', 8),\n",
       " ('life style', 8),\n",
       " ('scene class', 8),\n",
       " ('activity routine', 8),\n",
       " ('reliable user', 8),\n",
       " ('clinical trial protocol', 8),\n",
       " ('cluster ensemble', 8),\n",
       " ('visual analytic', 7),\n",
       " ('tourist attraction', 7),\n",
       " ('common topic', 7),\n",
       " ('time period', 7),\n",
       " ('destination image', 7),\n",
       " ('second step', 7),\n",
       " ('future study', 7),\n",
       " ('user study', 7),\n",
       " ('information propagation', 7),\n",
       " ('co-occurrence pattern', 7),\n",
       " ('novel technique', 7),\n",
       " ('pattern recognition', 7),\n",
       " ('simulation study', 7),\n",
       " ('retrieval result', 7),\n",
       " ('urban planning', 7),\n",
       " ('allocation topic modeling', 7),\n",
       " ('bag-of-word model', 7),\n",
       " ('software project', 7),\n",
       " ('significant challenge', 7),\n",
       " ('state-of-the-art model', 7),\n",
       " ('other topic', 7),\n",
       " ('Linguistic Inquiry', 7),\n",
       " ('dominant topic', 7),\n",
       " ('cold topic', 7),\n",
       " ('research gap', 7),\n",
       " ('latent interest', 7),\n",
       " ('online user', 7),\n",
       " ('empirical analysis', 7),\n",
       " ('intelligent system', 7),\n",
       " ('text mining approach', 7),\n",
       " ('mixture distribution', 7),\n",
       " ('Markov chain Monte Carlo', 7),\n",
       " ('available online', 7),\n",
       " ('hidden knowledge', 7),\n",
       " ('human perception', 7),\n",
       " ('trend analysis', 7),\n",
       " ('mental health', 7),\n",
       " ('computational efficiency', 7),\n",
       " ('transfer learning', 7),\n",
       " ('relevant study', 7),\n",
       " ('practical implication', 7),\n",
       " ('current approach', 7),\n",
       " ('effective tool', 7),\n",
       " ('various method', 7),\n",
       " ('urgent care center', 7),\n",
       " ('predictive power', 7),\n",
       " ('service description', 7),\n",
       " ('problem list', 7),\n",
       " ('great deal', 7),\n",
       " ('same event', 7),\n",
       " ('target user', 7),\n",
       " ('mining process', 7),\n",
       " ('many method', 7),\n",
       " ('visual information', 7),\n",
       " ('semantic level', 7),\n",
       " ('sparse coding', 7),\n",
       " ('information gain', 7),\n",
       " ('word model', 7),\n",
       " ('embedding vector', 7),\n",
       " ('spectral clustering', 7),\n",
       " ('multiple feature', 7),\n",
       " ('recent work', 7),\n",
       " ('last decade', 7),\n",
       " ('fraud detection', 7),\n",
       " ('sentiment polarity', 7),\n",
       " ('significant role', 7),\n",
       " ('input document', 7),\n",
       " ('limited number', 7),\n",
       " ('probabilistic modeling', 7),\n",
       " ('potential value', 7),\n",
       " ('text message', 7),\n",
       " ('unsupervised fashion', 7),\n",
       " ('linguistic feature', 7),\n",
       " ('allocation algorithm', 7),\n",
       " ('fake review', 7),\n",
       " ('other disease', 7),\n",
       " ('ground truth', 7),\n",
       " ('such method', 7),\n",
       " ('proposed approach', 7),\n",
       " ('huge number', 7),\n",
       " ('supervised information', 7),\n",
       " ('useful insight', 7),\n",
       " ('Hidden Markov Model', 7),\n",
       " ('many case', 7),\n",
       " ('literature review', 7),\n",
       " ('heterogeneous network', 7),\n",
       " ('document-topic distribution', 7),\n",
       " ('overall accuracy', 7),\n",
       " ('classification result', 7),\n",
       " ('new perspective', 7),\n",
       " ('full-text datum', 7),\n",
       " ('different size', 7),\n",
       " ('bow representation', 7),\n",
       " ('computational linguistics', 7),\n",
       " ('social image', 7),\n",
       " ('Markov model', 7),\n",
       " ('many researcher', 7),\n",
       " ('citation network', 7),\n",
       " ('content-based method', 7),\n",
       " ('word level', 7),\n",
       " ('real world datum', 7),\n",
       " ('further analysis', 7),\n",
       " ('training sample', 7),\n",
       " ('new challenge', 7),\n",
       " ('receiver operating', 7),\n",
       " ('time complexity', 7),\n",
       " ('different feature', 7),\n",
       " ('complementary information', 7),\n",
       " ('topic classification', 7),\n",
       " ('topic segmentation', 7),\n",
       " ('open problem', 7),\n",
       " ('new algorithm', 7),\n",
       " ('recent research', 7),\n",
       " ('further improvement', 7),\n",
       " ('recommendation performance', 7),\n",
       " ('space complexity', 7),\n",
       " ('software developer', 7),\n",
       " ('software evolution', 7),\n",
       " ('newspaper article', 7),\n",
       " ('service composition', 7),\n",
       " ('available datum', 7),\n",
       " ('traditional LDA model', 7),\n",
       " ('EM algorithm', 7),\n",
       " ('indonesian text', 7),\n",
       " ('web content', 7),\n",
       " ('various source', 7),\n",
       " ('textual review', 7),\n",
       " ('computational time', 7),\n",
       " ('computational model', 7),\n",
       " ('spatial pattern', 7),\n",
       " ('object concept', 7),\n",
       " ('recent advance', 7),\n",
       " ('sentiment lexicon', 7),\n",
       " ('activity recognition', 7),\n",
       " ('particular topic', 7),\n",
       " ('time dimension', 7),\n",
       " ('multi-document summarization', 7),\n",
       " ('expert finding system', 7),\n",
       " ('variational baye', 7),\n",
       " ('other domain', 7),\n",
       " ('social network datum', 7),\n",
       " ('keyword extraction', 7),\n",
       " ('mixture weight', 7),\n",
       " ('new question', 7),\n",
       " ('novel algorithm', 7),\n",
       " ('risk stratification', 7),\n",
       " ('time point', 7),\n",
       " ('latent Dirichlet allocation technique', 7),\n",
       " ('mobile application', 7),\n",
       " ('eligibility criterion', 7),\n",
       " ('temporal feature', 7),\n",
       " ('wide variety', 7),\n",
       " ('optimization problem', 7),\n",
       " ('patient trace', 7),\n",
       " ('lda-based method', 7),\n",
       " ('document cluster', 7),\n",
       " ('patient record', 7),\n",
       " ('datum point', 7),\n",
       " ('optical flow', 7),\n",
       " ('language model adaptation', 7),\n",
       " ('feature descriptor', 7),\n",
       " ('dimension reduction', 7),\n",
       " ('service ecosystem', 7),\n",
       " ('social circle', 7),\n",
       " ('target domain', 7),\n",
       " ('social tagging system', 7),\n",
       " ('human motion', 7),\n",
       " ('unigram scaling', 7),\n",
       " ('co-occurrence event', 7),\n",
       " ('drive topic', 7),\n",
       " ('unsupervised topic modeling', 6),\n",
       " ('research domain', 6),\n",
       " ('various application', 6),\n",
       " ('overall performance', 6),\n",
       " ('thematic structure', 6),\n",
       " ('local context', 6),\n",
       " ('real datum set', 6),\n",
       " ('CNN model', 6),\n",
       " ('precision rate', 6),\n",
       " ('digital document', 6),\n",
       " ('benchmark datum set', 6),\n",
       " ('learning model', 6),\n",
       " ('last year', 6),\n",
       " ('time frame', 6),\n",
       " ('research question', 6),\n",
       " ('higher performance', 6),\n",
       " ('temporal dynamics', 6),\n",
       " ('large-scale dataset', 6),\n",
       " ('k-means clustering algorithm', 6),\n",
       " ('datum collection', 6),\n",
       " ('compact representation', 6),\n",
       " ('expectation-maximization algorithm', 6),\n",
       " ('annotation performance', 6),\n",
       " ('many people', 6),\n",
       " ('topic modeling process', 6),\n",
       " ('current state', 6),\n",
       " ('integral part', 6),\n",
       " ('acoustic feature', 6),\n",
       " ('naive Bayes model', 6),\n",
       " ('sample size', 6),\n",
       " ('datum size', 6),\n",
       " ('patent analysis', 6),\n",
       " ('software engineering', 6),\n",
       " ('distinct topic', 6),\n",
       " ('similarity metric', 6),\n",
       " ('popular topic model', 6),\n",
       " ('patent document', 6),\n",
       " ('public sentiment', 6),\n",
       " ('spatial distribution', 6),\n",
       " ('topic modeling framework', 6),\n",
       " ('latent topic distribution', 6),\n",
       " ('correlation analysis', 6),\n",
       " ('machine translation', 6),\n",
       " ('domain adaptation', 6),\n",
       " ('contact surface', 6),\n",
       " ('main challenge', 6),\n",
       " ('traditional LDA', 6),\n",
       " ('decision making', 6),\n",
       " ('significant change', 6),\n",
       " ('high quality', 6),\n",
       " ('latent pattern', 6),\n",
       " ('comprehensive experiment', 6),\n",
       " ('quantitative evaluation', 6),\n",
       " ('specific aspect', 6),\n",
       " ('service discovery', 6),\n",
       " ('main goal', 6),\n",
       " ('evaluation metric', 6),\n",
       " ('important tool', 6),\n",
       " ('cryptocurrency investment', 6),\n",
       " ('nonnegative matrix factorization', 6),\n",
       " ('privacy policy', 6),\n",
       " ('feature representation', 6),\n",
       " ('recommendation model', 6),\n",
       " ('Gibbs sampling method', 6),\n",
       " ('news item', 6),\n",
       " ('great significance', 6),\n",
       " ('first stage', 6),\n",
       " ('semantic dependency', 6),\n",
       " ('paragraph vector', 6),\n",
       " ('latent theme', 6),\n",
       " ('real-life dataset', 6),\n",
       " ('biomedical researcher', 6),\n",
       " ('research direction', 6),\n",
       " ('main research area', 6),\n",
       " ('New York City', 6),\n",
       " ('significant increase', 6),\n",
       " ('active learning', 6),\n",
       " ('online news website', 6),\n",
       " ('key component', 6),\n",
       " ('information sharing', 6),\n",
       " ('background knowledge', 6),\n",
       " ('Bayes model', 6),\n",
       " ('lda approach', 6),\n",
       " ('smart city', 6),\n",
       " ('different time period', 6),\n",
       " ('ad video', 6),\n",
       " ('Dirichlet allocation algorithm', 6),\n",
       " ('asymmetrical prior', 6),\n",
       " ('supervised learning method', 6),\n",
       " ('online product review', 6),\n",
       " ('content similarity', 6),\n",
       " ('spatial relation', 6),\n",
       " ('probabilistic approach', 6),\n",
       " ('prior work', 6),\n",
       " ('full use', 6),\n",
       " ('latent Dirichlet allocation topic modeling technique', 6),\n",
       " ('complex network', 6),\n",
       " ('ranking function', 6),\n",
       " ('search space', 6),\n",
       " ('web service description', 6),\n",
       " ('hidden structure', 6),\n",
       " ('multiple source', 6),\n",
       " ('topic stability', 6),\n",
       " ('small amount', 6),\n",
       " ('topic layer', 6),\n",
       " ('different scale', 6),\n",
       " ('new framework', 6),\n",
       " ('language modeling', 6),\n",
       " ('great success', 6),\n",
       " ('open source project', 6),\n",
       " ('response variable', 6),\n",
       " ('variational approximation', 6),\n",
       " ('important factor', 6),\n",
       " ('sensor datum', 6),\n",
       " ('continuous datum', 6),\n",
       " ('hierarchical topic model', 6),\n",
       " ('model perplexity', 6),\n",
       " ('image patch', 6),\n",
       " ('count matrix', 6),\n",
       " ('research area', 6),\n",
       " ('training process', 6),\n",
       " ('Big Data', 6),\n",
       " ('little attention', 6),\n",
       " ('sentiment-aware topic', 6),\n",
       " ('pseudo document', 6),\n",
       " ('automatic method', 6),\n",
       " ('Dirichlet process', 6),\n",
       " ('target word', 6),\n",
       " ('semantic indexing', 6),\n",
       " ('support Vector Machines', 6),\n",
       " ('business process', 6),\n",
       " ('concurrency topic', 6),\n",
       " ('computation cost', 6),\n",
       " ('novel way', 6),\n",
       " ('single topic', 6),\n",
       " ('LDA topic', 6),\n",
       " ('high-dimensional datum', 6),\n",
       " ('financial news', 6),\n",
       " ('similar document', 6),\n",
       " ('popular method', 6),\n",
       " ('contextual feature', 6),\n",
       " ('single document', 6),\n",
       " ('other language', 6),\n",
       " ('user activity', 6),\n",
       " ('Vector Space Model', 6),\n",
       " ('lda-based approach', 6),\n",
       " ('supervised method', 6),\n",
       " ('average accuracy', 6),\n",
       " ('crowded scene', 6),\n",
       " ('many domain', 6),\n",
       " ('high performance', 6),\n",
       " ('large archive', 6),\n",
       " ('media platform', 6),\n",
       " ('topic clustering', 6),\n",
       " ('movie review', 6),\n",
       " ('co-occurrence relation', 6),\n",
       " ('many field', 6),\n",
       " ('difficult problem', 6),\n",
       " ('different model', 6),\n",
       " ('different context', 6),\n",
       " ('modeling topic', 6),\n",
       " ('recommendation quality', 6),\n",
       " ('unstructured document', 6),\n",
       " ('latent Dirichlet allocation topic', 6),\n",
       " ('point cloud', 6),\n",
       " ('different location', 6),\n",
       " ('bug localization', 6),\n",
       " ('topic generation', 6),\n",
       " ('unsupervised algorithm', 6),\n",
       " ('practical application', 6),\n",
       " ('partial membership', 6),\n",
       " ('user-generated contents', 6),\n",
       " ('online learning', 6),\n",
       " ('human knowledge', 6),\n",
       " ...]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = relation.get_phrases_by_pattern(parse)\n",
    "sorted(dict(Counter([\" \".join(x) for x in ps])).items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'newsparse.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-468c66136e0d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfail\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrelation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_parse_failed_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEWSPARSE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mCounter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m \u001b[0;34m//\u001b[0m \u001b[0;36m500\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mf\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfail\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/e/_ued/code/relation.py\u001b[0m in \u001b[0;36mget_parse_failed_idx\u001b[0;34m(persist_file_name)\u001b[0m\n\u001b[1;32m     81\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m     \"\"\"\n\u001b[0;32m---> 83\u001b[0;31m     \u001b[0mparseres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersister\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpersist_file_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m     \u001b[0mfailed_idxs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparseres\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/e/_ued/code/persister.py\u001b[0m in \u001b[0;36mread_parse\u001b[0;34m(json_name)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mread_parse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEWSPARSE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_name\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".json\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0mnewsparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0mnewsparse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnewsparse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'newsparse.json'"
     ]
    }
   ],
   "source": [
    "fail = relation.get_parse_failed_idx(configs.NEWSPARSE)\n",
    "Counter([f // 500 for f in fail])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lda362",
   "language": "python",
   "name": "lda362"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
