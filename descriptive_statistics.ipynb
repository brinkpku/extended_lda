{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not use stanford CoreNLP client!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading sentiwordnet: <urlopen error [Errno 111]\n",
      "[nltk_data]     Connection refused>\n"
     ]
    }
   ],
   "source": [
    "import preprocess as pp\n",
    "import configs\n",
    "import persister\n",
    "import relation\n",
    "import lda\n",
    "import vis\n",
    "import evaluate\n",
    "from utils import *\n",
    "\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from functools import reduce\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tree import Tree\n",
    "from nltk import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# is_news = 1\n",
    "\n",
    "# l = 7\n",
    "# t = 4\n",
    "# m = \"c_v\"\n",
    "# i = 200\n",
    "# min_df = 1\n",
    "\n",
    "is_news = 0\n",
    "l = 6\n",
    "t = 14\n",
    "m = \"c_v\"\n",
    "i = 200\n",
    "min_df = 1\n",
    "\n",
    "# load\n",
    "if is_news:\n",
    "    _raw = persister.load_json(configs.RAWNEWS)\n",
    "    parse = persister.read_parse()\n",
    "    _input = persister.read_input(configs.NEWSINPUT)\n",
    "    model_name = configs.NEWSMODEL.format(l, t, m, i, min_df)\n",
    "    terms, doc_topic, topic_word = persister.read_lda(configs.NEWSLDA.format(model_name))\n",
    "    model = persister.load_model(model_name)\n",
    "    vec = persister.load_model(configs.NEWSVEC.format(min_df))\n",
    "    print(\"load news\")\n",
    "else:\n",
    "    _raw = persister.load_json(configs.RAWABSTRACT)\n",
    "    _input = persister.read_input(configs.ABSTRACTINPUT)\n",
    "    model_name = configs.ABSTRACTMODEL.format(l, t, m, i, min_df)\n",
    "    terms, doc_topic, topic_word = persister.read_lda(configs.ABSTRACTLDA.format(model_name))\n",
    "    parse = persister.read_parse(configs.ABSTRACTPARSE)\n",
    "    model = persister.load_model(model_name)\n",
    "    vec = persister.load_model(configs.ABSVEC.format(min_df))\n",
    "    print(\"load abs\")\n",
    "tf = vec.fit_transform(_input)\n",
    "top_terms, top_docs = lda.get_topics(topic_word, terms, doc_topic)\n",
    "df_top_words, df_top_docs = lda.pd_topics_vis(top_terms, top_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distr = lda.get_dominant_topic(doc_topic)\n",
    "print(\"docs distribution:\",dict(Counter(distr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"top terms info:\")\n",
    "df_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"top docs info:\")\n",
    "df_top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis.pyLDA(topic_word, doc_topic, [len(s) for s in [word_tokenize(corp) for corp in _input]], vec.get_feature_names(), np.array(sum(tf).todense())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_idx = 1\n",
    "for top_doc in top_docs[t_idx]:\n",
    "    _idx = top_doc[0]\n",
    "    if type(parse[_idx]) is str:\n",
    "        print(_idx, \"parse\", \"err\")\n",
    "        continue\n",
    "    sents = relation.convert_parse2lemma_sents(parse[_idx])\n",
    "    sort_idxs, importance, counts = relation.extract_important_sents(sents, [terms[x[0]] for x in top_terms[t_idx]], [x[1] for x in top_terms[t_idx]])\n",
    "    for i in sort_idxs:\n",
    "        if importance[i]>0:\n",
    "            print(round(importance[i], 2), counts[i], _idx, i)\n",
    "            sent_tokens = parse[_idx][\"sentences\"][i][\"tokens\"]\n",
    "            sent_deps = parse[_idx][\"sentences\"][i][\"enhancedPlusPlusDependencies\"]\n",
    "            print(\" \".join([s[\"lemma\"] for s in sent_tokens]))\n",
    "            sents = sent_tokenize(_raw[_idx])\n",
    "            if len(sents) > i:\n",
    "                print(sents[i])\n",
    "            else:\n",
    "                print(\"sent token differ\")\n",
    "            triples = relation.extract_triples_from_sent(sent_deps, sent_tokens, True)\n",
    "            for triple in triples:\n",
    "                if None in triple:\n",
    "                    print(triple)\n",
    "                    continue\n",
    "                s = [sent_tokens[i][\"originalText\"] for i in triple[0]]\n",
    "                p = [sent_tokens[i][\"originalText\"] for i in triple[1]]\n",
    "                o = [sent_tokens[i][\"originalText\"] for i in triple[2]]\n",
    "                print(\"{}-{}-{}\".format(s,p,o))\n",
    "            print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extended = relation.extend_lda_results(parse, _input, top_terms, top_docs, terms, \"lemma\", top_n=-1, score_method=\"tfidf\")\n",
    "print(len(extended[t_idx]))\n",
    "for triple in extended[t_idx]:\n",
    "    print(round(triple[1],2),\"{}-[{}]-{}\".format(\" \".join(triple[0][0]),\" \".join(triple[0][1]),\" \".join(triple[0][2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = 110\n",
    "measure = \"c_v\"\n",
    "old = lda.get_coherence(tf, terms, topic_word, _input, measure, window_size=ws)\n",
    "fake_idxs = []\n",
    "min_top = 999\n",
    "for e in extended: # topic\n",
    "    new_word_idxs = []\n",
    "    for t in e[:20]:# triples\n",
    "         new_word_idxs.extend(evaluate.filter_triple2term_idx(t[0], vec))\n",
    "    if min_top > len(set(new_word_idxs)):\n",
    "        min_top = len(set(new_word_idxs))\n",
    "    fake_idxs.append(set(new_word_idxs))\n",
    "fake_topic_word = evaluate.gen_fake_topic_word(topic_word.shape, fake_idxs)\n",
    "new = lda.get_coherence(tf, terms, fake_topic_word, _input, measure,min_top,window_size=ws)\n",
    "print(old, new)\n",
    "print(np.mean(old), np.mean(new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([terms[i] for i in fake_idxs[1]])\n",
    "print([terms[i[0]] for i in top_terms[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = relation.get_phrases_by_pattern(parse)\n",
    "sorted(dict(Counter([\" \".join(x) for x in ps])).items(), key=lambda x:x[1], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lda362",
   "language": "python",
   "name": "lda362"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
