{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not use stanford CoreNLP client!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading sentiwordnet: <urlopen error [Errno 111]\n",
      "[nltk_data]     Connection refused>\n"
     ]
    }
   ],
   "source": [
    "import preprocess as pp\n",
    "import configs\n",
    "import persister\n",
    "import relation\n",
    "import lda\n",
    "import vis\n",
    "from utils import *\n",
    "\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from functools import reduce\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tree import Tree\n",
    "from nltk import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load abs\n"
     ]
    }
   ],
   "source": [
    "# is_news = 1\n",
    "\n",
    "# l = 7\n",
    "# t = 4\n",
    "# m = \"c_v\"\n",
    "# i = 200\n",
    "# min_df = 1\n",
    "\n",
    "is_news = 0\n",
    "l = 6\n",
    "t = 14\n",
    "m = \"c_v\"\n",
    "i = 200\n",
    "min_df = 1\n",
    "\n",
    "# load\n",
    "if is_news:\n",
    "    _raw = persister.load_json(configs.RAWNEWS)\n",
    "    parse = persister.read_parse()\n",
    "    _input = persister.read_input(configs.NEWSINPUT)\n",
    "    model_name = configs.NEWSMODEL.format(l, t, m, i, min_df)\n",
    "    terms, doc_topic, topic_word = persister.read_lda(configs.NEWSLDA.format(model_name))\n",
    "    model = persister.load_model(model_name)\n",
    "    vec = persister.load_model(configs.NEWSVEC.format(min_df))\n",
    "    print(\"load news\")\n",
    "else:\n",
    "    _raw = persister.load_json(configs.RAWABSTRACT)\n",
    "    _input = persister.read_input(configs.ABSTRACTINPUT)\n",
    "    model_name = configs.ABSTRACTMODEL.format(l, t, m, i, min_df)\n",
    "    terms, doc_topic, topic_word = persister.read_lda(configs.ABSTRACTLDA.format(model_name))\n",
    "    parse = persister.read_parse(configs.ABSTRACTPARSE)\n",
    "    model = persister.load_model(model_name)\n",
    "    vec = persister.load_model(configs.ABSVEC.format(min_df))\n",
    "    print(\"load abs\")\n",
    "tf = vec.fit_transform(_input) \n",
    "top_terms, top_docs = lda.get_topics(topic_word, terms, doc_topic, 20)\n",
    "df_top_words, df_top_docs = lda.pd_topics_vis(top_terms, top_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10453"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs distribution: {9: 152, 5: 33, 11: 969, 12: 1330, 1: 7, 13: 3, 10: 76, 3: 3, 0: 8, 7: 14, 6: 1}\n"
     ]
    }
   ],
   "source": [
    "distr = lda.get_dominant_topic(doc_topic)\n",
    "print(\"docs distribution:\",dict(Counter(distr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top terms info:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "      <th>Word 11</th>\n",
       "      <th>Word 12</th>\n",
       "      <th>Word 13</th>\n",
       "      <th>Word 14</th>\n",
       "      <th>Word 15</th>\n",
       "      <th>Word 16</th>\n",
       "      <th>Word 17</th>\n",
       "      <th>Word 18</th>\n",
       "      <th>Word 19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>(gene, 123.1)</td>\n",
       "      <td>(drug, 96.87)</td>\n",
       "      <td>(expression, 78.19)</td>\n",
       "      <td>(disease, 77.49)</td>\n",
       "      <td>(protein, 75.45)</td>\n",
       "      <td>(biomedical, 64.39)</td>\n",
       "      <td>(pathway, 51.34)</td>\n",
       "      <td>(cell, 51.19)</td>\n",
       "      <td>(identify, 48.51)</td>\n",
       "      <td>(biological, 47.03)</td>\n",
       "      <td>(btm, 40.99)</td>\n",
       "      <td>(ad, 40.76)</td>\n",
       "      <td>(study, 39.19)</td>\n",
       "      <td>(apply, 37.74)</td>\n",
       "      <td>(manufacturing, 31.56)</td>\n",
       "      <td>(mrna, 29.42)</td>\n",
       "      <td>(relationship, 29.31)</td>\n",
       "      <td>(conclusion, 28.59)</td>\n",
       "      <td>(tool, 26.52)</td>\n",
       "      <td>(related, 25.93)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>(patient, 223.06)</td>\n",
       "      <td>(medical, 143.69)</td>\n",
       "      <td>(health, 143.5)</td>\n",
       "      <td>(clinical, 139.72)</td>\n",
       "      <td>(treatment, 99.44)</td>\n",
       "      <td>(diagnosis, 86.78)</td>\n",
       "      <td>(disease, 81.07)</td>\n",
       "      <td>(risk, 79.0)</td>\n",
       "      <td>(record, 78.67)</td>\n",
       "      <td>(care, 68.06)</td>\n",
       "      <td>(friend, 47.64)</td>\n",
       "      <td>(healthcare, 42.31)</td>\n",
       "      <td>(cancer, 39.36)</td>\n",
       "      <td>(code, 36.28)</td>\n",
       "      <td>(data, 32.74)</td>\n",
       "      <td>(conclusion, 30.77)</td>\n",
       "      <td>(physician, 29.99)</td>\n",
       "      <td>(emr, 25.48)</td>\n",
       "      <td>(ehr, 25.13)</td>\n",
       "      <td>(doctor, 24.57)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>(mirna, 42.64)</td>\n",
       "      <td>(permission, 25.17)</td>\n",
       "      <td>(quran, 13.82)</td>\n",
       "      <td>(holy, 9.76)</td>\n",
       "      <td>(aircraft, 8.0)</td>\n",
       "      <td>(provenance, 7.74)</td>\n",
       "      <td>(lv, 7.74)</td>\n",
       "      <td>(laplace, 7.05)</td>\n",
       "      <td>(ast, 6.96)</td>\n",
       "      <td>(chara, 6.65)</td>\n",
       "      <td>(yuru, 6.65)</td>\n",
       "      <td>(ldaclm, 6.22)</td>\n",
       "      <td>(privilege, 5.86)</td>\n",
       "      <td>(mmrm, 5.86)</td>\n",
       "      <td>(iexpand, 5.13)</td>\n",
       "      <td>(configure, 5.01)</td>\n",
       "      <td>(transcriptional, 5.0)</td>\n",
       "      <td>(pcfg, 4.14)</td>\n",
       "      <td>(tctm, 4.12)</td>\n",
       "      <td>(seqlda, 4.12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>(emotion, 181.73)</td>\n",
       "      <td>(drive, 87.04)</td>\n",
       "      <td>(style, 86.19)</td>\n",
       "      <td>(speech, 55.34)</td>\n",
       "      <td>(genre, 39.85)</td>\n",
       "      <td>(recognition, 38.09)</td>\n",
       "      <td>(speaker, 34.84)</td>\n",
       "      <td>(state, 33.33)</td>\n",
       "      <td>(write, 31.62)</td>\n",
       "      <td>(language, 30.16)</td>\n",
       "      <td>(acoustic, 30.1)</td>\n",
       "      <td>(driving, 29.08)</td>\n",
       "      <td>(driver, 27.25)</td>\n",
       "      <td>(essay, 26.57)</td>\n",
       "      <td>(sleep, 24.96)</td>\n",
       "      <td>(vehicle, 24.49)</td>\n",
       "      <td>(authorship, 22.67)</td>\n",
       "      <td>(noca, 22.09)</td>\n",
       "      <td>(author, 21.59)</td>\n",
       "      <td>(disorder, 21.36)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>(sampling, 79.5)</td>\n",
       "      <td>(parallel, 73.15)</td>\n",
       "      <td>(gibbs, 67.68)</td>\n",
       "      <td>(security, 57.19)</td>\n",
       "      <td>(distribute, 52.59)</td>\n",
       "      <td>(collapse, 42.46)</td>\n",
       "      <td>(cluster, 33.26)</td>\n",
       "      <td>(bot, 32.4)</td>\n",
       "      <td>(brand, 31.0)</td>\n",
       "      <td>(large, 30.54)</td>\n",
       "      <td>(crime, 29.71)</td>\n",
       "      <td>(scale, 29.68)</td>\n",
       "      <td>(token, 29.34)</td>\n",
       "      <td>(approximation, 28.55)</td>\n",
       "      <td>(memory, 27.44)</td>\n",
       "      <td>(gpu, 27.32)</td>\n",
       "      <td>(sampler, 26.77)</td>\n",
       "      <td>(matrix, 23.32)</td>\n",
       "      <td>(atomic, 23.12)</td>\n",
       "      <td>(mh, 22.88)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 5</th>\n",
       "      <td>(review, 537.88)</td>\n",
       "      <td>(sentiment, 474.31)</td>\n",
       "      <td>(opinion, 265.32)</td>\n",
       "      <td>(tweet, 235.55)</td>\n",
       "      <td>(product, 228.45)</td>\n",
       "      <td>(post, 206.48)</td>\n",
       "      <td>(online, 196.94)</td>\n",
       "      <td>(topic, 194.3)</td>\n",
       "      <td>(analysis, 180.66)</td>\n",
       "      <td>(twitter, 174.52)</td>\n",
       "      <td>(media, 171.56)</td>\n",
       "      <td>(aspect, 166.01)</td>\n",
       "      <td>(customer, 159.65)</td>\n",
       "      <td>(rating, 142.65)</td>\n",
       "      <td>(consumer, 111.75)</td>\n",
       "      <td>(social, 101.7)</td>\n",
       "      <td>(use, 101.33)</td>\n",
       "      <td>(negative, 88.63)</td>\n",
       "      <td>(positive, 85.19)</td>\n",
       "      <td>(study, 81.34)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 6</th>\n",
       "      <td>(song, 81.12)</td>\n",
       "      <td>(audio, 74.02)</td>\n",
       "      <td>(music, 57.13)</td>\n",
       "      <td>(pn, 35.01)</td>\n",
       "      <td>(broadcast, 28.77)</td>\n",
       "      <td>(oov, 24.22)</td>\n",
       "      <td>(indonesian, 19.81)</td>\n",
       "      <td>(musical, 18.49)</td>\n",
       "      <td>(lyric, 17.49)</td>\n",
       "      <td>(estimate, 16.39)</td>\n",
       "      <td>(writer, 14.97)</td>\n",
       "      <td>(commonness, 14.53)</td>\n",
       "      <td>(submit, 13.59)</td>\n",
       "      <td>(hyperspectral, 11.69)</td>\n",
       "      <td>(eem, 10.71)</td>\n",
       "      <td>(mp, 10.39)</td>\n",
       "      <td>(isoform, 9.79)</td>\n",
       "      <td>(retrieve, 9.73)</td>\n",
       "      <td>(replicate, 9.62)</td>\n",
       "      <td>(timbre, 8.87)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 7</th>\n",
       "      <td>(software, 268.18)</td>\n",
       "      <td>(question, 203.92)</td>\n",
       "      <td>(code, 149.7)</td>\n",
       "      <td>(developer, 143.61)</td>\n",
       "      <td>(source, 143.52)</td>\n",
       "      <td>(bug, 139.43)</td>\n",
       "      <td>(report, 120.26)</td>\n",
       "      <td>(project, 87.55)</td>\n",
       "      <td>(program, 68.98)</td>\n",
       "      <td>(answer, 65.81)</td>\n",
       "      <td>(repository, 53.22)</td>\n",
       "      <td>(requirement, 49.86)</td>\n",
       "      <td>(evolution, 49.8)</td>\n",
       "      <td>(technique, 48.97)</td>\n",
       "      <td>(open, 44.34)</td>\n",
       "      <td>(study, 43.24)</td>\n",
       "      <td>(lsi, 40.78)</td>\n",
       "      <td>(stack, 33.35)</td>\n",
       "      <td>(change, 30.56)</td>\n",
       "      <td>(nfr, 30.05)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 8</th>\n",
       "      <td>(lm, 75.09)</td>\n",
       "      <td>(frame, 55.04)</td>\n",
       "      <td>(library, 31.56)</td>\n",
       "      <td>(asr, 28.23)</td>\n",
       "      <td>(street, 22.76)</td>\n",
       "      <td>(ldta, 18.42)</td>\n",
       "      <td>(adr, 18.41)</td>\n",
       "      <td>(rnn, 17.93)</td>\n",
       "      <td>(abbreviation, 15.75)</td>\n",
       "      <td>(adverse, 15.54)</td>\n",
       "      <td>(adapted, 15.29)</td>\n",
       "      <td>(lsm, 13.09)</td>\n",
       "      <td>(eligibility, 13.06)</td>\n",
       "      <td>(india, 12.3)</td>\n",
       "      <td>(attraction, 11.6)</td>\n",
       "      <td>(salience, 11.02)</td>\n",
       "      <td>(wall, 10.98)</td>\n",
       "      <td>(grams, 10.91)</td>\n",
       "      <td>(journal, 10.27)</td>\n",
       "      <td>(dntm, 10.12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 9</th>\n",
       "      <td>(topic, 893.39)</td>\n",
       "      <td>(research, 662.45)</td>\n",
       "      <td>(study, 393.56)</td>\n",
       "      <td>(analysis, 331.47)</td>\n",
       "      <td>(use, 254.09)</td>\n",
       "      <td>(article, 239.38)</td>\n",
       "      <td>(identify, 225.84)</td>\n",
       "      <td>(technology, 215.99)</td>\n",
       "      <td>(trend, 201.75)</td>\n",
       "      <td>(literature, 154.3)</td>\n",
       "      <td>(field, 150.61)</td>\n",
       "      <td>(text, 137.1)</td>\n",
       "      <td>(papers, 135.72)</td>\n",
       "      <td>(latent, 131.54)</td>\n",
       "      <td>(patent, 131.04)</td>\n",
       "      <td>(researcher, 130.56)</td>\n",
       "      <td>(scientific, 129.8)</td>\n",
       "      <td>(area, 125.64)</td>\n",
       "      <td>(dirichlet, 123.53)</td>\n",
       "      <td>(allocation, 122.98)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 10</th>\n",
       "      <td>(image, 923.95)</td>\n",
       "      <td>(feature, 404.79)</td>\n",
       "      <td>(object, 384.84)</td>\n",
       "      <td>(video, 298.9)</td>\n",
       "      <td>(visual, 296.7)</td>\n",
       "      <td>(scene, 291.52)</td>\n",
       "      <td>(propose, 268.69)</td>\n",
       "      <td>(method, 208.2)</td>\n",
       "      <td>(level, 190.16)</td>\n",
       "      <td>(model, 189.69)</td>\n",
       "      <td>(spatial, 171.52)</td>\n",
       "      <td>(region, 157.06)</td>\n",
       "      <td>(annotation, 151.42)</td>\n",
       "      <td>(classification, 148.49)</td>\n",
       "      <td>(human, 143.97)</td>\n",
       "      <td>(recognition, 138.61)</td>\n",
       "      <td>(result, 128.93)</td>\n",
       "      <td>(framework, 127.79)</td>\n",
       "      <td>(segmentation, 122.98)</td>\n",
       "      <td>(motion, 117.66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 11</th>\n",
       "      <td>(user, 1748.72)</td>\n",
       "      <td>(use, 1591.63)</td>\n",
       "      <td>(datum, 1381.07)</td>\n",
       "      <td>(topic, 1103.26)</td>\n",
       "      <td>(information, 1036.7)</td>\n",
       "      <td>(method, 971.08)</td>\n",
       "      <td>(latent, 941.61)</td>\n",
       "      <td>(propose, 858.59)</td>\n",
       "      <td>(allocation, 801.15)</td>\n",
       "      <td>(social, 798.08)</td>\n",
       "      <td>(approach, 753.49)</td>\n",
       "      <td>(dirichlet, 750.28)</td>\n",
       "      <td>(model, 704.64)</td>\n",
       "      <td>(result, 701.73)</td>\n",
       "      <td>(base, 692.76)</td>\n",
       "      <td>(network, 607.05)</td>\n",
       "      <td>(lda, 589.42)</td>\n",
       "      <td>(service, 569.67)</td>\n",
       "      <td>(paper, 561.59)</td>\n",
       "      <td>(time, 553.23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 12</th>\n",
       "      <td>(topic, 5025.38)</td>\n",
       "      <td>(model, 4893.76)</td>\n",
       "      <td>(lda, 2687.1)</td>\n",
       "      <td>(latent, 2152.96)</td>\n",
       "      <td>(document, 1992.37)</td>\n",
       "      <td>(use, 1943.68)</td>\n",
       "      <td>(method, 1839.26)</td>\n",
       "      <td>(word, 1773.7)</td>\n",
       "      <td>(dirichlet, 1564.05)</td>\n",
       "      <td>(propose, 1468.27)</td>\n",
       "      <td>(text, 1357.69)</td>\n",
       "      <td>(allocation, 1355.55)</td>\n",
       "      <td>(result, 1055.09)</td>\n",
       "      <td>(base, 997.56)</td>\n",
       "      <td>(approach, 918.45)</td>\n",
       "      <td>(paper, 905.81)</td>\n",
       "      <td>(algorithm, 897.52)</td>\n",
       "      <td>(datum, 891.62)</td>\n",
       "      <td>(feature, 814.18)</td>\n",
       "      <td>(information, 704.53)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 13</th>\n",
       "      <td>(hashtag, 94.97)</td>\n",
       "      <td>(membership, 54.98)</td>\n",
       "      <td>(chain, 51.44)</td>\n",
       "      <td>(routine, 49.23)</td>\n",
       "      <td>(functional, 38.29)</td>\n",
       "      <td>(carlo, 35.55)</td>\n",
       "      <td>(monte, 35.55)</td>\n",
       "      <td>(population, 30.26)</td>\n",
       "      <td>(incident, 26.4)</td>\n",
       "      <td>(trait, 24.96)</td>\n",
       "      <td>(signature, 23.26)</td>\n",
       "      <td>(poi, 23.24)</td>\n",
       "      <td>(ecosystem, 22.99)</td>\n",
       "      <td>(personality, 22.4)</td>\n",
       "      <td>(tissue, 21.34)</td>\n",
       "      <td>(hyperparameter, 21.08)</td>\n",
       "      <td>(composition, 20.08)</td>\n",
       "      <td>(zone, 19.08)</td>\n",
       "      <td>(site, 18.45)</td>\n",
       "      <td>(type, 17.93)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      Word 0               Word 1               Word 2  \\\n",
       "Topic 0        (gene, 123.1)        (drug, 96.87)  (expression, 78.19)   \n",
       "Topic 1    (patient, 223.06)    (medical, 143.69)      (health, 143.5)   \n",
       "Topic 2       (mirna, 42.64)  (permission, 25.17)       (quran, 13.82)   \n",
       "Topic 3    (emotion, 181.73)       (drive, 87.04)       (style, 86.19)   \n",
       "Topic 4     (sampling, 79.5)    (parallel, 73.15)       (gibbs, 67.68)   \n",
       "Topic 5     (review, 537.88)  (sentiment, 474.31)    (opinion, 265.32)   \n",
       "Topic 6        (song, 81.12)       (audio, 74.02)       (music, 57.13)   \n",
       "Topic 7   (software, 268.18)   (question, 203.92)        (code, 149.7)   \n",
       "Topic 8          (lm, 75.09)       (frame, 55.04)     (library, 31.56)   \n",
       "Topic 9      (topic, 893.39)   (research, 662.45)      (study, 393.56)   \n",
       "Topic 10     (image, 923.95)    (feature, 404.79)     (object, 384.84)   \n",
       "Topic 11     (user, 1748.72)       (use, 1591.63)     (datum, 1381.07)   \n",
       "Topic 12    (topic, 5025.38)     (model, 4893.76)        (lda, 2687.1)   \n",
       "Topic 13    (hashtag, 94.97)  (membership, 54.98)       (chain, 51.44)   \n",
       "\n",
       "                       Word 3                 Word 4                Word 5  \\\n",
       "Topic 0      (disease, 77.49)       (protein, 75.45)   (biomedical, 64.39)   \n",
       "Topic 1    (clinical, 139.72)     (treatment, 99.44)    (diagnosis, 86.78)   \n",
       "Topic 2          (holy, 9.76)        (aircraft, 8.0)    (provenance, 7.74)   \n",
       "Topic 3       (speech, 55.34)         (genre, 39.85)  (recognition, 38.09)   \n",
       "Topic 4     (security, 57.19)    (distribute, 52.59)     (collapse, 42.46)   \n",
       "Topic 5       (tweet, 235.55)      (product, 228.45)        (post, 206.48)   \n",
       "Topic 6           (pn, 35.01)     (broadcast, 28.77)          (oov, 24.22)   \n",
       "Topic 7   (developer, 143.61)       (source, 143.52)         (bug, 139.43)   \n",
       "Topic 8          (asr, 28.23)        (street, 22.76)         (ldta, 18.42)   \n",
       "Topic 9    (analysis, 331.47)          (use, 254.09)     (article, 239.38)   \n",
       "Topic 10       (video, 298.9)        (visual, 296.7)       (scene, 291.52)   \n",
       "Topic 11     (topic, 1103.26)  (information, 1036.7)      (method, 971.08)   \n",
       "Topic 12    (latent, 2152.96)    (document, 1992.37)        (use, 1943.68)   \n",
       "Topic 13     (routine, 49.23)    (functional, 38.29)        (carlo, 35.55)   \n",
       "\n",
       "                       Word 6                Word 7                 Word 8  \\\n",
       "Topic 0      (pathway, 51.34)         (cell, 51.19)      (identify, 48.51)   \n",
       "Topic 1      (disease, 81.07)          (risk, 79.0)        (record, 78.67)   \n",
       "Topic 2            (lv, 7.74)       (laplace, 7.05)            (ast, 6.96)   \n",
       "Topic 3      (speaker, 34.84)        (state, 33.33)         (write, 31.62)   \n",
       "Topic 4      (cluster, 33.26)           (bot, 32.4)          (brand, 31.0)   \n",
       "Topic 5      (online, 196.94)        (topic, 194.3)     (analysis, 180.66)   \n",
       "Topic 6   (indonesian, 19.81)      (musical, 18.49)         (lyric, 17.49)   \n",
       "Topic 7      (report, 120.26)      (project, 87.55)       (program, 68.98)   \n",
       "Topic 8          (adr, 18.41)          (rnn, 17.93)  (abbreviation, 15.75)   \n",
       "Topic 9    (identify, 225.84)  (technology, 215.99)        (trend, 201.75)   \n",
       "Topic 10    (propose, 268.69)       (method, 208.2)        (level, 190.16)   \n",
       "Topic 11     (latent, 941.61)     (propose, 858.59)   (allocation, 801.15)   \n",
       "Topic 12    (method, 1839.26)        (word, 1773.7)   (dirichlet, 1564.05)   \n",
       "Topic 13       (monte, 35.55)   (population, 30.26)       (incident, 26.4)   \n",
       "\n",
       "                       Word 9              Word 10                Word 11  \\\n",
       "Topic 0   (biological, 47.03)         (btm, 40.99)            (ad, 40.76)   \n",
       "Topic 1         (care, 68.06)      (friend, 47.64)    (healthcare, 42.31)   \n",
       "Topic 2         (chara, 6.65)         (yuru, 6.65)         (ldaclm, 6.22)   \n",
       "Topic 3     (language, 30.16)     (acoustic, 30.1)       (driving, 29.08)   \n",
       "Topic 4        (large, 30.54)       (crime, 29.71)         (scale, 29.68)   \n",
       "Topic 5     (twitter, 174.52)      (media, 171.56)       (aspect, 166.01)   \n",
       "Topic 6     (estimate, 16.39)      (writer, 14.97)    (commonness, 14.53)   \n",
       "Topic 7       (answer, 65.81)  (repository, 53.22)   (requirement, 49.86)   \n",
       "Topic 8      (adverse, 15.54)     (adapted, 15.29)           (lsm, 13.09)   \n",
       "Topic 9   (literature, 154.3)      (field, 150.61)          (text, 137.1)   \n",
       "Topic 10      (model, 189.69)    (spatial, 171.52)       (region, 157.06)   \n",
       "Topic 11     (social, 798.08)   (approach, 753.49)    (dirichlet, 750.28)   \n",
       "Topic 12   (propose, 1468.27)      (text, 1357.69)  (allocation, 1355.55)   \n",
       "Topic 13       (trait, 24.96)   (signature, 23.26)           (poi, 23.24)   \n",
       "\n",
       "                       Word 12                   Word 13  \\\n",
       "Topic 0         (study, 39.19)            (apply, 37.74)   \n",
       "Topic 1        (cancer, 39.36)             (code, 36.28)   \n",
       "Topic 2      (privilege, 5.86)              (mmrm, 5.86)   \n",
       "Topic 3        (driver, 27.25)            (essay, 26.57)   \n",
       "Topic 4         (token, 29.34)    (approximation, 28.55)   \n",
       "Topic 5     (customer, 159.65)          (rating, 142.65)   \n",
       "Topic 6        (submit, 13.59)    (hyperspectral, 11.69)   \n",
       "Topic 7      (evolution, 49.8)        (technique, 48.97)   \n",
       "Topic 8   (eligibility, 13.06)             (india, 12.3)   \n",
       "Topic 9       (papers, 135.72)          (latent, 131.54)   \n",
       "Topic 10  (annotation, 151.42)  (classification, 148.49)   \n",
       "Topic 11       (model, 704.64)          (result, 701.73)   \n",
       "Topic 12     (result, 1055.09)            (base, 997.56)   \n",
       "Topic 13    (ecosystem, 22.99)       (personality, 22.4)   \n",
       "\n",
       "                         Word 14                  Word 15  \\\n",
       "Topic 0   (manufacturing, 31.56)            (mrna, 29.42)   \n",
       "Topic 1            (data, 32.74)      (conclusion, 30.77)   \n",
       "Topic 2          (iexpand, 5.13)        (configure, 5.01)   \n",
       "Topic 3           (sleep, 24.96)         (vehicle, 24.49)   \n",
       "Topic 4          (memory, 27.44)             (gpu, 27.32)   \n",
       "Topic 5       (consumer, 111.75)          (social, 101.7)   \n",
       "Topic 6             (eem, 10.71)              (mp, 10.39)   \n",
       "Topic 7            (open, 44.34)           (study, 43.24)   \n",
       "Topic 8       (attraction, 11.6)        (salience, 11.02)   \n",
       "Topic 9         (patent, 131.04)     (researcher, 130.56)   \n",
       "Topic 10         (human, 143.97)    (recognition, 138.61)   \n",
       "Topic 11          (base, 692.76)        (network, 607.05)   \n",
       "Topic 12      (approach, 918.45)          (paper, 905.81)   \n",
       "Topic 13         (tissue, 21.34)  (hyperparameter, 21.08)   \n",
       "\n",
       "                         Word 16              Word 17                 Word 18  \\\n",
       "Topic 0    (relationship, 29.31)  (conclusion, 28.59)           (tool, 26.52)   \n",
       "Topic 1       (physician, 29.99)         (emr, 25.48)            (ehr, 25.13)   \n",
       "Topic 2   (transcriptional, 5.0)         (pcfg, 4.14)            (tctm, 4.12)   \n",
       "Topic 3      (authorship, 22.67)        (noca, 22.09)         (author, 21.59)   \n",
       "Topic 4         (sampler, 26.77)      (matrix, 23.32)         (atomic, 23.12)   \n",
       "Topic 5            (use, 101.33)    (negative, 88.63)       (positive, 85.19)   \n",
       "Topic 6          (isoform, 9.79)     (retrieve, 9.73)       (replicate, 9.62)   \n",
       "Topic 7             (lsi, 40.78)       (stack, 33.35)         (change, 30.56)   \n",
       "Topic 8            (wall, 10.98)       (grams, 10.91)        (journal, 10.27)   \n",
       "Topic 9      (scientific, 129.8)       (area, 125.64)     (dirichlet, 123.53)   \n",
       "Topic 10        (result, 128.93)  (framework, 127.79)  (segmentation, 122.98)   \n",
       "Topic 11           (lda, 589.42)    (service, 569.67)         (paper, 561.59)   \n",
       "Topic 12     (algorithm, 897.52)      (datum, 891.62)       (feature, 814.18)   \n",
       "Topic 13    (composition, 20.08)        (zone, 19.08)           (site, 18.45)   \n",
       "\n",
       "                        Word 19  \n",
       "Topic 0        (related, 25.93)  \n",
       "Topic 1         (doctor, 24.57)  \n",
       "Topic 2          (seqlda, 4.12)  \n",
       "Topic 3       (disorder, 21.36)  \n",
       "Topic 4             (mh, 22.88)  \n",
       "Topic 5          (study, 81.34)  \n",
       "Topic 6          (timbre, 8.87)  \n",
       "Topic 7            (nfr, 30.05)  \n",
       "Topic 8           (dntm, 10.12)  \n",
       "Topic 9    (allocation, 122.98)  \n",
       "Topic 10       (motion, 117.66)  \n",
       "Topic 11         (time, 553.23)  \n",
       "Topic 12  (information, 704.53)  \n",
       "Topic 13          (type, 17.93)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"top terms info:\")\n",
    "df_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top docs info:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>(198, 0.548)</td>\n",
       "      <td>(263, 0.493)</td>\n",
       "      <td>(107, 0.407)</td>\n",
       "      <td>(1764, 0.386)</td>\n",
       "      <td>(2009, 0.383)</td>\n",
       "      <td>(1770, 0.371)</td>\n",
       "      <td>(1740, 0.361)</td>\n",
       "      <td>(894, 0.346)</td>\n",
       "      <td>(896, 0.334)</td>\n",
       "      <td>(65, 0.32)</td>\n",
       "      <td>(751, 0.318)</td>\n",
       "      <td>(725, 0.318)</td>\n",
       "      <td>(122, 0.314)</td>\n",
       "      <td>(52, 0.285)</td>\n",
       "      <td>(2577, 0.271)</td>\n",
       "      <td>(562, 0.264)</td>\n",
       "      <td>(526, 0.263)</td>\n",
       "      <td>(50, 0.257)</td>\n",
       "      <td>(685, 0.247)</td>\n",
       "      <td>(2248, 0.245)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>(489, 0.751)</td>\n",
       "      <td>(142, 0.651)</td>\n",
       "      <td>(115, 0.587)</td>\n",
       "      <td>(722, 0.42)</td>\n",
       "      <td>(1566, 0.37)</td>\n",
       "      <td>(1101, 0.364)</td>\n",
       "      <td>(1112, 0.346)</td>\n",
       "      <td>(80, 0.346)</td>\n",
       "      <td>(32, 0.345)</td>\n",
       "      <td>(627, 0.339)</td>\n",
       "      <td>(415, 0.319)</td>\n",
       "      <td>(2304, 0.319)</td>\n",
       "      <td>(1037, 0.313)</td>\n",
       "      <td>(1446, 0.291)</td>\n",
       "      <td>(1402, 0.291)</td>\n",
       "      <td>(1129, 0.281)</td>\n",
       "      <td>(174, 0.277)</td>\n",
       "      <td>(64, 0.276)</td>\n",
       "      <td>(338, 0.275)</td>\n",
       "      <td>(1624, 0.266)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>(1855, 0.23)</td>\n",
       "      <td>(1319, 0.208)</td>\n",
       "      <td>(1400, 0.199)</td>\n",
       "      <td>(872, 0.198)</td>\n",
       "      <td>(1309, 0.196)</td>\n",
       "      <td>(1079, 0.187)</td>\n",
       "      <td>(1340, 0.151)</td>\n",
       "      <td>(1310, 0.146)</td>\n",
       "      <td>(1164, 0.133)</td>\n",
       "      <td>(1350, 0.131)</td>\n",
       "      <td>(2158, 0.115)</td>\n",
       "      <td>(2386, 0.114)</td>\n",
       "      <td>(2188, 0.112)</td>\n",
       "      <td>(1242, 0.11)</td>\n",
       "      <td>(2220, 0.107)</td>\n",
       "      <td>(1868, 0.105)</td>\n",
       "      <td>(1932, 0.104)</td>\n",
       "      <td>(2422, 0.102)</td>\n",
       "      <td>(2457, 0.075)</td>\n",
       "      <td>(1876, 0.074)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>(81, 0.685)</td>\n",
       "      <td>(874, 0.517)</td>\n",
       "      <td>(109, 0.374)</td>\n",
       "      <td>(771, 0.349)</td>\n",
       "      <td>(18, 0.338)</td>\n",
       "      <td>(195, 0.293)</td>\n",
       "      <td>(1178, 0.287)</td>\n",
       "      <td>(344, 0.285)</td>\n",
       "      <td>(1281, 0.265)</td>\n",
       "      <td>(1813, 0.255)</td>\n",
       "      <td>(171, 0.254)</td>\n",
       "      <td>(1422, 0.254)</td>\n",
       "      <td>(1761, 0.244)</td>\n",
       "      <td>(1306, 0.24)</td>\n",
       "      <td>(1266, 0.238)</td>\n",
       "      <td>(737, 0.233)</td>\n",
       "      <td>(1278, 0.228)</td>\n",
       "      <td>(113, 0.222)</td>\n",
       "      <td>(1175, 0.22)</td>\n",
       "      <td>(365, 0.216)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>(508, 0.371)</td>\n",
       "      <td>(461, 0.346)</td>\n",
       "      <td>(413, 0.333)</td>\n",
       "      <td>(98, 0.282)</td>\n",
       "      <td>(1076, 0.27)</td>\n",
       "      <td>(720, 0.268)</td>\n",
       "      <td>(740, 0.268)</td>\n",
       "      <td>(741, 0.268)</td>\n",
       "      <td>(1666, 0.252)</td>\n",
       "      <td>(606, 0.248)</td>\n",
       "      <td>(693, 0.247)</td>\n",
       "      <td>(2027, 0.247)</td>\n",
       "      <td>(1379, 0.246)</td>\n",
       "      <td>(82, 0.241)</td>\n",
       "      <td>(2447, 0.231)</td>\n",
       "      <td>(886, 0.226)</td>\n",
       "      <td>(89, 0.225)</td>\n",
       "      <td>(1273, 0.217)</td>\n",
       "      <td>(450, 0.215)</td>\n",
       "      <td>(56, 0.206)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 5</th>\n",
       "      <td>(85, 0.951)</td>\n",
       "      <td>(57, 0.727)</td>\n",
       "      <td>(386, 0.669)</td>\n",
       "      <td>(111, 0.651)</td>\n",
       "      <td>(1, 0.631)</td>\n",
       "      <td>(575, 0.57)</td>\n",
       "      <td>(434, 0.518)</td>\n",
       "      <td>(698, 0.513)</td>\n",
       "      <td>(1577, 0.493)</td>\n",
       "      <td>(786, 0.486)</td>\n",
       "      <td>(103, 0.48)</td>\n",
       "      <td>(59, 0.471)</td>\n",
       "      <td>(6, 0.466)</td>\n",
       "      <td>(270, 0.455)</td>\n",
       "      <td>(957, 0.454)</td>\n",
       "      <td>(2272, 0.452)</td>\n",
       "      <td>(67, 0.452)</td>\n",
       "      <td>(973, 0.451)</td>\n",
       "      <td>(410, 0.435)</td>\n",
       "      <td>(1936, 0.42)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 6</th>\n",
       "      <td>(1570, 0.465)</td>\n",
       "      <td>(1117, 0.415)</td>\n",
       "      <td>(1606, 0.337)</td>\n",
       "      <td>(477, 0.302)</td>\n",
       "      <td>(757, 0.276)</td>\n",
       "      <td>(970, 0.254)</td>\n",
       "      <td>(1874, 0.252)</td>\n",
       "      <td>(1167, 0.169)</td>\n",
       "      <td>(1520, 0.166)</td>\n",
       "      <td>(684, 0.162)</td>\n",
       "      <td>(1668, 0.15)</td>\n",
       "      <td>(854, 0.147)</td>\n",
       "      <td>(2526, 0.144)</td>\n",
       "      <td>(702, 0.141)</td>\n",
       "      <td>(581, 0.141)</td>\n",
       "      <td>(1021, 0.131)</td>\n",
       "      <td>(2325, 0.126)</td>\n",
       "      <td>(2396, 0.125)</td>\n",
       "      <td>(1720, 0.124)</td>\n",
       "      <td>(995, 0.12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 7</th>\n",
       "      <td>(1586, 0.575)</td>\n",
       "      <td>(747, 0.526)</td>\n",
       "      <td>(224, 0.503)</td>\n",
       "      <td>(831, 0.447)</td>\n",
       "      <td>(453, 0.428)</td>\n",
       "      <td>(898, 0.427)</td>\n",
       "      <td>(20, 0.397)</td>\n",
       "      <td>(1594, 0.396)</td>\n",
       "      <td>(731, 0.392)</td>\n",
       "      <td>(2283, 0.386)</td>\n",
       "      <td>(380, 0.385)</td>\n",
       "      <td>(1473, 0.382)</td>\n",
       "      <td>(891, 0.379)</td>\n",
       "      <td>(1181, 0.377)</td>\n",
       "      <td>(2312, 0.376)</td>\n",
       "      <td>(2533, 0.375)</td>\n",
       "      <td>(945, 0.371)</td>\n",
       "      <td>(2467, 0.368)</td>\n",
       "      <td>(752, 0.357)</td>\n",
       "      <td>(141, 0.357)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 8</th>\n",
       "      <td>(441, 0.215)</td>\n",
       "      <td>(1958, 0.173)</td>\n",
       "      <td>(890, 0.172)</td>\n",
       "      <td>(626, 0.172)</td>\n",
       "      <td>(2121, 0.167)</td>\n",
       "      <td>(2184, 0.165)</td>\n",
       "      <td>(70, 0.161)</td>\n",
       "      <td>(802, 0.16)</td>\n",
       "      <td>(1442, 0.157)</td>\n",
       "      <td>(273, 0.152)</td>\n",
       "      <td>(2561, 0.148)</td>\n",
       "      <td>(1343, 0.147)</td>\n",
       "      <td>(1440, 0.137)</td>\n",
       "      <td>(576, 0.136)</td>\n",
       "      <td>(2505, 0.129)</td>\n",
       "      <td>(2233, 0.125)</td>\n",
       "      <td>(1882, 0.123)</td>\n",
       "      <td>(849, 0.114)</td>\n",
       "      <td>(2522, 0.113)</td>\n",
       "      <td>(621, 0.108)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 9</th>\n",
       "      <td>(2, 0.994)</td>\n",
       "      <td>(41, 0.9)</td>\n",
       "      <td>(1044, 0.871)</td>\n",
       "      <td>(317, 0.861)</td>\n",
       "      <td>(1013, 0.836)</td>\n",
       "      <td>(97, 0.813)</td>\n",
       "      <td>(996, 0.8)</td>\n",
       "      <td>(47, 0.798)</td>\n",
       "      <td>(1475, 0.772)</td>\n",
       "      <td>(335, 0.772)</td>\n",
       "      <td>(1305, 0.772)</td>\n",
       "      <td>(388, 0.765)</td>\n",
       "      <td>(308, 0.755)</td>\n",
       "      <td>(120, 0.752)</td>\n",
       "      <td>(546, 0.741)</td>\n",
       "      <td>(51, 0.737)</td>\n",
       "      <td>(8, 0.727)</td>\n",
       "      <td>(617, 0.724)</td>\n",
       "      <td>(264, 0.716)</td>\n",
       "      <td>(564, 0.712)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 10</th>\n",
       "      <td>(745, 0.69)</td>\n",
       "      <td>(399, 0.664)</td>\n",
       "      <td>(291, 0.662)</td>\n",
       "      <td>(2580, 0.654)</td>\n",
       "      <td>(2520, 0.652)</td>\n",
       "      <td>(1111, 0.645)</td>\n",
       "      <td>(1224, 0.645)</td>\n",
       "      <td>(284, 0.642)</td>\n",
       "      <td>(760, 0.638)</td>\n",
       "      <td>(695, 0.633)</td>\n",
       "      <td>(1694, 0.626)</td>\n",
       "      <td>(1410, 0.624)</td>\n",
       "      <td>(1222, 0.621)</td>\n",
       "      <td>(1900, 0.618)</td>\n",
       "      <td>(374, 0.612)</td>\n",
       "      <td>(2339, 0.605)</td>\n",
       "      <td>(662, 0.604)</td>\n",
       "      <td>(2363, 0.598)</td>\n",
       "      <td>(1550, 0.589)</td>\n",
       "      <td>(538, 0.582)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 11</th>\n",
       "      <td>(14, 0.994)</td>\n",
       "      <td>(149, 0.993)</td>\n",
       "      <td>(529, 0.993)</td>\n",
       "      <td>(619, 0.992)</td>\n",
       "      <td>(392, 0.991)</td>\n",
       "      <td>(2018, 0.989)</td>\n",
       "      <td>(1526, 0.989)</td>\n",
       "      <td>(1997, 0.988)</td>\n",
       "      <td>(162, 0.988)</td>\n",
       "      <td>(1549, 0.987)</td>\n",
       "      <td>(1180, 0.987)</td>\n",
       "      <td>(2326, 0.987)</td>\n",
       "      <td>(974, 0.987)</td>\n",
       "      <td>(2503, 0.986)</td>\n",
       "      <td>(206, 0.985)</td>\n",
       "      <td>(589, 0.985)</td>\n",
       "      <td>(485, 0.983)</td>\n",
       "      <td>(1741, 0.982)</td>\n",
       "      <td>(1862, 0.982)</td>\n",
       "      <td>(1703, 0.98)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 12</th>\n",
       "      <td>(966, 0.992)</td>\n",
       "      <td>(739, 0.991)</td>\n",
       "      <td>(1033, 0.991)</td>\n",
       "      <td>(1075, 0.991)</td>\n",
       "      <td>(2306, 0.991)</td>\n",
       "      <td>(354, 0.991)</td>\n",
       "      <td>(211, 0.991)</td>\n",
       "      <td>(870, 0.991)</td>\n",
       "      <td>(1071, 0.991)</td>\n",
       "      <td>(2168, 0.991)</td>\n",
       "      <td>(1408, 0.99)</td>\n",
       "      <td>(173, 0.99)</td>\n",
       "      <td>(1788, 0.99)</td>\n",
       "      <td>(1704, 0.99)</td>\n",
       "      <td>(1828, 0.99)</td>\n",
       "      <td>(2543, 0.99)</td>\n",
       "      <td>(422, 0.99)</td>\n",
       "      <td>(824, 0.99)</td>\n",
       "      <td>(390, 0.989)</td>\n",
       "      <td>(2477, 0.989)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 13</th>\n",
       "      <td>(53, 0.618)</td>\n",
       "      <td>(377, 0.343)</td>\n",
       "      <td>(158, 0.312)</td>\n",
       "      <td>(1762, 0.288)</td>\n",
       "      <td>(55, 0.285)</td>\n",
       "      <td>(356, 0.284)</td>\n",
       "      <td>(1744, 0.278)</td>\n",
       "      <td>(1499, 0.255)</td>\n",
       "      <td>(288, 0.225)</td>\n",
       "      <td>(342, 0.222)</td>\n",
       "      <td>(665, 0.221)</td>\n",
       "      <td>(75, 0.206)</td>\n",
       "      <td>(1045, 0.199)</td>\n",
       "      <td>(68, 0.194)</td>\n",
       "      <td>(1003, 0.189)</td>\n",
       "      <td>(777, 0.189)</td>\n",
       "      <td>(269, 0.186)</td>\n",
       "      <td>(1657, 0.164)</td>\n",
       "      <td>(601, 0.164)</td>\n",
       "      <td>(562, 0.162)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0              1              2              3  \\\n",
       "Topic 0    (198, 0.548)   (263, 0.493)   (107, 0.407)  (1764, 0.386)   \n",
       "Topic 1    (489, 0.751)   (142, 0.651)   (115, 0.587)    (722, 0.42)   \n",
       "Topic 2    (1855, 0.23)  (1319, 0.208)  (1400, 0.199)   (872, 0.198)   \n",
       "Topic 3     (81, 0.685)   (874, 0.517)   (109, 0.374)   (771, 0.349)   \n",
       "Topic 4    (508, 0.371)   (461, 0.346)   (413, 0.333)    (98, 0.282)   \n",
       "Topic 5     (85, 0.951)    (57, 0.727)   (386, 0.669)   (111, 0.651)   \n",
       "Topic 6   (1570, 0.465)  (1117, 0.415)  (1606, 0.337)   (477, 0.302)   \n",
       "Topic 7   (1586, 0.575)   (747, 0.526)   (224, 0.503)   (831, 0.447)   \n",
       "Topic 8    (441, 0.215)  (1958, 0.173)   (890, 0.172)   (626, 0.172)   \n",
       "Topic 9      (2, 0.994)      (41, 0.9)  (1044, 0.871)   (317, 0.861)   \n",
       "Topic 10    (745, 0.69)   (399, 0.664)   (291, 0.662)  (2580, 0.654)   \n",
       "Topic 11    (14, 0.994)   (149, 0.993)   (529, 0.993)   (619, 0.992)   \n",
       "Topic 12   (966, 0.992)   (739, 0.991)  (1033, 0.991)  (1075, 0.991)   \n",
       "Topic 13    (53, 0.618)   (377, 0.343)   (158, 0.312)  (1762, 0.288)   \n",
       "\n",
       "                      4              5              6              7  \\\n",
       "Topic 0   (2009, 0.383)  (1770, 0.371)  (1740, 0.361)   (894, 0.346)   \n",
       "Topic 1    (1566, 0.37)  (1101, 0.364)  (1112, 0.346)    (80, 0.346)   \n",
       "Topic 2   (1309, 0.196)  (1079, 0.187)  (1340, 0.151)  (1310, 0.146)   \n",
       "Topic 3     (18, 0.338)   (195, 0.293)  (1178, 0.287)   (344, 0.285)   \n",
       "Topic 4    (1076, 0.27)   (720, 0.268)   (740, 0.268)   (741, 0.268)   \n",
       "Topic 5      (1, 0.631)    (575, 0.57)   (434, 0.518)   (698, 0.513)   \n",
       "Topic 6    (757, 0.276)   (970, 0.254)  (1874, 0.252)  (1167, 0.169)   \n",
       "Topic 7    (453, 0.428)   (898, 0.427)    (20, 0.397)  (1594, 0.396)   \n",
       "Topic 8   (2121, 0.167)  (2184, 0.165)    (70, 0.161)    (802, 0.16)   \n",
       "Topic 9   (1013, 0.836)    (97, 0.813)     (996, 0.8)    (47, 0.798)   \n",
       "Topic 10  (2520, 0.652)  (1111, 0.645)  (1224, 0.645)   (284, 0.642)   \n",
       "Topic 11   (392, 0.991)  (2018, 0.989)  (1526, 0.989)  (1997, 0.988)   \n",
       "Topic 12  (2306, 0.991)   (354, 0.991)   (211, 0.991)   (870, 0.991)   \n",
       "Topic 13    (55, 0.285)   (356, 0.284)  (1744, 0.278)  (1499, 0.255)   \n",
       "\n",
       "                      8              9             10             11  \\\n",
       "Topic 0    (896, 0.334)     (65, 0.32)   (751, 0.318)   (725, 0.318)   \n",
       "Topic 1     (32, 0.345)   (627, 0.339)   (415, 0.319)  (2304, 0.319)   \n",
       "Topic 2   (1164, 0.133)  (1350, 0.131)  (2158, 0.115)  (2386, 0.114)   \n",
       "Topic 3   (1281, 0.265)  (1813, 0.255)   (171, 0.254)  (1422, 0.254)   \n",
       "Topic 4   (1666, 0.252)   (606, 0.248)   (693, 0.247)  (2027, 0.247)   \n",
       "Topic 5   (1577, 0.493)   (786, 0.486)    (103, 0.48)    (59, 0.471)   \n",
       "Topic 6   (1520, 0.166)   (684, 0.162)   (1668, 0.15)   (854, 0.147)   \n",
       "Topic 7    (731, 0.392)  (2283, 0.386)   (380, 0.385)  (1473, 0.382)   \n",
       "Topic 8   (1442, 0.157)   (273, 0.152)  (2561, 0.148)  (1343, 0.147)   \n",
       "Topic 9   (1475, 0.772)   (335, 0.772)  (1305, 0.772)   (388, 0.765)   \n",
       "Topic 10   (760, 0.638)   (695, 0.633)  (1694, 0.626)  (1410, 0.624)   \n",
       "Topic 11   (162, 0.988)  (1549, 0.987)  (1180, 0.987)  (2326, 0.987)   \n",
       "Topic 12  (1071, 0.991)  (2168, 0.991)   (1408, 0.99)    (173, 0.99)   \n",
       "Topic 13   (288, 0.225)   (342, 0.222)   (665, 0.221)    (75, 0.206)   \n",
       "\n",
       "                     12             13             14             15  \\\n",
       "Topic 0    (122, 0.314)    (52, 0.285)  (2577, 0.271)   (562, 0.264)   \n",
       "Topic 1   (1037, 0.313)  (1446, 0.291)  (1402, 0.291)  (1129, 0.281)   \n",
       "Topic 2   (2188, 0.112)   (1242, 0.11)  (2220, 0.107)  (1868, 0.105)   \n",
       "Topic 3   (1761, 0.244)   (1306, 0.24)  (1266, 0.238)   (737, 0.233)   \n",
       "Topic 4   (1379, 0.246)    (82, 0.241)  (2447, 0.231)   (886, 0.226)   \n",
       "Topic 5      (6, 0.466)   (270, 0.455)   (957, 0.454)  (2272, 0.452)   \n",
       "Topic 6   (2526, 0.144)   (702, 0.141)   (581, 0.141)  (1021, 0.131)   \n",
       "Topic 7    (891, 0.379)  (1181, 0.377)  (2312, 0.376)  (2533, 0.375)   \n",
       "Topic 8   (1440, 0.137)   (576, 0.136)  (2505, 0.129)  (2233, 0.125)   \n",
       "Topic 9    (308, 0.755)   (120, 0.752)   (546, 0.741)    (51, 0.737)   \n",
       "Topic 10  (1222, 0.621)  (1900, 0.618)   (374, 0.612)  (2339, 0.605)   \n",
       "Topic 11   (974, 0.987)  (2503, 0.986)   (206, 0.985)   (589, 0.985)   \n",
       "Topic 12   (1788, 0.99)   (1704, 0.99)   (1828, 0.99)   (2543, 0.99)   \n",
       "Topic 13  (1045, 0.199)    (68, 0.194)  (1003, 0.189)   (777, 0.189)   \n",
       "\n",
       "                     16             17             18             19  \n",
       "Topic 0    (526, 0.263)    (50, 0.257)   (685, 0.247)  (2248, 0.245)  \n",
       "Topic 1    (174, 0.277)    (64, 0.276)   (338, 0.275)  (1624, 0.266)  \n",
       "Topic 2   (1932, 0.104)  (2422, 0.102)  (2457, 0.075)  (1876, 0.074)  \n",
       "Topic 3   (1278, 0.228)   (113, 0.222)   (1175, 0.22)   (365, 0.216)  \n",
       "Topic 4     (89, 0.225)  (1273, 0.217)   (450, 0.215)    (56, 0.206)  \n",
       "Topic 5     (67, 0.452)   (973, 0.451)   (410, 0.435)   (1936, 0.42)  \n",
       "Topic 6   (2325, 0.126)  (2396, 0.125)  (1720, 0.124)    (995, 0.12)  \n",
       "Topic 7    (945, 0.371)  (2467, 0.368)   (752, 0.357)   (141, 0.357)  \n",
       "Topic 8   (1882, 0.123)   (849, 0.114)  (2522, 0.113)   (621, 0.108)  \n",
       "Topic 9      (8, 0.727)   (617, 0.724)   (264, 0.716)   (564, 0.712)  \n",
       "Topic 10   (662, 0.604)  (2363, 0.598)  (1550, 0.589)   (538, 0.582)  \n",
       "Topic 11   (485, 0.983)  (1741, 0.982)  (1862, 0.982)   (1703, 0.98)  \n",
       "Topic 12    (422, 0.99)    (824, 0.99)   (390, 0.989)  (2477, 0.989)  \n",
       "Topic 13   (269, 0.186)  (1657, 0.164)   (601, 0.164)   (562, 0.162)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"top docs info:\")\n",
    "df_top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis.pyLDA(topic_word, doc_topic, [len(s) for s in [word_tokenize(corp) for corp in _input]], vec.get_feature_names(), np.array(sum(tf).todense())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.07 2 489 5\n",
      "lda show that , prior to surgery , patient ' priority be primarily in cancer surgery and recovery .\n",
      "LDA showed that, prior to surgery, patients' priorities were primarily in cancer surgery and recovery.\n",
      "[(0,), (1,), None]\n",
      "['priorities']-['were']-['cancer', 'surgery', 'and', 'recovery']\n",
      "[(0,), (17,), None]\n",
      "========================================\n",
      "1.07 2 489 4\n",
      "we aim to apply LDA to interview datum collect as part of a prospective , longitudinal study of qol in patient undergo radical cystectomy and urinary diversion for bladder cancer .\n",
      "We aim to apply LDA to interview data collected as part of a prospective, longitudinal study of QOL in patients undergoing radical cystectomy and urinary diversion for bladder cancer.\n",
      "[(0,), (1,), None]\n",
      "['We']-['apply']-['LDA']\n",
      "['We']-['interview']-['data']\n",
      "========================================\n",
      "1.0 1 489 3\n",
      "latent Dirichlet allocation -lrb- lda -rrb- may offer statistical rigor in summarize patient ' concern and cope strategy in a life-threatening illness .\n",
      "Latent Dirichlet Allocation (LDA) may offer statistical rigor in summarizing patients' concerns and coping strategies in a life-threatening illness.\n",
      "['Latent', 'Dirichlet', 'Allocation']-['offer']-['statistical', 'rigor']\n",
      "['Latent', 'Dirichlet', 'Allocation']-['summarizing']-['concerns']\n",
      "['Latent', 'Dirichlet', 'Allocation']-['coping']-['life-threatening', 'illness']\n",
      "========================================\n",
      "0.82 2 489 0\n",
      "as we begin to leverage Big Data in health care setting and particularly in assess patient-reported outcome , there be a need for novel analytic to address unique challenge .\n",
      "As we begin to leverage Big Data in health care settings and particularly in assessing patient-reported outcomes, there is a need for novel analytics to address unique challenges.\n",
      "[(21, 22, 24), (19,), None]\n",
      "[(1,), (2,), None]\n",
      "['we']-['leverage']-['Big', 'Data']\n",
      "['we']-['leverage']-['health', 'care', 'settings', 'and', 'assessing']\n",
      "========================================\n",
      "0.06 1 489 7\n",
      "novel analytic such as lda offer the possibility of summarize personal goal in real time without the need for conventional fixed-length measure and qualitative datum code .\n",
      "Novel analytics such as LDA offer the possibility of summarizing personal goals in real time without the need for conventional fixed-length measures and qualitative data coding.\n",
      "['Novel', 'analytics']-['offer']-['possibility']\n",
      "========================================\n",
      "0.06 1 489 1\n",
      "one such challenge be in code transcribe interview datum , typically free-text entry of statement make by interviewee during face-to-face interview .\n",
      "One such challenge is in coding transcribed interview data, typically free-text entries of statements made by interviewees during face-to-face interviews.\n",
      "['such', 'challenge']-['is']-['transcribed', 'interview', 'data']\n",
      "========================================\n",
      "1.07 2 115 7\n",
      "prior to surgery , patient ' priority be primarily in cancer surgery and recovery .\n",
      "Prior to surgery, patients' priorities were primarily in cancer surgery and recovery.\n",
      "['priorities']-['were']-['cancer', 'surgery', 'and', 'recovery']\n",
      "========================================\n",
      "1.07 2 115 3\n",
      "method LDA be apply to interview datum collect as part of a prospective , longitudinal study of qol in n = 211 patient undergo radical cystectomy and urinary diversion for bladder cancer .\n",
      "Methods LDA was applied to interview data collected as part of a prospective, longitudinal study of QOL in N = 211 patients undergoing radical cystectomy and urinary diversion for bladder cancer.\n",
      "[(0, 1), (3,), None]\n",
      "['Methods', 'LDA']-['interview']-['data']\n",
      "========================================\n",
      "1.0 1 115 4\n",
      "lda analyze personal goal statement to extract the latent topic and theme , stratify by time , and on thing patient want to accomplish and prevent .\n",
      "LDA analyzed personal goal statements to extract the latent topics and themes, stratified by time, and on things patients wanted to accomplish and prevent.\n",
      "['LDA']-['analyzed']-['personal', 'goal', 'statements', 'and', 'patients']\n",
      "['LDA']-['analyzed']-['things', 'patients']\n",
      "========================================\n",
      "1.0 1 115 2\n",
      "latent Dirichlet allocation -lrb- lda -rrb- offer statistical rigor and consistency in automate the interpretation of patient ' express concern and cope strategy .\n",
      "Latent Dirichlet Allocation (LDA) offers statistical rigor and consistency in automating the interpretation of patients' expressed concerns and coping strategies.\n",
      "['Latent', 'Dirichlet', 'Allocation']-['offers']-['statistical', 'rigor', 'and', 'consistency']\n",
      "['Latent', 'Dirichlet', 'Allocation']-['automating']-['interpretation']\n",
      "['Latent', 'Dirichlet', 'Allocation']-['coping']-['strategies']\n",
      "========================================\n",
      "0.82 2 115 0\n",
      "purpose as we begin to leverage Big Data in health care setting and particularly in assess patient-reported outcome , there be a need for novel analytic to address unique challenge .\n",
      "Purpose As we begin to leverage Big Data in health care settings and particularly in assessing patient-reported outcomes, there is a need for novel analytics to address unique challenges.\n",
      "[(22, 23, 25), (20,), None]\n",
      "[(22, 23, 25), (0,), None]\n",
      "[(2,), (3,), None]\n",
      "['we']-['leverage']-['Big', 'Data']\n",
      "['we']-['leverage']-['health', 'care', 'settings', 'and', 'assessing']\n",
      "========================================\n",
      "0.12 1 115 8\n",
      "six month after the surgery , they be replace by goal on regain a sense of normalcy , to resume work , to enjoy life more fully , and to appreciate friend and family more .\n",
      "Six months after the surgery, they were replaced by goals on regaining a sense of normalcy, to resume work, to enjoy life more fully, and to appreciate friends and family more.\n",
      "[(6,), (8,), None]\n",
      "['they']-['resume']-['work']\n",
      "['they']-['enjoy']-['life']\n",
      "['they']-['appreciate']-['friends', 'and', 'family']\n",
      "========================================\n",
      "0.09 2 115 10\n",
      "conclusion novel Big Data analytic such as lda offer the possibility of summarize personal goal without the need for conventional fixed-length measure and resource-intensive qualitative datum code .\n",
      "Conclusions Novel Big Data analytics such as LDA offer the possibility of summarizing personal goals without the need for conventional fixed-length measures and resource-intensive qualitative data coding.\n",
      "['Conclusions', 'Novel', 'Big', 'Data', 'analytics']-['offer']-['possibility']\n",
      "========================================\n",
      "0.07 1 115 9\n",
      "LDA model parameter show change priority , e.g. , immediate concern on surgery and resume employment decrease post-surgery and be replace by concern over cancer recurrence and a desire to remain healthy and strong .\n",
      "LDA model parameters showed changing priorities, e.g., immediate concerns on surgery and resuming employment decreased post-surgery and were replaced by concerns over cancer recurrence and a desire to remain healthy and strong.\n",
      "[(0, 1, 2), (3,), None]\n",
      "['LDA', 'model', 'parameters']-['decreased']-['post-surgery']\n",
      "[(0, 1, 2), (20,), None]\n",
      "========================================\n",
      "0.06 1 115 1\n",
      "one such challenge be in code transcribe interview datum , typically free-text entry of statement make during a face-to-face interview .\n",
      "One such challenge is in coding transcribed interview data, typically free-text entries of statements made during a face-to-face interview.\n",
      "['such', 'challenge']-['is']-['transcribed', 'interview', 'data']\n",
      "========================================\n",
      "1.58 2 722 6\n",
      "a total of 33.3 % -lrb- 12/36 -rrb- patient positive for hpv-16 have cervical intraepithelial neoplasia -lrb- cin -rrb- 2 or a worse result , which be significantly higher than the prevalence of cin2 of 1.8 % -lrb- 8/455 -rrb- in patient negative for hpv-16 -lrb- p < 0.001 -rrb- , while no significant association be identify for other genotype in term of genotype and clinical progress .\n",
      "A total of 33.3% (12/36) patients positive for HPV-16 had cervical intraepithelial neoplasia (CIN) 2 or a worse result, which was significantly higher than the prevalence of CIN2 of 1.8% (8/455) in patients negative for HPV-16 (P< 0.001), while no significant association was identified for other genotypes in terms of genotype and clinical progress.\n",
      "['%']-['had']-['cervical', 'intraepithelial', 'neoplasia', 'or', 'result']\n",
      "['%']-['had']-['worse', 'result']\n",
      "['significant', 'association']-['identified', 'for']-['other', 'genotypes']\n",
      "['significant', 'association']-['identified', 'for']-['terms', 'of', 'genotype']\n",
      "========================================\n",
      "1.0 1 722 12\n",
      "therefore , lda result may be present as explanatory evidence during time-constrained patient-doctor consultation in order to deliver information regard the patient 's status .\n",
      "Therefore, LDA results may be presented as explanatory evidence during time-constrained patient-doctor consultations in order to deliver information regarding the patient's status.\n",
      "['LDA', 'results']-['presented', 'as']-['explanatory', 'evidence']\n",
      "========================================\n",
      "1.0 1 722 8\n",
      "persistent infection be higher in patient aged > = 51 year -lrb- 38.7 % -rrb- than in those aged < = 50 year -lrb- 20.4 % ; p = 0.036 -rrb- .\n",
      "Persistent infection was higher in patients aged >= 51 years (38.7%) than in those aged <= 50 years (20.4%; P= 0.036).\n",
      "['Persistent', 'infection']-['was']-['higher', 'in', 'patients']\n",
      "========================================\n",
      "1.0 1 722 3\n",
      "patient undergo PAP and HPV DNA chip test between January 2006 and January 2009 .\n",
      "Patients underwent PAP and HPV DNA chip tests between January 2006 and January 2009.\n",
      "['Patients']-['underwent']-['PAP', 'HPV', 'DNA', 'chip', 'tests']\n",
      "========================================\n",
      "1.0 1 722 2\n",
      "the present study assess 491 patient -lrb- 139 hpv-positive and 352 hpv-negative case -rrb- with a PAP test result of ascus with a follow-up period > = 2 year .\n",
      "The present study assessed 491 patients (139 HPV-positive and 352 HPV-negative cases) with a PAP test result of ASCUS with a follow-up period >= 2 years.\n",
      "['present', 'study']-['assessed']-['patients', 'with', 'result']\n",
      "========================================\n",
      "1.0 1 722 0\n",
      "the present study aim to investigate difference in prognosis base on human papillomavirus -lrb- hpv -rrb- infection , persistent infection and genotype variation for patient exhibit atypical squamous cell of undetermined significance -lrb- ascus -rrb- in they initial Papanicolaou -lrb- PAP -rrb- test result .\n",
      "The present study aimed to investigate differences in prognosis based on human papillomavirus (HPV) infection, persistent infection and genotype variations for patients exhibiting atypical squamous cells of undetermined significance (ASCUS) in their initial Papanicolaou (PAP) test results.\n",
      "[(1, 2), (43,), None]\n",
      "========================================\n",
      "0.58 1 722 11\n",
      "statistical and lda analysis produce consistent result regard the association between persistent infection of hpv-16 , old age and long infection period with a clinical progression of cin2 or worse .\n",
      "Statistical and LDA analyses produced consistent results regarding the association between persistent infection of HPV-16, old age and long infection period with a clinical progression of CIN2 or worse.\n",
      "['Statistical', 'LDA', 'analyses']-['produced']-['consistent', 'results']\n",
      "========================================\n",
      "2.77 6 1566 1\n",
      "by examine the clinical observation -lrb- such as diagnosis , risk factor , and medication -rrb- mention in longitudinal emr , we can use patient ' medical chronology to automatically predict the progression of they pathology .\n",
      "By examining the clinical observations (such as diagnoses, risk factors, and medications) mentioned in longitudinal EMRs, we can use patients' medical chronologies to automatically predict the progression of their pathologies.\n",
      "['we']-['use']-['medical', 'chronologies']\n",
      "['we']-['examining']-['clinical', 'observations']\n",
      "['we']-['predict']-['progression', 'of', 'pathologies']\n",
      "========================================\n",
      "2.27 5 1566 0\n",
      "the expand clinical information provide by the advent of electronic medical record offer a exciting opportunity to substantially improve the quality of health care .\n",
      "The expanding clinical information provided by the advent of electronic medical records offers an exciting opportunity to substantially improve the quality of health care.\n",
      "['expanding', 'clinical', 'information']-['offers']-['exciting', 'opportunity']\n",
      "========================================\n",
      "1.8 3 1566 4\n",
      "in addition , we model have the potential to improve the quality of over-all patient care in practice by predict the most likely set of clinical observation at a arbitrary point in the future .\n",
      "In addition, our model has the potential to improve the quality of over-all patient care in practice by predicting the most likely set of clinical observations at an arbitrary point in the future.\n",
      "['model']-['has']-['addition']\n",
      "['model']-['has']-['potential']\n",
      "========================================\n",
      "1.58 2 1566 3\n",
      "we show that we model can be use to not only track how a patient 's clinical finding might change over time , but to also identify which patient be due for preventative visit .\n",
      "We show that our model can be used to not only track how a patient's clinical findings might change over time, but to also identify which patients are due for preventative visits.\n",
      "[(0,), (1,), None]\n",
      "[(4,), (7,), None]\n",
      "[(4,), (26,), None]\n",
      "[(4,), (11,), None]\n",
      "['patients']-['are']-['due', 'for', 'visits']\n",
      "['clinical', 'findings']-['change', 'over']-['time']\n",
      "========================================\n",
      "1.58 2 1566 2\n",
      "in this paper , we present a novel probabilistic model which jointly learn how to -lrb- 1 -rrb- group patient base on the similarity between they clinical observation as well as how to -lrb- 2 -rrb- predict the way a new patient 's clinical observation might evolve in the future .\n",
      "In this paper, we present a novel probabilistic model which jointly learns how to (1) group patients based on the similarities between their clinical observations as well as how to (2) predict the way a new patient's clinical observations might evolve in the future.\n",
      "['we']-['present']-['novel', 'probabilistic', 'model']\n",
      "========================================\n",
      "1.88 4 1101 1\n",
      "a hospital emr dataset typically consist of medical record of hospitalize patient .\n",
      "A hospital EMR dataset typically consists of medical records of hospitalized patients.\n",
      "['hospital', 'EMR', 'dataset']-['consists', 'of']-['medical', 'records', 'of', 'patients']\n",
      "========================================\n",
      "1.66 3 1101 4\n",
      "this topic modeling help to understand the constitution of patient disease and offer a tool for better planning of treatment .\n",
      "This topic modeling helps to understand the constitution of patient diseases and offers a tool for better planning of treatment.\n",
      "[(1, 2), (3,), None]\n",
      "['topic', 'modeling']-['understand']-['constitution', 'of', 'diseases']\n",
      "========================================\n",
      "1.66 5 1101 3\n",
      "traditional topic model , such as latent dirichlet allocation -lrb- lda -rrb- and hierarchical dirichlet process -lrb- hdp -rrb- , can be employ to discover disease topic from emr datum by treat patient as document and diagnosis code as word .\n",
      "Traditional topic models, such as latent Dirichlet allocation (LDA) and hierarchical Dirichlet process (HDP), can be employed to discover disease topics from EMR data by treating patients as documents and diagnosis codes as words.\n",
      "[(0, 1, 2), (22,), None]\n",
      "['Traditional', 'topic', 'models']-['discover']-['disease', 'topics']\n",
      "['Traditional', 'topic', 'models']-['discover']-['EMR', 'data']\n",
      "['Traditional', 'topic', 'models']-['treating']-['documents', 'and', 'codes', 'as', 'words']\n",
      "========================================\n",
      "1.25 4 1101 2\n",
      "a medical record contain diagnostic information -lrb- diagnosis code -rrb- , procedure perform -lrb- procedure code -rrb- and admission detail .\n",
      "A medical record contains diagnostic information (diagnosis codes), procedures performed (procedure codes) and admission details.\n",
      "['medical', 'record']-['contains']-['diagnostic', 'information']\n",
      "========================================\n",
      "0.88 2 1101 11\n",
      "we evaluate the propose model on two real-world medical dataset - polyvascular disease and acute myocardial infarction disease .\n",
      "We evaluate the proposed models on two real-world medical datasets - PolyVascular disease and Acute Myocardial Infarction disease.\n",
      "['We']-['evaluate']-['proposed', 'models', 'on', 'datasets']\n",
      "========================================\n",
      "0.66 3 1101 9\n",
      "furthermore , since procedure code be often correlate with diagnosis code , we develop the correspondence wddcrf -lrb- corr-wddcrf -rrb- to explore conditional relationship of procedure code for a give disease pattern .\n",
      "Furthermore, since procedure codes are often correlated with diagnosis codes, we develop the correspondence wddCRF (Corr-wddCRF) to explore conditional relationships of procedure codes for a given disease pattern.\n",
      "['we']-['develop']-['correspondence', 'wddCRF']\n",
      "['procedure', 'codes']-['correlated', 'with']-['diagnosis', 'codes']\n",
      "['we']-['explore']-['conditional', 'relationships', 'of', 'codes']\n",
      "['we']-['explore']-['given', 'disease', 'pattern']\n",
      "========================================\n",
      "0.6 1 1101 0\n",
      "Electronic Medical Record -lrb- EMR -rrb- have establish itself as a valuable resource for large scale analysis of health datum .\n",
      "Electronic Medical Record (EMR) has established itself as a valuable resource for large scale analysis of health data.\n",
      "['Electronic', 'Medical', 'Record']-['established']-['valuable', 'resource', 'for', 'analysis']\n",
      "========================================\n",
      "0.37 2 1101 6\n",
      "we be motivate by the fact that diagnosis code be connect in the form of icd-10 tree structure which present semantic relationship between code .\n",
      "We are motivated by the fact that diagnosis codes are connected in the form of ICD-10 tree structure which presents semantic relationships between codes.\n",
      "[(0,), (2,), None]\n",
      "========================================\n",
      "0.28 1 1101 13\n",
      "we also use disease topic proportion as new feature and show that use feature from the corr-wddcrf outperform the baseline on 14-day readmission prediction .\n",
      "We also use disease topic proportions as new features and show that using features from the Corr-wddCRF outperforms the baselines on 14-days readmission prediction.\n",
      "['We']-['use']-['disease', 'topic', 'proportions']\n",
      "['We']-['use']-['new', 'features']\n",
      "========================================\n",
      "0.28 1 1101 5\n",
      "in this paper , we propose a novel and flexible hierarchical bayesian nonparametric model , the word distance dependent chinese restaurant franchise -lrb- wddcrf -rrb- , which incorporate word-to-word distance to discover semantically-coherent disease topic .\n",
      "In this paper, we propose a novel and flexible hierarchical Bayesian nonparametric model, the word distance dependent Chinese restaurant franchise (wddCRF), which incorporates word-to-word distances to discover semantically-coherent disease topics.\n",
      "['we']-['propose']-['novel', 'flexible', 'hierarchical', 'Bayesian', 'nonparametric', 'model']\n",
      "========================================\n",
      "0.06 1 1101 14\n",
      "beside these , the prediction for procedure code base on the corr-wddcrf also show considerable accuracy .\n",
      "Beside these, the prediction for procedure codes based on the Corr-wddCRF also shows considerable accuracy.\n",
      "['prediction', 'for', 'codes']-['shows']-['considerable', 'accuracy']\n",
      "========================================\n",
      "1.4 3 1112 0\n",
      "information and communication technology have enable healthcare institution to accumulate large amount of healthcare datum that include diagnosis , medication , and additional contextual information such as patient demographic .\n",
      "Information and communications technologies have enabled healthcare institutions to accumulate large amounts of healthcare data that include diagnoses, medications, and additional contextual information such as patient demographics.\n",
      "['Information', 'communications', 'technologies']-['enabled']-['healthcare', 'institutions']\n",
      "['healthcare', 'institutions']-['accumulate']-['large', 'amounts', 'of', 'data']\n",
      "========================================\n",
      "0.98 3 1112 1\n",
      "to gain a better understanding of big healthcare datum and to develop better data-driven clinical decision support system , we propose a novel multiple - channel latent dirichlet allocation -lrb- mclda -rrb- approach for modeling diagnosis , medication , and contextual information in healthcare datum .\n",
      "To gain a better understanding of big healthcare data and to develop better data-driven clinical decision support systems, we propose a novel multiple -channel latent Dirichlet allocation (MCLDA) approach for modeling diagnoses, medications, and contextual information in healthcare data.\n",
      "['we']-['propose']-['allocation', 'approach', 'for', 'diagnoses']\n",
      "['we']-['gain']-['better', 'understanding', 'of', 'data']\n",
      "['we']-['develop']-['better', 'data-driven', 'clinical', 'decision', 'support', 'systems']\n",
      "========================================\n",
      "0.91 2 1112 2\n",
      "the propose MCLDA model assume that a latent health status group structure be responsible for the observe co-occurrence among diagnosis , medication , and contextual information .\n",
      "The proposed MCLDA model assumes that a latent health status group structure is responsible for the observed co-occurrences among diagnoses, medications, and contextual information.\n",
      "[(1, 2, 3), (4,), None]\n",
      "['latent', 'health', 'status', 'group', 'structure']-['is']-['responsible', 'for', 'co-occurrences']\n",
      "========================================\n",
      "0.67 2 1112 8\n",
      "thus , mclda represent a promising approach to modeling healthcare datum for clinical decision support .\n",
      "Thus, MCLDA represents a promising approach to modeling healthcare data for clinical decision support.\n",
      "['MCLDA']-['represents']-['promising', 'approach', 'to', 'data', 'for', 'support']\n",
      "========================================\n",
      "0.59 2 1112 6\n",
      "MCLDA can also be employ to predict miss medication or diagnosis give partial record .\n",
      "MCLDA can also be employed to predict missing medications or diagnoses given partial records.\n",
      "[(0,), (4,), None]\n",
      "['MCLDA']-['predict']-['missing', 'medications', 'or', 'diagnoses']\n",
      "========================================\n",
      "0.59 2 1112 5\n",
      "moreover , MCLDA be able to identify the pairing between diagnosis and medication in a record base on the assign latent group .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moreover, MCLDA is able to identify the pairing between diagnoses and medications in a record based on the assigned latent groups.\n",
      "['MCLDA']-['is']-['able']\n",
      "['MCLDA']-['identify']-['pairing']\n",
      "['MCLDA']-['identify']-['record']\n",
      "========================================\n",
      "0.36 2 1112 3\n",
      "use a real-world research testb that include one million healthcare insurance claim record , we investigate the utility of MCLDA .\n",
      "Using a real-world research testbed that includes one million healthcare insurance claim records, we investigate the utility of MCLDA.\n",
      "['we']-['investigate']-['utility', 'of', 'MCLDA']\n",
      "[(14,), (4,), None]\n",
      "['that']-['includes']-['healthcare', 'insurance', 'claim', 'records']\n",
      "========================================\n",
      "0.31 1 1112 7\n",
      "we evaluation result also show that , in most case , MCLDA outperform alternative method such as logistic regression and the k-nearest-neighbor -lrb- knn -rrb- model for two prediction task , i.e. , medication and diagnosis prediction .\n",
      "Our evaluation results also show that, in most cases, MCLDA outperforms alternative methods such as logistic regressions and the k-nearest-neighbor (KNN) model for two prediction tasks, i.e., medication and diagnosis prediction.\n",
      "[(1, 2), (4,), None]\n",
      "['MCLDA']-['outperforms']-['most', 'cases']\n",
      "['MCLDA']-['outperforms']-['alternative', 'methods']\n",
      "========================================\n",
      "1.34 4 80 9\n",
      "however , this study suggest that imperfect datum -lrb- e.g. , icd code in combination with other ehr feature -rrb- can serve as a silver standard to develop a risk model , apply that model to patient without dementia code , and then select a case-detection threshold .\n",
      "The study is one of the first to utilize both structured and unstructured EHRs to develop risk scores for the diagnosis of dementia.\n",
      "['study']-['suggests']-['that']\n",
      "========================================\n",
      "0.87 3 80 3\n",
      "this study seek to identify case of undiagnosed dementia by develop and validate a weakly supervise machine-learning approach that incorporate the analysis of both structured and unstructured electronic health record -lrb- ehr -rrb- data.methodsa topic modeling approach that include latent Dirichlet allocation , stable topic extraction , and random sampling be apply to VHA ehr .\n",
      "This study seeks to identify cases of undiagnosed dementia by developing and validating a weakly supervised machine-learning approach that incorporates the analysis of both structured and unstructured electronic health record (EHR) data.MethodsA topic modeling approach that included latent Dirichlet allocation, stable topic extraction, and random sampling was applied to VHA EHRs.\n",
      "========================================\n",
      "0.59 3 80 10\n",
      "the study be one of the first to utilize both structured and unstructured ehr to develop risk score for the diagnosis of dementia .\n",
      "sent token differ\n",
      "['study']-['is']-['one', 'of', 'first']\n",
      "========================================\n",
      "0.37 2 80 8\n",
      "ConclusionsDementia be underdiagnosed , and thus , icd code alone can not serve as a gold standard for diagnosis .\n",
      "However, this study suggests that imperfect data (e.g., ICD codes in combination with other EHR features) can serve as a silver standard to develop a risk model, apply that model to patients without dementia codes, and then select a case-detection threshold.\n",
      "['ConclusionsDementia']-['is']-['underdiagnosed', 'and']\n",
      "========================================\n",
      "0.33 2 80 6\n",
      "these score be validate in a subset of veteran without icd-9 dementia code -lrb- n = 120 -rrb- by expert in dementia who perform manual record review and achieve a high level of inter-rater agreement .\n",
      "These scores were validated in a subset of Veterans without ICD-9 dementia codes (n=120) by experts in dementia who performed manual record reviews and achieved a high level of inter-rater agreement.\n",
      "['scores']-['validated', 'in']-['subset', 'of', 'Veterans']\n",
      "['scores']-['validated', 'in']-['ICD-9', 'dementia', 'codes']\n",
      "========================================\n",
      "0.06 1 80 5\n",
      "a logistic regression model be use to develop dementia prediction score , and manual review be conduct to validate the machine-learning results.resultsa total of 853 feature be identify -lrb- 290 topic , 174 non-dementia icd code , 159 CPT code , 59 medication , and 171 note type -rrb- for the development of logistic regression prediction score .\n",
      "A logistic regression model was used to develop dementia prediction scores, and manual reviews were conducted to validate the machine-learning results.ResultsA total of 853 features were identified (290 topics, 174 non-dementia ICD codes, 159 CPT codes, 59 medications, and 171 note types) for the development of logistic regression prediction scores.\n",
      "[(1, 2, 3), (5,), None]\n",
      "['logistic', 'regression', 'model']-['develop']-['dementia', 'prediction', 'scores']\n",
      "========================================\n",
      "0.06 1 80 4\n",
      "topic feature from unstructured datum and feature from structured datum be compare between veteran with -lrb- n = 1861 -rrb- and without -lrb- n = 9305 -rrb- icd-9 dementia code .\n",
      "Topic features from unstructured data and features from structured data were compared between Veterans with (n=1861) and without (n=9305) ICD-9 dementia codes.\n",
      "[(0, 1, 2, 4), (11,), None]\n",
      "========================================\n",
      "1.0 1 32 1\n",
      "predict diabetic complication be regard as a highly effective technique for increase the survival rate of diabetic patient .\n",
      "Predicting diabetic complications is regarded as a highly effective technique for increasing the survival rate of diabetic patients.\n",
      "========================================\n",
      "0.87 2 32 5\n",
      "specifically , we first estimate the similarity between textual medical record after datum preprocessing , and then we perform selda-based diabetic complication topic mining base on similarity constraint .\n",
      "Specifically, we first estimate the similarity between textual medical records after data preprocessing, and then we perform seLDA-based diabetic complication topic mining based on similarity constraints.\n",
      "['we']-['estimate']-['similarity']\n",
      "========================================\n",
      "0.87 2 32 3\n",
      "moreover , the similarity among medical record that be overlook by exist approach could potentially improve the accuracy of prediction model .\n",
      "Moreover, the similarities among medical records that are overlooked by existing approaches could potentially improve the accuracy of prediction models.\n",
      "['similarities']-['improve']-['accuracy', 'of', 'models']\n",
      "========================================\n",
      "0.87 2 32 2\n",
      "while many study currently use medical image and structured medical record , very limited effort have be dedicate to apply datum mining technique for unstructured textual medical record , such as admission and discharge record .\n",
      "While many studies currently use medical images and structured medical records, very limited efforts have been dedicated to applying data mining techniques for unstructured textual medical records, such as admission and discharge records.\n",
      "[(13, 14), (17,), None]\n",
      "['many', 'studies']-['use']-['medical', 'images', 'and', 'records']\n",
      "['many', 'studies']-['use']-['structured', 'medical', 'records']\n",
      "['limited', 'efforts']-['applying']-['data', 'mining', 'techniques']\n",
      "['limited', 'efforts']-['applying']-['unstructured', 'textual', 'medical', 'records']\n",
      "========================================\n",
      "0.6 1 32 0\n",
      "Diabetes and its complication have be recognize worldwide as a major public health threat .\n",
      "Diabetes and its complications have been recognized worldwide as a major public health threat.\n",
      "['Diabetes', 'and', 'complications']-['recognized']-['major', 'public', 'health', 'threat']\n",
      "========================================\n",
      "1.48 4 627 0\n",
      "obstetric electronic medical record -lrb- emr -rrb- contain massive amount of medical datum and health information .\n",
      "Obstetric electronic medical records (EMRs) contain massive amounts of medical data and health information.\n",
      "['Obstetric', 'electronic', 'medical', 'records']-['contain']-['massive', 'amounts', 'of', 'data']\n",
      "========================================\n",
      "0.91 2 627 6\n",
      "the result of the diagnosis assistant can be introduce as a supplementary learning method for medical student .\n",
      "The result of the diagnosis assistant can be introduced as a supplementary learning method for medical students.\n",
      "['result', 'of', 'assistant']-['introduced', 'as']-['supplementary', 'learning', 'method', 'for', 'students']\n",
      "========================================\n",
      "0.88 3 627 7\n",
      "additionally , the method can be use not only for obstetric emr but also for other medical record .\n",
      "Additionally, the method can be used not only for obstetric EMRs but also for other medical records.\n",
      "['method']-['used']-['obstetric', 'EMRs']\n",
      "['method']-['used']-['other', 'medical', 'records']\n",
      "['method']-['used']-['obstetric', 'EMRs']\n",
      "['method']-['used']-['other', 'medical', 'records']\n",
      "========================================\n",
      "0.59 3 627 2\n",
      "the admit diagnosis in the first course record of the emr be reason from various source , such as chief complaint , auxiliary examination , and physical examination .\n",
      "The admitting diagnosis in the first course record of the EMR is reasoned from various sources, such as chief complaints, auxiliary examinations, and physical examinations.\n",
      "['admitting', 'diagnosis', 'in', 'record']-['reasoned', 'from']-['various', 'sources']\n",
      "========================================\n",
      "0.32 2 627 3\n",
      "this paper treat the diagnosis assistant as a multilabel classification task base on the analysis of obstetric emr .\n",
      "This paper treats the diagnosis assistant as a multilabel classification task based on the analyses of obstetric EMRs.\n",
      "['paper']-['treats']-['diagnosis', 'assistant']\n",
      "['paper']-['treats']-['multilabel', 'classification', 'task']\n",
      "========================================\n",
      "0.32 2 627 1\n",
      "the information extraction and diagnosis assistant of obstetric emr be of great significance in improve the fertility level of the population .\n",
      "The information extraction and diagnosis assistants of obstetric EMRs are of great significance in improving the fertility level of the population.\n",
      "['information', 'extraction', 'and', 'assistants', 'of', 'EMRs']-['are']-['great', 'significance']\n",
      "========================================\n",
      "0.31 1 627 4\n",
      "the latent Dirichlet allocation -lrb- lda -rrb- topic and the word vector be use as feature and the four multilabel classification method , bp-mll -lrb- backpropagation multilabel learning -rrb- , rakel -lrb- random k labelset -rrb- , mlknn -lrb- multilabel k-nearest neighbor -rrb- , and cc -lrb- chain classifier -rrb- , be utilize to build the diagnosis assistant model .\n",
      "The latent Dirichlet allocation (LDA) topic and the word vector are used as features and the four multilabel classification methods, BP-MLL (backpropagation multilabel learning), RAkEL (RAndom k labELsets), MLkNN (multilabel k-nearest neighbor), and CC (chain classifier), are utilized to build the diagnosis assistant models.\n",
      "['allocation', 'topic', 'and', 'vector']-['used', 'as']-['features', 'and', 'methods']\n",
      "['allocation', 'topic', 'and', 'vector']-['used', 'as']-['multilabel', 'classification', 'methods']\n",
      "['word', 'vector']-['used', 'as']-['features', 'and', 'methods']\n",
      "['word', 'vector']-['used', 'as']-['multilabel', 'classification', 'methods']\n",
      "========================================\n",
      "1.28 2 415 3\n",
      "the result obtain from the mkl method be give to the ANFIS classifier to classify the heart disease and healthy patient .\n",
      "The result obtained from the MKL method is given to the ANFIS classifier to classify the heart disease and healthy patients.\n",
      "['result']-['given', 'to']-['ANFIS', 'classifier']\n",
      "['result']-['classify']-['heart', 'disease', 'and', 'patients']\n",
      "['result']-['classify']-['healthy', 'patients']\n",
      "========================================\n",
      "1.28 2 415 2\n",
      "mkl method be use to divide parameter between heart disease patient and normal individual .\n",
      "MKL method is used to divide parameters between heart disease patients and normal individuals.\n",
      "[(0, 1), (3,), None]\n",
      "['MKL', 'method']-['divide']-['parameters']\n",
      "========================================\n",
      "0.6 2 415 0\n",
      "multiple Kernel Learning with adaptive Neuro-Fuzzy Inference System -lrb- mkl with anfis -rrb- base deep learning method be propose in this paper for heart disease diagnosis .\n",
      "Multiple Kernel Learning with Adaptive Neuro-Fuzzy Inference System (MKL with ANFIS) based deep learning method is proposed in this paper for heart disease diagnosis.\n",
      "['Learning', 'based', 'deep', 'learning', 'method']-['proposed', 'in']-['heart', 'disease', 'diagnosis']\n",
      "========================================\n",
      "1.58 4 2304 4\n",
      "we obtain meaningful diagnosis and treatment topic -lrb- cluster -rrb- from the datum , which clinically indicate some important medical group correspond to comorbidity disease -lrb- e.g. , heart disease and diabetic kidney disease in t2dm inpatient -rrb- .\n",
      "We obtained meaningful diagnosis and treatment topics (clusters) from the data, which clinically indicated some important medical groups corresponding to comorbidity diseases (e.g., heart disease and diabetic kidney diseases in T2DM inpatients).\n",
      "['We']-['obtained']-['meaningful', 'diagnosis', 'and', 'topics']\n",
      "['We']-['obtained']-['treatment', 'topics']\n",
      "========================================\n",
      "1.0 1 2304 5\n",
      "the result show that manifestation sub-category actually exist in t2dm patient that need specific , individualised cm therapy .\n",
      "The results show that manifestation sub-categories actually exist in T2DM patients that need specific, individualised CM therapies.\n",
      "[(1,), (2,), None]\n",
      "['manifestation', 'sub-categories']-['exist', 'in']-['T2DM', 'patients']\n",
      "========================================\n",
      "0.93 3 2304 1\n",
      "in this paper , we propose a data mining method , call the Symptom-Herb-Diagnosis topic -lrb- shot -rrb- model , to automatically extract the common relationship among symptom , herb combination and diagnosis from large-scale cm clinical datum .\n",
      "In this paper, we propose a data mining method, called the Symptom-Herb-Diagnosis topic (SHOT) model, to automatically extract the common relationships among symptoms, herb combinations and diagnoses from large-scale CM clinical data.\n",
      "['we']-['propose']-['data', 'mining', 'method']\n",
      "========================================\n",
      "0.69 2 2304 3\n",
      "we apply the SHDT model to discover the common cm diagnosis and treatment knowledge for type 2 diabetes mellitus -lrb- t2dm -rrb- use 3 238 inpatient case .\n",
      "We applied the SHDT model to discover the common CM diagnosis and treatment knowledge for type 2 diabetes mellitus (T2DM) using 3 238 inpatient cases.\n",
      "['We']-['applied']-['SHDT', 'model']\n",
      "['We']-['discover']-['common', 'CM', 'diagnosis', 'and', 'knowledge', 'for', 'mellitus']\n",
      "========================================\n",
      "0.58 1 2304 6\n",
      "furthermore , the result demonstrate that this method be helpful for generate cm clinical guideline for t2dm base on structured collect clinical datum .\n",
      "Furthermore, the results demonstrate that this method is helpful for generating CM clinical guidelines for T2DM based on structured collected clinical data.\n",
      "[(3,), (4,), None]\n",
      "['method']-['is']-['helpful']\n",
      "['method']-['generating']-['CM', 'clinical', 'guidelines', 'for', 'T2DM']\n",
      "========================================\n",
      "0.58 1 2304 0\n",
      "induction of common knowledge or regularity from large-scale clinical datum be a vital task for chinese medicine -lrb- cm -rrb- .\n",
      "Induction of common knowledge or regularities from large-scale clinical data is a vital task for Chinese medicine (CM).\n",
      "['Induction', 'of', 'knowledge', 'from', 'data']-['is']-['vital', 'task', 'for', 'medicine']\n",
      "========================================\n",
      "1.0 1 1037 5\n",
      "of 4687 patient with inpatient discharge summary , 470 be readmit within 30 day .\n",
      "Of 4687 patients with inpatient discharge summaries, 470 were readmitted within 30 days.\n",
      "['470']-['readmitted']-['patients', 'with', 'summaries']\n",
      "========================================\n",
      "0.93 4 1037 1\n",
      "the symptom or characteristic of illness course necessary to develop reliable predictor be not available in code billing datum , but may be present in narrative electronic health record -lrb- ehr -rrb- discharge summary .\n",
      "The symptoms or characteristics of illness course necessary to develop reliable predictors are not available in coded billing data, but may be present in narrative electronic health record (EHR) discharge summaries.\n",
      "========================================\n",
      "0.62 2 1037 4\n",
      "the cohort be randomly split to derive a training -lrb- 70 % -rrb- and testing -lrb- 30 % -rrb- data set , and we train separate support vector machine model for baseline clinical feature alone , baseline feature plus common individual word and the above plus topic identify from the 75-topic LDA model .\n",
      "The cohort was randomly split to derive a training (70%) and testing (30%) data set, and we trained separate support vector machine models for baseline clinical features alone, baseline features plus common individual words and the above plus topics identified from the 75-topic LDA model.\n",
      "[(1,), (4,), None]\n",
      "['cohort']-['derive']-['training', 'and', 'set']\n",
      "['cohort']-['derive']-['testing', 'data', 'set']\n",
      "========================================\n",
      "0.58 2 1037 9\n",
      "topic modeling and related approach offer the potential to improve prediction use ehr , if generalizability can be establish in other clinical cohort .\n",
      "Topic modeling and related approaches offer the potential to improve prediction using EHRs, if generalizability can be established in other clinical cohorts.\n",
      "['Topic', 'modeling', 'and', 'approaches']-['offer']-['potential']\n",
      "['related', 'approaches']-['offer']-['potential']\n",
      "['generalizability']-['established', 'in']-['other', 'clinical', 'cohorts']\n",
      "========================================\n",
      "0.31 1 1037 2\n",
      "we identify a cohort of individual admit to a psychiatric inpatient unit between 1994 and 2012 with a principal diagnosis of major depressive disorder , and extract inpatient psychiatric discharge narrative note .\n",
      "We identified a cohort of individuals admitted to a psychiatric inpatient unit between 1994 and 2012 with a principal diagnosis of major depressive disorder, and extracted inpatient psychiatric discharge narrative notes.\n",
      "['We']-['identified']-['cohort', 'of', 'individuals', 'and', 'notes']\n",
      "['We']-['identified']-['extracted', 'inpatient', 'psychiatric', 'discharge', 'narrative', 'notes']\n",
      "========================================\n",
      "0.28 1 1037 6\n",
      "the 75-topic LDA model include topic link to psychiatric symptom -lrb- suicide , severe depression , anxiety , trauma , eating/weight and panic -rrb- and major depressive disorder comorbidity -lrb- infection , postpartum , brain tumor , diarrhea and pulmonary disease -rrb- .\n",
      "The 75-topic LDA model included topics linked to psychiatric symptoms (suicide, severe depression, anxiety, trauma, eating/weight and panic) and major depressive disorder comorbidities (infection, postpartum, brain tumor, diarrhea and pulmonary disease).\n",
      "['75-topic', 'LDA', 'model']-['included']-['topics']\n",
      "========================================\n",
      "0.27 1 1037 8\n",
      "inclusion of topic derive from narrative note allow more accurate discrimination of individual at high risk for psychiatric readmission in this cohort .\n",
      "Inclusion of topics derived from narrative notes allows more accurate discrimination of individuals at high risk for psychiatric readmission in this cohort.\n",
      "['Inclusion', 'of', 'topics']-['allows']-['accurate', 'discrimination', 'of', 'individuals', 'at', 'risk']\n",
      "========================================\n",
      "0.27 1 1037 0\n",
      "the ability to predict psychiatric readmission would facilitate the development of intervention to reduce this risk , a major driver of psychiatric health-care cost .\n",
      "The ability to predict psychiatric readmission would facilitate the development of interventions to reduce this risk, a major driver of psychiatric health-care costs.\n",
      "['ability']-['facilitate']-['development', 'of', 'interventions']\n",
      "========================================\n",
      "0.04 1 1037 7\n",
      "by include LDA topic , prediction of readmission , as measure by area under receiver-operating characteristic curve in the testing data set , be improve from baseline -lrb- area under the curve 0.618 -rrb- to baseline + 1000 word -lrb- 0.682 -rrb- to baseline +75 topic -lrb- 0.784 -rrb- .\n",
      "By including LDA topics, prediction of readmission, as measured by area under receiver-operating characteristic curves in the testing data set, was improved from baseline (area under the curve 0.618) to baseline + 1000 words (0.682) to baseline+75 topics (0.784).\n",
      "['prediction', 'of', 'readmission']-['improved', 'from']-['baseline', '+', 'words', 'to', 'topics']\n",
      "========================================\n",
      "1.58 2 1446 0\n",
      "clinical pathway -lrb- cp -rrb- analysis play a important role in health-care management in ensure specialize , standardized , normalize and sophisticated therapy procedure for individual patient .\n",
      "Clinical pathway (CP) analysis plays an important role in health-care management in ensuring specialized, standardized, normalized and sophisticated therapy procedures for individual patients.\n",
      "['pathway', 'analysis']-['plays']-['important', 'role']\n",
      "['pathway', 'analysis']-['plays']-['health-care', 'management']\n",
      "['pathway', 'analysis']-['ensuring']-['specialized', 'standardized', 'normalized', 'sophisticated', 'therapy', 'procedures', 'for', 'patients']\n",
      "========================================\n",
      "1.4 3 1446 4\n",
      "discover treatment pattern , as actionable knowledge represent the best practice for most patient in most time of they treatment process , form the backbone of cp , and can be exploit to help physician better understand they specialty and learn from previous experience for cp analysis and improvement .\n",
      "Discovered treatment patterns, as actionable knowledge representing the best practice for most patients in most time of their treatment processes, form the backbone of CPs, and can be exploited to help physicians better understand their specialty and learn from previous experiences for CP analysis and improvement.\n",
      "[(0, 1, 2), (31,), None]\n",
      "['actionable', 'knowledge', 'in', 'time']-['form']-['backbone', 'of', 'CPs']\n",
      "['Discovered', 'treatment', 'patterns']-['help']-['physicians']\n",
      "========================================\n",
      "1.38 3 1446 3\n",
      "more specifically , we develop a probabilistic topic model to link patient feature and treatment behavior together to mine treatment pattern hide in emr .\n",
      "More specifically, we develop a probabilistic topic model to link patient features and treatment behaviors together to mine treatment patterns hidden in EMRs.\n",
      "['we']-['develop']-['probabilistic', 'topic', 'model']\n",
      "========================================\n",
      "0.88 3 1446 1\n",
      "recently , with the rapid development of hospital information system , a large volume of electronic medical record -lrb- emr -rrb- have be produce , which provide a comprehensive source for cp analysis .\n",
      "Recently, with the rapid development of hospital information systems, a large volume of electronic medical records (EMRs) has been produced, which provides a comprehensive source for CP analysis.\n",
      "['large', 'volume', 'of', 'records']-['produced']-['rapid', 'development', 'of', 'systems']\n",
      "========================================\n",
      "0.38 2 1446 5\n",
      "experimental result on a real collection of 985 emr collect from a chinese hospital show that the propose approach can effectively identify meaningful treatment pattern from emr .\n",
      "Experimental results on a real collection of 985 EMRs collected from a Chinese hospital show that the proposed approach can effectively identify meaningful treatment patterns from EMRs.\n",
      "[(0, 1, 2, 5), (14,), None]\n",
      "['proposed', 'approach']-['identify']-['meaningful', 'treatment', 'patterns', 'from', 'EMRs']\n",
      "========================================\n",
      "0.0 1 1446 2\n",
      "in this paper , we be concern with the problem of utilize the heterogeneous emr to assist cp analysis and improvement .\n",
      "In this paper, we are concerned with the problem of utilizing the heterogeneous EMRs to assist CP analysis and improvement.\n",
      "['we']-['concerned', 'with']-['paper']\n",
      "['we']-['concerned', 'with']-['problem']\n",
      "========================================\n",
      "1.88 4 1402 0\n",
      "background and objective : risk stratification aim to provide physician with the accurate assessment of a patient 's clinical risk such that a individualized prevention or management strategy can be develop and deliver .\n",
      "Background and objective: Risk stratification aims to provide physicians with the accurate assessment of a patient's clinical risk such that an individualized prevention or management strategy can be developed and delivered.\n",
      "========================================\n",
      "1.86 3 1402 7\n",
      "result : we verify the effectiveness of the propose approach on a clinical dataset contain 3463 coronary heart disease -lrb- chd -rrb- patient instance .\n",
      "Results: We verify the effectiveness of the proposed approach on a clinical dataset containing 3463 coronary heart disease (CHD) patient instances.\n",
      "========================================\n",
      "1.86 4 1402 4\n",
      "the propose PRSM recognize a patient clinical state as a probabilistic combination of latent sub-profile , and generate sub-profile-specific risk tier of patient from they ehr in a fully unsupervised fashion .\n",
      "The proposed PRSM recognizes a patient clinical state as a probabilistic combination of latent sub-profiles, and generates sub-profile-specific risk tiers of patients from their EHRs in a fully unsupervised fashion.\n",
      "['proposed', 'PRSM']-['recognizes']-['patient', 'clinical', 'state']\n",
      "['proposed', 'PRSM']-['recognizes']-['probabilistic', 'combination', 'of', 'sub-profiles']\n",
      "========================================\n",
      "1.85 3 1402 1\n",
      "exist risk stratification technique mainly focus on predict the overall risk of a individual patient in a supervised manner , and , at the cohort level , often offer little insight beyond a flat score-based segmentation from the label clinical dataset .\n",
      "Existing risk stratification techniques mainly focus on predicting the overall risk of an individual patient in a supervised manner, and, at the cohort level, often offer little insight beyond a flat score-based segmentation from the labeled clinical dataset.\n",
      "[(0, 1, 2, 3), (5,), None]\n",
      "['Existing', 'risk', 'stratification', 'techniques']-['predicting']-['overall', 'risk', 'of', 'patient']\n",
      "========================================\n",
      "1.27 2 1402 12\n",
      "moreover , patient sub-profile and sub-profile-specific risk tier generate by we model be coherent and informative , and provide significant potential to be explore for the further task , such as patient cohort analysis .\n",
      "Moreover, patient sub-profiles and sub-profile-specific risk tiers generated by our models are coherent and informative, and provide significant potential to be explored for the further tasks, such as patient cohort analysis.\n",
      "['patient', 'sub-profiles', 'and', 'tiers']-['are']-['coherent', 'and', 'informative', 'and', 'provide']\n",
      "['sub-profile-specific', 'risk', 'tiers']-['are']-['coherent', 'and', 'informative', 'and', 'provide']\n",
      "========================================\n",
      "1.15 4 1402 2\n",
      "to this end , in this paper , we propose a new approach for risk stratification by explore a large volume of electronic health record -lrb- ehr -rrb- in a unsupervised fashion .\n",
      "To this end, in this paper, we propose a new approach for risk stratification by exploring a large volume of electronic health records (EHRs) in an unsupervised fashion.\n",
      "['we']-['propose']-['new', 'approach', 'for', 'stratification']\n",
      "['we']-['exploring']-['large', 'volume', 'of', 'records']\n",
      "['we']-['exploring']-['unsupervised', 'fashion']\n",
      "========================================\n",
      "0.56 2 1402 11\n",
      "in addition , the unsupervised nature of we model make they highly portable to the risk stratification task of various disease .\n",
      "In addition, the unsupervised nature of our models makes them highly portable to the risk stratification tasks of various diseases.\n",
      "['unsupervised', 'nature', 'of', 'models']-['makes']-['addition']\n",
      "['them']-['portable', 'to']-['risk', 'stratification', 'tasks', 'of', 'diseases']\n",
      "========================================\n",
      "0.56 2 1402 6\n",
      "in addition , we present a extension of PRSM , call weakly supervise prsm -lrb- ws-prsm -rrb- by incorporate minimum prior information into the model , in order to improve the risk stratification accuracy , and to make we model highly portable to risk stratification task of various disease .\n",
      "In addition, we present an extension of PRSM, called weakly supervised PRSM (WS-PRSM) by incorporating minimum prior information into the model, in order to improve the risk stratification accuracy, and to make our models highly portable to risk stratification tasks of various diseases.\n",
      "['we']-['present']-['extension', 'of', 'PRSM']\n",
      "========================================\n",
      "0.31 2 1402 10\n",
      "conclusion : experimental result reveal that we model achieve competitive performance in risk stratification in comparison with exist supervised approach .\n",
      "Conclusions: Experimental results reveal that our models achieve competitive performance in risk stratification in comparison with existing supervised approaches.\n",
      "========================================\n",
      "0.28 2 1402 13\n",
      "we hypothesize that the propose framework can readily meet the demand for risk stratification from a large volume of ehr in a open-ended fashion .\n",
      "We hypothesize that the proposed framework can readily meet the demand for risk stratification from a large volume of EHRs in an open-ended fashion.\n",
      "[(0,), (1,), None]\n",
      "['proposed', 'framework']-['meet']-['demand', 'for', 'stratification', 'from', 'volume']\n",
      "========================================\n",
      "0.27 1 1402 9\n",
      "as well , in comparison with prsm , ws-prsm have over 2 % performance gain , on the experimental dataset , demonstrate that incorporate risk score knowledge as prior information can improve the performance in risk stratification .\n",
      "As well, in comparison with PRSM, WS-PRSM has over 2% performance gain, on the experimental dataset, demonstrating that incorporating risk scoring knowledge as prior information can improve the performance in risk stratification.\n",
      "['WS-PRSM']-['has']-['comparison', 'with', 'PRSM']\n",
      "['WS-PRSM']-['has']-['performance', 'gain']\n",
      "['WS-PRSM']-['has']-['experimental', 'dataset']\n",
      "['WS-PRSM']-['demonstrating']-['that']\n",
      "========================================\n",
      "0.27 1 1402 8\n",
      "both prsm and ws-prsm be compare with two established supervised risk stratification algorithm , i.e. , logistic regression and support vector machine , and show the effectiveness of we model in risk stratification of chd in term of the Area under the receiver operating characteristic curve -lrb- auc -rrb- analysis .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Both PRSM and WS-PRSM were compared with two established supervised risk stratification algorithms, i.e., logistic regression and support vector machine, and showed the effectiveness of our models in risk stratification of CHD in terms of the Area Under the receiver operating characteristic Curve (AUC) analysis.\n",
      "['PRSM', 'and', 'WS-PRSM']-['compared', 'with']-['established', 'supervised', 'risk', 'stratification', 'algorithms']\n",
      "========================================\n",
      "0.27 1 1402 3\n",
      "method : along this line , this paper propose a novel probabilistic topic modeling framework call probabilistic risk stratification model -lrb- prsm -rrb- base on latent Dirichlet allocation -lrb- lda -rrb- .\n",
      "Methods: Along this line, this paper proposes a novel probabilistic topic modeling framework called probabilistic risk stratification model (PRSM) based on Latent Dirichlet Allocation (LDA).\n",
      "========================================\n",
      "1.67 3 1129 0\n",
      "in healthcare organizational setting , the design of a clinical pathway -lrb- cp -rrb- be challenge since patient follow a particular pathway may have not only one single first-diagnosis but also several typical comorbidity , and thus it require different discipline involve to put together they partial knowledge about the overall pathway .\n",
      "In healthcare organizational settings, the design of a clinical pathway (CP) is challenging since patients following a particular pathway may have not only one single first-diagnosis but also several typical comorbidities, and thus it requires different disciplines involved to put together their partial knowledge about the overall pathway.\n",
      "['design', 'of', 'pathway']-['challenging']-['healthcare', 'organizational', 'settings']\n",
      "========================================\n",
      "1.58 2 1129 7\n",
      "we verify the effectiveness of the propose model on a real clinical dataset contain 12,120 patient trace , which pertain to the unstable angina cp .\n",
      "We verify the effectiveness of the proposed model on a real clinical dataset containing 12,120 patient traces, which pertain to the unstable angina CP.\n",
      "['We']-['verify']-['effectiveness', 'of', 'model']\n",
      "['We']-['verify']-['real', 'clinical', 'dataset']\n",
      "========================================\n",
      "0.98 2 1129 9\n",
      "in addition , a possible medical application in term of treatment recommendation be provide to illustrate the potential of the propose model .\n",
      "In addition, a possible medical application in terms of treatment recommendation is provided to illustrate the potential of the proposed model.\n",
      "['possible', 'medical', 'application', 'in', 'terms']-['provided']-['addition']\n",
      "['possible', 'medical', 'application', 'in', 'terms']-['illustrate']-['potential', 'of', 'model']\n",
      "========================================\n",
      "0.96 2 1129 1\n",
      "although many datum mining technique have be propose to discover latent treatment information for cp analysis and reconstruction from a large volume of clinical datum , they be specific to extract nontrivial information about the therapy and treatment of the first-diagnosis .\n",
      "Although many data mining techniques have been proposed to discover latent treatment information for CP analysis and reconstruction from a large volume of clinical data, they are specific to extract nontrivial information about the therapy and treatment of the first-diagnosis.\n",
      "['they']-['are']-['specific']\n",
      "[(1, 2, 3, 4), (7,), None]\n",
      "['they']-['extract']-['nontrivial', 'information', 'about', 'therapy']\n",
      "['many', 'data', 'mining', 'techniques']-['discover']-['latent', 'treatment', 'information', 'for', 'analysis']\n",
      "['many', 'data', 'mining', 'techniques']-['discover']-['large', 'volume', 'of', 'data']\n",
      "========================================\n",
      "0.69 2 1129 6\n",
      "it first generate a set of latent treatment pattern from diagnosis label , follow by sampling treatment from each pattern .\n",
      "It first generates a set of latent treatment patterns from diagnosis labels, followed by sampling treatments from each pattern.\n",
      "['It']-['generates']-['set', 'of', 'patterns']\n",
      "========================================\n",
      "0.69 2 1129 4\n",
      "in particular , we propose a generative statistical model to extract underlie treatment pattern , unveil the latent association between diagnosis label -lrb- include both first-diagnosis and comorbidity -rrb- and treatment , and compute the contribution of comorbidity in these pattem .\n",
      "In particular, we propose a generative statistical model to extract underlying treatment patterns, unveil the latent associations between diagnosis labels (including both first-diagnosis and comorbidities) and treatments, and compute the contribution of comorbidities in these pattems.\n",
      "['we']-['propose']-['generative', 'statistical', 'model']\n",
      "========================================\n",
      "0.38 1 1129 10\n",
      "experimental result indicate that we approach can discover not only meaningful latent treatment pattern exhibit comorbidity focus , but also implicit change of treatment of first-diagnosis due to the incorporation of typical comorbidity potentially .\n",
      "Experimental results indicate that our approach can discover not only meaningful latent treatment patterns exhibiting comorbidity focus, but also implicit changes of treatments of first-diagnosis due to the incorporation of typical comorbidities potentially.\n",
      "[(0, 1), (2,), None]\n",
      "['approach']-['discover']-['meaningful', 'latent', 'treatment', 'patterns', 'but', 'changes']\n",
      "['approach']-['discover']-['implicit', 'changes', 'of', 'treatments']\n",
      "========================================\n",
      "0.38 1 1129 8\n",
      "three treatment pattern be discover from datum , indicate latent correlation between comorbidity and treatment in the pathway .\n",
      "Three treatment patterns are discovered from data, indicating latent correlations between comorbidities and treatments in the pathway.\n",
      "['treatment', 'patterns']-['discovered', 'from']-['data']\n",
      "['treatment', 'patterns']-['indicating']-['latent', 'correlations', 'in', 'pathway']\n",
      "========================================\n",
      "0.38 1 1129 3\n",
      "this study propose to extract latent treatment pattem that characterize essential treatment for both first-diagnosis and typical comorbidity from the execution datum of a pathway .\n",
      "This study proposes to extract latent treatment pattems that characterize essential treatments for both first-diagnosis and typical comorbidities from the execution data of a pathway.\n",
      "[(1,), (2,), None]\n",
      "['study']-['extract']-['latent', 'treatment', 'pattems']\n",
      "========================================\n",
      "0.38 1 1129 2\n",
      "the influence of comorbidity on adopt essential treatment be crucial for a pathway but have seldom be explore .\n",
      "The influence of comorbidities on adopting essential treatments is crucial for a pathway but has seldom been explored.\n",
      "['influence', 'of', 'comorbidities']-['is']-['crucial', 'for', 'pathway', 'but', 'explored']\n",
      "========================================\n",
      "0.31 1 1129 5\n",
      "the propose model extend latent Dirichlet allocation with a additional layer for diagnosis modeling .\n",
      "The proposed model extends latent Dirichlet allocation with an additional layer for diagnosis modeling.\n",
      "['proposed', 'model']-['extends']-['latent', 'Dirichlet', 'allocation']\n",
      "['proposed', 'model']-['extends']-['additional', 'layer', 'for', 'modeling']\n",
      "========================================\n",
      "1.04 2 174 1\n",
      "in this study , we use social media data to examine the knowledge , attitude , and belief of as patient regard biologic therapy .\n",
      "In this study, we used social media data to examine the knowledge, attitudes, and beliefs of AS patients regarding biologic therapies.\n",
      "['we']-['used']-['social', 'media', 'data']\n",
      "['we']-['examine']-['knowledge', 'attitudes', 'and', 'beliefs']\n",
      "['we']-['examine']-['beliefs', 'of', 'patients']\n",
      "========================================\n",
      "1.03 2 174 12\n",
      "conclusion Social media reveal a dynamic range of theme govern as patient ' experience with and choice of biologic agent .\n",
      "Conclusion Social media revealed a dynamic range of themes governing AS patients' experience with and choice of biologic agents.\n",
      "['Conclusion', 'Social', 'media']-['revealed']-['dynamic', 'range', 'of', 'themes']\n",
      "========================================\n",
      "1.0 1 174 13\n",
      "the complexity of select biologic from among many such agent and navigate they risk/benefit profile suggest the merit of create online tool tailor to support patient ' decision-making with regard to biologic therapy for as .\n",
      "The complexity of selecting biologics from among many such agents and navigating their risk/benefit profiles suggests the merit of creating online tools tailored to support patients' decision-making with regard to biologic therapies for AS.\n",
      "['complexity']-['suggests']-['merit']\n",
      "========================================\n",
      "1.0 1 174 11\n",
      "additional implicit patient need -lrb- e.g. , support -rrb- be identify use qualitative analysis .\n",
      "Additional implicit patient needs (e.g., support) were identified using qualitative analyses.\n",
      "[(0, 1, 2, 3), (10,), None]\n",
      "['Additional', 'implicit', 'patient', 'needs']-['using']-['qualitative', 'analyses']\n",
      "========================================\n",
      "1.0 1 174 0\n",
      "objective few study have examine ankylose spondylitis -lrb- as -rrb- patient ' concern about and perception of biologic therapy , apart from traditional survey .\n",
      "Objective Few studies have examined ankylosing spondylitis (AS) patients' concerns about and perceptions of biologic therapies, apart from traditional surveys.\n",
      "[(0, 1, 2), (4,), None]\n",
      "['Objective', 'Few', 'studies']-['ankylosing']-['spondylitis']\n",
      "========================================\n",
      "0.88 2 174 9\n",
      "other theme , include the psychological impact of as , reporting of medical literature , and as disease consequence , account for the remain 40 % -lrb- n = 45 -rrb- .\n",
      "Other themes, including the psychological impact of AS, reporting of medical literature, and AS disease consequences, accounted for the remaining 40% (n = 45).\n",
      "========================================\n",
      "0.73 3 174 10\n",
      "in discussion regard as treatment , most topic involve biologic , and most subtheme involve side effect -lrb- e.g. , fatigue , allergic reaction -rrb- , biologic treatment attribute -lrb- e.g. , dosing , frequency -rrb- , and concern about use of biologic -lrb- e.g. , increase cancer risk -rrb- .\n",
      "In discussions regarding AS treatment, most topics involved biologics, and most subthemes involved side effects (e.g., fatigue, allergic reactions), biologic treatment attributes (e.g., dosing, frequency), and concerns about use of biologics (e.g., increased cancer risk).\n",
      "['most', 'topics']-['involved']-['discussions']\n",
      "['most', 'topics']-['involved']-['biologics']\n",
      "========================================\n",
      "0.38 1 174 8\n",
      "the majority of theme -lrb- n = 67 -lsb- 60 % -rsb- -rrb- focus on discussion relate to as treatment .\n",
      "The majority of themes (n = 67 [60%]) focused on discussions related to AS treatment.\n",
      "['majority', 'of', 'themes']-['focused', 'on']-['discussions']\n",
      "========================================\n",
      "0.04 1 174 6\n",
      "the topic be manually review to identify theme , which be confirm use thematic data analysis .\n",
      "The topics were manually reviewed to identify themes, which were confirmed using thematic data analysis.\n",
      "[(1,), (4,), None]\n",
      "['topics']-['identify']-['themes']\n",
      "========================================\n",
      "0.04 1 174 4\n",
      "to explore theme within the collection of post in a unsupervised manner , a latent Dirichlet allocation topic model be fit to the data set .\n",
      "To explore themes within the collection of posts in an unsupervised manner, a latent Dirichlet allocation topic model was fit to the data set.\n",
      "['latent', 'Dirichlet', 'allocation', 'topic', 'model']-['fit', 'to']-['data', 'set']\n",
      "['latent', 'Dirichlet', 'allocation', 'topic', 'model']-['explore']-['unsupervised', 'manner']\n",
      "========================================\n",
      "2.2 4 338 14\n",
      "meanwhile , patient with serious disease be more interested in medical competence -lrb- Cohen d = -0.99 , Delta u = -0.165 , t = -32.58 , and p < .001 -rrb- , medical advice and prescription -lrb- Cohen d = -0.65 , Delta u = -0.082 , t = -21.45 , and p < .001 -rrb- , financing -lrb- Cohen d = -0.26 , Delta u = -0.018 , t = -8.45 , and p < .001 -rrb- , and diagnosis and pathogenesis -lrb- Cohen d = -1.55 , Delta u = -0.229 , t = -50.93 , and p < .001 -rrb- .\n",
      "Meanwhile, patients with serious diseases were more interested in medical competence (Cohen d=-0.99, Delta u=-0.165, t=-32.58, and P<.001), medical advice and prescription (Cohen d=-0.65, Delta u=-0.082, t=-21.45, and P<.001), financing (Cohen d=-0.26, Delta u=-0.018, t=-8.45, and P<.001), and diagnosis and pathogenesis (Cohen d=-1.55, Delta u=-0.229, t=-50.93, and P<.001).\n",
      "['patients', 'with', 'diseases']-['were']-['interested', 'in', 'competence']\n",
      "========================================\n",
      "1.88 3 338 13\n",
      "patient with mild disease be more interested in medical ethic -lrb- Cohen d = 0.25 , Delta u 0.039 , t = 8.33 , and p < .001 -rrb- , operation process -lrb- Cohen d = 0.57 , Delta u 0.060 , t = 18.75 , and p < .001 -rrb- , patient profile -lrb- Cohen d = 1.19 , Delta u 0.132 , t = 39.33 , and p < .001 -rrb- , and symptom -lrb- Cohen d = 1.91 , Delta u = 0.274 , t = 62.82 , and p < .001 -rrb- .\n",
      "Patients with mild diseases were more interested in medical ethics (Cohen d=0.25, Delta u 0.039, t=8.33, and P<.001), operation process (Cohen d=0.57, Delta u 0.060, t=18.75, and P<.001), patient profile (Cohen d=1.19, Delta u 0.132, t=39.33, and P<.001), and symptoms (Cohen d=1.91, Delta u=0.274, t=62.82, and P<.001).\n",
      "['Patients', 'with', 'diseases']-['were']-['interested', 'in', 'ethics']\n",
      "========================================\n",
      "1.6 3 338 12\n",
      "symptom -lrb- Cohen d = 1.58 , Delta u = 0.216 , t = 229.75 , and p < .001 -rrb- be more often mention by patient with acute disease , whereas communication skill -lrb- Cohen d = -0.29 , Delta u = -0.038 , t = -42.01 , and p < .001 -rrb- , financing -lrb- Cohen d = -0.68 , Delta u = -0.098 , t = -99.26 , and p < .001 -rrb- , and diagnosis and pathogenesis -lrb- Cohen d = -0.55 , Delta u = -0.078 , t = -80.09 , and p < .001 -rrb- be more often mention by patient with chronic disease .\n",
      "Symptoms (Cohen d=1.58, Delta u=0.216, t=229.75, and P<.001) are more often mentioned by patients with acute diseases, whereas communication skills (Cohen d=-0.29, Delta u=-0.038, t=-42.01, and P<.001), financing (Cohen d=-0.68, Delta u=-0.098, t=-99.26, and P<.001), and diagnosis and pathogenesis (Cohen d=-0.55, Delta u=-0.078, t=-80.09, and P<.001) are more often mentioned by patients with chronic diseases.\n",
      "[(0,), (24,), None]\n",
      "========================================\n",
      "1.31 2 338 9\n",
      "the patient-related domain include the category of the patient profile , symptom , diagnosis , and pathogenesis .\n",
      "The patient-related domain included the categories of the patient profile, symptoms, diagnosis, and pathogenesis.\n",
      "['patient-related', 'domain']-['included']-['categories', 'of', 'profile']\n",
      "========================================\n",
      "1.28 2 338 17\n",
      "furthermore , the mining result reveal marked difference in patient ' interest across different disease type , socioeconomic development level , and hospital level .\n",
      "Furthermore, the mining results reveal marked differences in patients' interests across different disease types, socioeconomic development levels, and hospital levels.\n",
      "['mining']-['results']-['reveal', 'levels', 'and', 'levels']\n",
      "['mining']-['results']-['socioeconomic', 'development', 'levels']\n",
      "['mining']-['results']-['hospital', 'levels']\n",
      "========================================\n",
      "1.03 2 338 4\n",
      "objective : this study aim to develop a hierarchical topic taxonomy to uncover the latent structure of physician review and illustrate its application for mining patient ' interest base on the propose taxonomy and algorithm .\n",
      "Objective: This study aims to develop a hierarchical topic taxonomy to uncover the latent structure of physician reviews and illustrate its application for mining patients' interests based on the proposed taxonomy and algorithm.\n",
      "========================================\n",
      "1.0 1 338 16\n",
      "the propose algorithm base on labeled-latent Dirichlet allocation can achieve impressive classification result for mining patient ' interest .\n",
      "The proposed algorithm based on Labeled-Latent Dirichlet Allocation can achieve impressive classification results for mining patients' interests.\n",
      "[(1, 2), (9,), None]\n",
      "['impressive', 'classification']-['results', 'for']-['interests']\n",
      "========================================\n",
      "0.6 1 338 8\n",
      "the physician-related domain include the category of medical ethic , medical competence , communication skill , medical advice , and prescription .\n",
      "The physician-related domain included the categories of medical ethics, medical competence, communication skills, medical advice, and prescriptions.\n",
      "['physician-related', 'domain']-['included']-['categories', 'of', 'ethics']\n",
      "========================================\n",
      "0.06 2 338 15\n",
      "conclusion : this mixed-methods approach , integrate literature review , data-driven topic discovery , and human annotation , be a effective and rigorous way to develop a physician review topic taxonomy .\n",
      "Conclusions: This mixed-methods approach, integrating literature reviews, data-driven topic discovery, and human annotation, is an effective and rigorous way to develop a physician review topic taxonomy.\n",
      "========================================\n",
      "0.03 1 338 6\n",
      "mixed method , include a literature review , data-driven-based topic discovery , and human annotation be use to develop the physician review topic taxonomy .\n",
      "Mixed methods, including a literature review, data-driven-based topic discovery, and human annotation were used to develop the physician review topic taxonomy.\n",
      "[(0, 1), (16,), None]\n",
      "['Mixed', 'methods']-['develop']-['physician', 'review', 'topic', 'taxonomy']\n",
      "========================================\n",
      "0.03 2 338 5\n",
      "method : datum comprise 122,716 physician review , include review of 8501 doctor from a lead physician review website in China -lrb- haodf.com -rrb- , collect between 2007 and 2015 .\n",
      "Methods: Data comprised 122,716 physician reviews, including reviews of 8501 doctors from a leading physician review website in China (haodf.com), collected between 2007 and 2015.\n",
      "========================================\n",
      "0.03 1 338 2\n",
      "the first step toward mining physician review be to determine how the natural structure or dimension be embed in review .\n",
      "The first step toward mining physician reviews is to determine how the natural structure or dimensions is embedded in reviews.\n",
      "[(1, 2), (7,), None]\n",
      "[(1, 2), (9,), None]\n",
      "['natural', 'structure', 'or', 'dimensions']-['embedded', 'in']-['reviews']\n",
      "========================================\n",
      "0.03 1 338 1\n",
      "although many study have explore the text information of physician review , very few have focus on develop a systematic topic taxonomy embed in physician review .\n",
      "Although many studies have explored the text information of physician reviews, very few have focused on developing a systematic topic taxonomy embedded in physician reviews.\n",
      "[(13,), (15,), None]\n",
      "['many', 'studies']-['explored']-['text', 'information', 'of', 'reviews']\n",
      "['few']-['developing']-['systematic', 'topic', 'taxonomy']\n",
      "========================================\n",
      "0.03 1 338 0\n",
      "background : web-based physician review be invaluable gold mine that merit further investigation .\n",
      "Background: Web-based physician reviews are invaluable gold mines that merit further investigation.\n",
      "========================================\n",
      "2.56 5 1624 1\n",
      "these patient record contain valuable medical information include patient information , diagnosis , treatment method , and eventual patient outcome .\n",
      "These patient records contain valuable medical information including patient information, diagnosis, treatment methods, and eventual patient outcomes.\n",
      "['patient', 'records']-['contain']-['valuable', 'medical', 'information']\n",
      "========================================\n",
      "1.09 3 1624 0\n",
      "the recent year have see a surge in the implementation of electronic health care record .\n",
      "The recent years have seen a surge in the implementation of electronic health care records.\n",
      "['recent', 'years']-['seen']-['surge', 'in', 'implementation']\n",
      "========================================\n",
      "1.0 1 1624 3\n",
      "in this paper , we present a method for automatically discover underlie theme and pattern within patient datum .\n",
      "In this paper, we present a method for automatically discovering underlying themes and patterns within patient data.\n",
      "['we']-['present']-['paper']\n",
      "['we']-['present']-['method']\n",
      "========================================\n",
      "0.87 2 1624 5\n",
      "in we research , we partition graph from term gather from electronic health record .\n",
      "In our research, we partitioned graphs from terms gathered from electronic health records.\n",
      "['we']-['partitioned']-['research']\n",
      "['we']-['partitioned']-['graphs']\n",
      "['we']-['partitioned']-['terms']\n",
      "========================================\n",
      "0.27 1 1624 2\n",
      "it be important to analyze pattern within these record in order to more efficiently treat individual .\n",
      "It is important to analyze patterns within these records in order to more efficiently treat individuals.\n",
      "['It']-['is']-['important']\n",
      "['It']-['analyze']-['patterns']\n",
      "['It']-['treat']-['individuals']\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "t_idx = 1\n",
    "for top_doc in top_docs[t_idx]:\n",
    "    _idx = top_doc[0]\n",
    "    if type(parse[_idx]) is str:\n",
    "        print(_idx, \"parse\", \"err\")\n",
    "        continue\n",
    "    sents = relation.convert_parse2lemma_sents(parse[_idx])\n",
    "    sort_idxs, importance, counts = relation.extract_important_sents(sents, [x[0] for x in top_terms[t_idx]], [x[1] for x in top_terms[t_idx]])\n",
    "    for i in sort_idxs:\n",
    "        if importance[i]>0:\n",
    "            print(round(importance[i], 2), counts[i], _idx, i)\n",
    "            sent_tokens = parse[_idx][\"sentences\"][i][\"tokens\"]\n",
    "            sent_deps = parse[_idx][\"sentences\"][i][\"enhancedPlusPlusDependencies\"]\n",
    "            print(\" \".join([s[\"lemma\"] for s in sent_tokens]))\n",
    "            sents = sent_tokenize(_raw[_idx])\n",
    "            if len(sents) > i:\n",
    "                print(sents[i])\n",
    "            else:\n",
    "                print(\"sent token differ\")\n",
    "            triples = relation.extract_triples_from_sent(sent_deps, sent_tokens, True)\n",
    "            for triple in triples:\n",
    "                if None in triple:\n",
    "                    print(triple)\n",
    "                    continue\n",
    "                s = [sent_tokens[i][\"originalText\"] for i in triple[0]]\n",
    "                p = [sent_tokens[i][\"originalText\"] for i in triple[1]]\n",
    "                o = [sent_tokens[i][\"originalText\"] for i in triple[2]]\n",
    "                print(\"{}-{}-{}\".format(s,p,o))\n",
    "            print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('patient', 223.06), ('medical', 143.69), ('health', 143.5), ('clinical', 139.72), ('treatment', 99.44), ('diagnosis', 86.78), ('disease', 81.07), ('risk', 79.0), ('record', 78.67), ('care', 68.06), ('friend', 47.64), ('healthcare', 42.31), ('cancer', 39.36), ('code', 36.28), ('data', 32.74), ('conclusion', 30.77), ('physician', 29.99), ('emr', 25.48), ('ehr', 25.13), ('doctor', 24.57)]\n",
      "1.88 hospital emr dataset-[consist of]-medical record of patient\n",
      "1.87 patient record-[contain]-valuable medical information\n",
      "1.58 propose PRSM-[recognize]-patient clinical state\n",
      "1.55 exist risk stratification technique-[predict]-overall risk of patient\n",
      "1.28 patient with disease-[be]-interested in competence\n",
      "1.28 patient with disease-[be]-interested in ethic\n",
      "1.28 result-[classify]-heart disease and patient\n",
      "1.0 result-[classify]-healthy patient\n",
      "1.0 additional implicit patient need-[use]-qualitative analysis\n",
      "1.0 patient-[be]-due for visit\n",
      "1.0 manifestation sub-category-[exist in]-t2dm patient\n",
      "1.0 470-[readmit]-patient with summary\n",
      "1.0 present study-[assess]-patient with result\n",
      "1.0 patient-[undergo]-PAP HPV DNA chip test\n",
      "1.0 persistent infection-[be]-higher in patient\n",
      "1.0 patient sub-profile and tier-[be]-coherent and informative and provide\n",
      "1.0 lda-[analyze]-thing patient\n",
      "1.0 lda-[analyze]-personal goal statement and patient\n",
      "1.0 pathway analysis-[ensure]-specialize standardized normalize sophisticated therapy procedure for patient\n",
      "1.0 we-[examine]-belief of patient\n",
      "0.87 method-[use]-other medical record\n",
      "0.87 method-[use]-other medical record\n",
      "0.87 obstetric electronic medical record-[contain]-massive amount of datum\n",
      "0.87 limited effort-[apply]-unstructured textual medical record\n",
      "0.87 many study-[use]-medical image and record\n",
      "0.87 many study-[use]-structured medical record\n",
      "0.87 medical record-[contain]-diagnostic information\n",
      "0.82 we-[leverage]-health care setting and assess\n",
      "0.82 we-[leverage]-health care setting and assess\n",
      "0.6 possible medical application in term-[provide]-addition\n",
      "0.6 possible medical application in term-[illustrate]-potential of model\n",
      "0.6 we-[use]-medical chronology\n",
      "0.6 latent health status group structure-[be]-responsible for co-occurrence\n",
      "0.6 Diabetes and complication-[recognize]-major public health threat\n",
      "0.6 Learning base deep learning method-[propose in]-heart disease diagnosis\n",
      "0.59 admit diagnosis in record-[reason from]-various source\n",
      "0.58 we-[examine]-clinical observation\n",
      "0.58 clinical finding-[change over]-time\n",
      "0.58 method-[generate]-cm clinical guideline for t2dm\n",
      "0.58 expand clinical information-[offer]-exciting opportunity\n",
      "0.58 we-[verify]-real clinical dataset\n",
      "0.58 we-[develop]-better data-driven clinical decision support system\n",
      "0.58 generalizability-[establish in]-other clinical cohort\n",
      "0.56 they-[portable to]-risk stratification task of disease\n",
      "0.43 procedure code-[correlate with]-diagnosis code\n",
      "0.4 discover treatment pattern-[help]-physician\n",
      "0.38 propose approach-[identify]-meaningful treatment pattern from emr\n",
      "0.38 we-[obtain]-treatment topic\n",
      "0.38 many datum mining technique-[discover]-latent treatment information for analysis\n",
      "0.38 approach-[discover]-meaningful latent treatment pattern but change\n",
      "0.38 approach-[discover]-implicit change of treatment\n",
      "0.38 treatment pattern-[discover from]-datum\n",
      "0.38 treatment pattern-[indicate]-latent correlation in pathway\n",
      "0.38 study-[extract]-latent treatment pattem\n",
      "0.36 that-[include]-healthcare insurance claim record\n",
      "0.31 we-[obtain]-meaningful diagnosis and topic\n",
      "0.31 we-[discover]-common cm diagnosis and knowledge for mellitus\n",
      "0.31 we-[propose]-allocation approach for diagnosis\n",
      "0.31 paper-[treat]-diagnosis assistant\n",
      "0.31 MCLDA-[predict]-miss medication or diagnosis\n",
      "0.28 we-[use]-disease topic proportion\n",
      "0.28 we-[explore]-give disease pattern\n",
      "0.28 topic modeling-[understand]-constitution of disease\n",
      "0.28 traditional topic model-[discover]-disease topic\n",
      "0.27 prsm and ws-prsm-[compare with]-established supervised risk stratification algorithm\n",
      "0.27 inclusion of topic-[allow]-accurate discrimination of individual at risk\n",
      "0.27 sub-profile-specific risk tier-[be]-coherent and informative and provide\n",
      "0.27 large volume of record-[produce]-rapid development of system\n",
      "0.27 we-[explore]-large volume of record\n",
      "0.27 MCLDA-[identify]-record\n",
      "0.12 they-[appreciate]-friend and family\n",
      "0.09 design of pathway-[challenge]-healthcare organizational setting\n",
      "0.09 healthcare institution-[accumulate]-large amount of datum\n",
      "0.09 information communication technology-[enable]-healthcare institution\n",
      "0.07 priority-[be]-cancer surgery and recovery\n",
      "0.07 priority-[be]-cancer surgery and recovery\n",
      "0.06 score-[validate in]-icd-9 dementia code\n",
      "0.06 prediction for code-[show]-considerable accuracy\n",
      "0.06 we-[explore]-conditional relationship of code\n",
      "0.06 traditional topic model-[treat]-document and code as word\n",
      "0.04 cohort-[derive]-testing data set\n",
      "0.04 we-[use]-social media data\n",
      "0.04 we-[propose]-data mining method\n",
      "0.04 latent Dirichlet allocation topic model-[fit to]-data set\n",
      "0.03 conclusion Social media-[reveal]-dynamic range of theme\n",
      "0.03 conclusion novel Big Data analytic-[offer]-possibility\n",
      "0.03 mixed method-[develop]-physician review topic taxonomy\n",
      "0.0 method-[use]-obstetric emr\n",
      "0.0 method-[use]-obstetric emr\n",
      "0.0 traditional topic model-[discover]-emr datum\n",
      "0.0 information extraction and assistant of emr-[be]-great significance\n",
      "total 208 triples, left 91\n"
     ]
    }
   ],
   "source": [
    "topic_triples = []\n",
    "print(top_terms[t_idx][:20])\n",
    "for top_doc in top_docs[t_idx]:\n",
    "    _idx = top_doc[0]\n",
    "    if type(parse[_idx]) is str:\n",
    "        print(_idx, \"parse\", \"err\")\n",
    "        continue\n",
    "    sents = relation.convert_parse2lemma_sents(parse[_idx])\n",
    "    sort_idxs, importance, counts = relation.extract_important_sents(sents, [x[0] for x in top_terms[t_idx][:20]], [x[1] for x in top_terms[t_idx][:20]])\n",
    "    for i in sort_idxs:\n",
    "        if importance[i]>0:\n",
    "            sent_tokens = parse[_idx][\"sentences\"][i][\"tokens\"]\n",
    "            sent_deps = parse[_idx][\"sentences\"][i][\"enhancedPlusPlusDependencies\"]\n",
    "            triples = relation.extract_triples_from_sent(sent_deps, sent_tokens, True)\n",
    "            for triple in triples:\n",
    "                if None in triple:\n",
    "                    continue\n",
    "                topic_triples.append([\n",
    "                    [sent_tokens[i][\"lemma\"] for i in triple[0]],\n",
    "                    [sent_tokens[i][\"lemma\"] for i in triple[1]],\n",
    "                    [sent_tokens[i][\"lemma\"] for i in triple[2]]\n",
    "                                     ])\n",
    "scores = []\n",
    "for t in topic_triples:\n",
    "    s = relation.evaluate_topic_triple(reduce(lambda a,b:a+b, t), [x[0] for x in top_terms[t_idx][:20]], [x[1] for x in top_terms[t_idx][:20]])\n",
    "    scores.append(s)\n",
    "left = 0\n",
    "for idx in np.argsort(scores)[-1::-1]:\n",
    "    if scores[idx]>0:\n",
    "        left+=1\n",
    "        print(round(scores[idx], 2), \n",
    "              \"{}-[{}]-{}\".format(\" \".join(topic_triples[idx][0]),\" \".join(topic_triples[idx][1]),\" \".join(topic_triples[idx][2])))\n",
    "print(\"total {} triples, left {}\".format(len(topic_triples), left))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "func get_phrases_by_pattern exec time: 0.36412763595581055\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('latent Dirichlet allocation', 1216),\n",
       " ('topic model', 692),\n",
       " ('experimental result', 431),\n",
       " ('topic modeling', 337),\n",
       " ('latent topic', 203),\n",
       " ('LDA model', 166),\n",
       " ('Dirichlet allocation', 158),\n",
       " ('short text', 123),\n",
       " ('Elsevier B.V. All rights', 108),\n",
       " ('sentiment analysis', 103),\n",
       " ('dirichlet allocation', 102),\n",
       " ('social media', 101),\n",
       " ('topic distribution', 93),\n",
       " ('social network', 91),\n",
       " ('recent year', 86),\n",
       " ('large number', 82),\n",
       " ('Elsevier Ltd.', 81),\n",
       " ('text mining', 79),\n",
       " ('Gibbs sampling', 77),\n",
       " ('case study', 73),\n",
       " ('information retrieval', 70),\n",
       " ('probabilistic topic model', 70),\n",
       " ('natural language processing', 66),\n",
       " ('previous work', 66),\n",
       " ('latent Dirichlet Allocation', 65),\n",
       " ('large amount', 61),\n",
       " ('visual word', 60),\n",
       " ('latent Dirichlet allocation model', 58),\n",
       " ('novel approach', 57),\n",
       " ('extensive experiment', 54),\n",
       " ('machine learning', 53),\n",
       " ('big datum', 53),\n",
       " ('better performance', 53),\n",
       " ('new method', 52),\n",
       " ('online review', 51),\n",
       " ('important role', 51),\n",
       " ('language model', 51),\n",
       " ('semantic analysis', 51),\n",
       " ('text document', 50),\n",
       " ('generative model', 50),\n",
       " ('textual datum', 50),\n",
       " ('different type', 49),\n",
       " ('different topic', 48),\n",
       " ('datum set', 46),\n",
       " ('new approach', 45),\n",
       " ('training datum', 45),\n",
       " ('source code', 45),\n",
       " ('probability distribution', 45),\n",
       " ('topic detection', 44),\n",
       " ('latent variable', 43),\n",
       " ('topic modelling', 42),\n",
       " ('large collection', 42),\n",
       " ('hidden topic', 41),\n",
       " ('word embedding', 41),\n",
       " ('text datum', 40),\n",
       " ('text classification', 40),\n",
       " ('large volume', 40),\n",
       " ('state-of-the-art method', 39),\n",
       " ('user interest', 39),\n",
       " ('significant improvement', 38),\n",
       " ('other hand', 37),\n",
       " ('support vector machine', 37),\n",
       " ('novel method', 37),\n",
       " ('natural language', 37),\n",
       " ('probabilistic model', 37),\n",
       " ('Elsevier Inc.', 36),\n",
       " ('challenging task', 36),\n",
       " ('latent Dirichlet Allocation model', 36),\n",
       " ('research topic', 34),\n",
       " ('text categorization', 34),\n",
       " ('model parameter', 34),\n",
       " ('document collection', 34),\n",
       " ('text corpora', 33),\n",
       " ('proposed system', 33),\n",
       " ('real-world dataset', 32),\n",
       " ('news article', 32),\n",
       " ('previous study', 32),\n",
       " ('prior knowledge', 32),\n",
       " ('topic modeling technique', 32),\n",
       " ('topic space', 32),\n",
       " ('same time', 31),\n",
       " ('user preference', 30),\n",
       " ('text analysis', 30),\n",
       " ('unsupervised manner', 30),\n",
       " ('support Vector machine', 29),\n",
       " ('semantic information', 29),\n",
       " ('sentiment classification', 29),\n",
       " ('bug report', 29),\n",
       " ('Twitter datum', 29),\n",
       " ('recommender system', 29),\n",
       " ('topic analysis', 29),\n",
       " ('classification accuracy', 28),\n",
       " ('hot topic', 28),\n",
       " ('empirical study', 27),\n",
       " ('neural network', 27),\n",
       " ('latent Dirichlet allocation topic model', 27),\n",
       " ('Semantic analysis', 27),\n",
       " ('recommendation system', 27),\n",
       " ('other method', 26),\n",
       " ('rapid development', 26),\n",
       " ('traditional method', 26),\n",
       " ('baseline method', 26),\n",
       " ('small number', 26),\n",
       " ('experiment result', 26),\n",
       " ('real dataset', 26),\n",
       " ('huge amount', 26),\n",
       " ('web service', 26),\n",
       " ('topic discovery', 26),\n",
       " ('parameter estimation', 26),\n",
       " ('many application', 25),\n",
       " ('topic modeling approach', 25),\n",
       " ('contextual information', 25),\n",
       " ('k-means clustering', 25),\n",
       " ('unsupervised learning', 25),\n",
       " ('topic modeling algorithm', 25),\n",
       " ('topic extraction', 25),\n",
       " ('real datum', 25),\n",
       " ('user behavior', 25),\n",
       " ('classification task', 24),\n",
       " ('word distribution', 24),\n",
       " ('useful information', 24),\n",
       " ('clustering algorithm', 24),\n",
       " ('document classification', 24),\n",
       " ('mixture model', 24),\n",
       " ('unstructured datum', 23),\n",
       " ('topic feature', 23),\n",
       " ('variational inference', 23),\n",
       " ('data set', 23),\n",
       " ('topic evolution', 23),\n",
       " ('specific topic', 22),\n",
       " ('unsupervised method', 22),\n",
       " ('relevant topic', 22),\n",
       " ('topic coherence', 22),\n",
       " ('much attention', 22),\n",
       " ('text mining technique', 22),\n",
       " ('clustering method', 22),\n",
       " ('high accuracy', 22),\n",
       " ('lda model', 22),\n",
       " ('classification performance', 22),\n",
       " ('scene classification', 22),\n",
       " ('topic structure', 21),\n",
       " ('wide range', 21),\n",
       " ('public opinion', 21),\n",
       " ('statistical model', 21),\n",
       " ('datum mining', 21),\n",
       " ('topic modeling method', 21),\n",
       " ('rapid growth', 21),\n",
       " ('conventional method', 21),\n",
       " ('dimensionality reduction', 21),\n",
       " ('naive baye', 20),\n",
       " ('empirical result', 20),\n",
       " ('main topic', 20),\n",
       " ('better understanding', 20),\n",
       " ('latent topic model', 20),\n",
       " ('social media data', 20),\n",
       " ('topic information', 20),\n",
       " ('class label', 20),\n",
       " ('topic proportion', 20),\n",
       " ('video sequence', 20),\n",
       " ('software system', 20),\n",
       " ('similarity measure', 19),\n",
       " ('speech recognition', 19),\n",
       " ('feature extraction', 19),\n",
       " ('statistical method', 19),\n",
       " ('posterior distribution', 19),\n",
       " ('document representation', 19),\n",
       " ('topic vector', 19),\n",
       " ('different level', 19),\n",
       " ('good performance', 19),\n",
       " ('opinion mining', 19),\n",
       " ('search engine', 19),\n",
       " ('text corpus', 19),\n",
       " ('web page', 19),\n",
       " ('urban area', 19),\n",
       " ('LDA algorithm', 19),\n",
       " ('first step', 18),\n",
       " ('textual information', 18),\n",
       " ('large set', 18),\n",
       " ('scientific literature', 18),\n",
       " ('different aspect', 18),\n",
       " ('Probabilistic topic model', 18),\n",
       " ('% improvement', 18),\n",
       " ('classification method', 18),\n",
       " ('main contribution', 18),\n",
       " ('important task', 18),\n",
       " ('systematic review', 18),\n",
       " ('promising result', 18),\n",
       " ('topic word', 18),\n",
       " ('feature space', 18),\n",
       " ('extract topic', 18),\n",
       " ('tv program', 18),\n",
       " ('e. g.', 18),\n",
       " ('previous research', 17),\n",
       " ('LDA topic model', 17),\n",
       " ('machine learning method', 17),\n",
       " ('Topic Model', 17),\n",
       " ('valuable information', 17),\n",
       " ('semantic relationship', 17),\n",
       " ('novel framework', 17),\n",
       " ('content analysis', 17),\n",
       " ('spatial information', 17),\n",
       " ('new model', 17),\n",
       " ('Social media', 17),\n",
       " ('dirichlet distribution', 17),\n",
       " ('image classification', 17),\n",
       " ('semantic relation', 17),\n",
       " ('document clustering', 17),\n",
       " ('better result', 17),\n",
       " ('tag recommendation', 17),\n",
       " ('topic trend', 17),\n",
       " ('multinomial distribution', 17),\n",
       " ('computer vision', 17),\n",
       " ('semantic feature', 17),\n",
       " ('effective method', 17),\n",
       " ('experimental evaluation', 17),\n",
       " ('meaningful topic', 17),\n",
       " ('lda method', 17),\n",
       " ('classification algorithm', 16),\n",
       " ('discrete datum', 16),\n",
       " ('textual document', 16),\n",
       " ('relevant document', 16),\n",
       " ('large dataset', 16),\n",
       " ('different domain', 16),\n",
       " ('state-of-the-art approach', 16),\n",
       " ('policy maker', 16),\n",
       " ('inference algorithm', 16),\n",
       " ('graphical model', 16),\n",
       " ('social network analysis', 16),\n",
       " ('daily life', 16),\n",
       " ('information need', 16),\n",
       " ('bayesian model', 16),\n",
       " ('multiple topic', 16),\n",
       " ('matrix factorization', 16),\n",
       " ('new topic model', 16),\n",
       " ('small set', 16),\n",
       " ('higher accuracy', 16),\n",
       " ('training set', 16),\n",
       " ('probabilistic topic modeling', 15),\n",
       " ('deep learning', 15),\n",
       " ('United States', 15),\n",
       " ('generative process', 15),\n",
       " ('novel model', 15),\n",
       " ('Stack Overflow', 15),\n",
       " ('visual feature', 15),\n",
       " ('new topic', 15),\n",
       " ('user-generated content', 15),\n",
       " ('textual content', 15),\n",
       " ('allocation model', 15),\n",
       " ('predictive performance', 15),\n",
       " ('semantic meaning', 15),\n",
       " ('word frequency', 15),\n",
       " ('data analysis', 15),\n",
       " ('benchmark dataset', 15),\n",
       " ('traditional approach', 15),\n",
       " ('search query', 15),\n",
       " ('community structure', 15),\n",
       " ('relevant information', 15),\n",
       " ('social interaction', 15),\n",
       " ('latent Semantic analysis', 15),\n",
       " ('feature selection', 15),\n",
       " ('text mining method', 14),\n",
       " ('traditional topic model', 14),\n",
       " ('latent Dirichlet allocation method', 14),\n",
       " ('vector space model', 14),\n",
       " ('other approach', 14),\n",
       " ('large scale', 14),\n",
       " ('behavior pattern', 14),\n",
       " ('real time', 14),\n",
       " ('past decade', 14),\n",
       " ('topic-word distribution', 14),\n",
       " ('topic correlation', 14),\n",
       " ('real-world datum', 14),\n",
       " ('hierarchical structure', 14),\n",
       " ('large corpora', 14),\n",
       " ('large corpus', 14),\n",
       " ('lda topic', 14),\n",
       " ('previous method', 14),\n",
       " ('important information', 14),\n",
       " ('best result', 14),\n",
       " ('blog post', 14),\n",
       " ('research papers', 13),\n",
       " ('lda analysis', 13),\n",
       " ('artificial intelligence', 13),\n",
       " ('coherent topic', 13),\n",
       " ('topic probability', 13),\n",
       " ('word vector', 13),\n",
       " ('research trend', 13),\n",
       " ('future research', 13),\n",
       " ('powerful tool', 13),\n",
       " ('research field', 13),\n",
       " ('more attention', 13),\n",
       " ('efficient method', 13),\n",
       " ('important issue', 13),\n",
       " ('computer science', 13),\n",
       " ('human activity', 13),\n",
       " ('word co-occurrence', 13),\n",
       " ('competitive performance', 13),\n",
       " ('human behavior', 13),\n",
       " ('novel topic model', 13),\n",
       " ('best performance', 13),\n",
       " ('latent Dirichlet allocation algorithm', 13),\n",
       " ('satellite image', 13),\n",
       " ('document level', 13),\n",
       " ('text collection', 13),\n",
       " ('topic representation', 13),\n",
       " ('user query', 13),\n",
       " ('review text', 12),\n",
       " ('term frequency', 12),\n",
       " ('research community', 12),\n",
       " ('such datum', 12),\n",
       " ('information technology', 12),\n",
       " ('customer satisfaction', 12),\n",
       " ('product review', 12),\n",
       " ('climate change', 12),\n",
       " ('heart disease', 12),\n",
       " ('learning method', 12),\n",
       " ('different language', 12),\n",
       " ('inference method', 12),\n",
       " ('mobile device', 12),\n",
       " ('similar topic', 12),\n",
       " ('context information', 12),\n",
       " ('social media post', 12),\n",
       " ('appropriate number', 12),\n",
       " ('online community', 12),\n",
       " ('topic hierarchy', 12),\n",
       " ('social event', 12),\n",
       " ('latent semantics', 12),\n",
       " ('vast amount', 12),\n",
       " ('latent structure', 12),\n",
       " ('Cohen d', 12),\n",
       " ('Delta u', 12),\n",
       " ('specific domain', 12),\n",
       " ('standard lda', 12),\n",
       " ('posterior inference', 12),\n",
       " ('topic number', 12),\n",
       " ('new feature', 12),\n",
       " ('semantic gap', 12),\n",
       " ('topic cluster', 12),\n",
       " ('feature word', 12),\n",
       " ('important problem', 12),\n",
       " ('general framework', 12),\n",
       " ('abnormal event', 12),\n",
       " ('hierarchical model', 12),\n",
       " ('color harmony model', 12),\n",
       " ('text information', 11),\n",
       " ('major challenge', 11),\n",
       " ('superior performance', 11),\n",
       " ('various aspect', 11),\n",
       " ('massive amount', 11),\n",
       " ('various topic', 11),\n",
       " ('unsupervised approach', 11),\n",
       " ('effective way', 11),\n",
       " ('temporal trend', 11),\n",
       " ('good result', 11),\n",
       " ('great challenge', 11),\n",
       " ('Markov chain Monte Carlo', 11),\n",
       " ('synthetic datum', 11),\n",
       " ('future work', 11),\n",
       " ('real world', 11),\n",
       " ('real world dataset', 11),\n",
       " ('statistical analysis', 11),\n",
       " ('other topic model', 11),\n",
       " ('dialogue system', 11),\n",
       " ('topic similarity', 11),\n",
       " ('long text', 11),\n",
       " ('semantic similarity', 11),\n",
       " ('vector space', 11),\n",
       " ('different method', 11),\n",
       " ('datum source', 11),\n",
       " ('search result', 11),\n",
       " ('clinical pathway', 11),\n",
       " ('evaluation result', 11),\n",
       " ('similar user', 11),\n",
       " ('bag-of-word assumption', 11),\n",
       " ('several topic', 11),\n",
       " ('time series', 11),\n",
       " ('user profile', 11),\n",
       " ('service recommendation', 11),\n",
       " ('unlabeled datum', 11),\n",
       " ('topic identification', 11),\n",
       " ('various type', 11),\n",
       " ('topic level', 11),\n",
       " ('image datum', 11),\n",
       " ('such model', 11),\n",
       " ('high resolution', 11),\n",
       " ('motion pattern', 11),\n",
       " ('image clustering', 11),\n",
       " ('singular value decomposition', 11),\n",
       " ('image annotation', 11),\n",
       " ('other user', 11),\n",
       " ('human action', 11),\n",
       " ('Topic model', 11),\n",
       " ('atomic activity', 11),\n",
       " ('social datum', 10),\n",
       " ('topic quality', 10),\n",
       " ('consumer review', 10),\n",
       " ('popular topic', 10),\n",
       " ('text feature', 10),\n",
       " ('user review', 10),\n",
       " ('such information', 10),\n",
       " ('non-negative Matrix factorization', 10),\n",
       " ('high dimensionality', 10),\n",
       " ('main idea', 10),\n",
       " ('efficient way', 10),\n",
       " ('dirichlet allocation model', 10),\n",
       " ('Practical implication', 10),\n",
       " ('word representation', 10),\n",
       " ('most case', 10),\n",
       " ('present study', 10),\n",
       " ('public dataset', 10),\n",
       " ('topic label', 10),\n",
       " ('many topic', 10),\n",
       " ('user experience', 10),\n",
       " ('electronic health record', 10),\n",
       " ('fault diagnosis', 10),\n",
       " ('individual user', 10),\n",
       " ('effective approach', 10),\n",
       " ('related topic', 10),\n",
       " ('real-world application', 10),\n",
       " ('test datum', 10),\n",
       " ('online datum', 10),\n",
       " ('word order', 10),\n",
       " ('fundamental problem', 10),\n",
       " ('random mixture', 10),\n",
       " ('online algorithm', 10),\n",
       " ('document corpus', 10),\n",
       " ('large document collection', 10),\n",
       " ('Gibbs sampling algorithm', 10),\n",
       " ('support Vector Machine', 10),\n",
       " ('baseline model', 10),\n",
       " ('mutual information', 10),\n",
       " ('few study', 10),\n",
       " ('domain knowledge', 10),\n",
       " ('point cloud', 10),\n",
       " ('k-means algorithm', 10),\n",
       " ('hierarchical clustering', 10),\n",
       " ('empirical evaluation', 10),\n",
       " ('image segmentation', 10),\n",
       " ('content information', 10),\n",
       " ('discriminative power', 10),\n",
       " ('temporal information', 10),\n",
       " ('multimodal datum', 10),\n",
       " ('retrieval performance', 10),\n",
       " ('hierarchical Dirichlet', 10),\n",
       " ('learning process', 10),\n",
       " ('certain topic', 10),\n",
       " ('evaluation experiment', 10),\n",
       " ('semantic space', 10),\n",
       " ('word image', 10),\n",
       " ('original LDA', 10),\n",
       " ('challenging issue', 10),\n",
       " ('customer review', 10),\n",
       " ('genetic algorithm', 10),\n",
       " ('image collection', 10),\n",
       " ('digital library', 10),\n",
       " ('low-level feature', 10),\n",
       " ('oov pn', 10),\n",
       " ('query photo', 10),\n",
       " ('topic assignment', 10),\n",
       " ('bayesian inference', 10),\n",
       " ('text stream', 10),\n",
       " ('word error rate', 10),\n",
       " ('conversational content', 10),\n",
       " ('large text corpora', 10),\n",
       " ('author-topic model', 10),\n",
       " ('super user', 10),\n",
       " ('trained model', 10),\n",
       " ('object recognition', 10),\n",
       " ('mobility pattern', 10),\n",
       " ('hybrid approach', 10),\n",
       " ('feature location', 10),\n",
       " ('product feature', 10),\n",
       " ('different word', 9),\n",
       " ('machine learning technique', 9),\n",
       " ('scientific publication', 9),\n",
       " ('other model', 9),\n",
       " ('conventional topic model', 9),\n",
       " ('sparsity problem', 9),\n",
       " ('negative sentiment', 9),\n",
       " ('research work', 9),\n",
       " ('such topic', 9),\n",
       " ('prediction accuracy', 9),\n",
       " ('image retrieval', 9),\n",
       " ('different way', 9),\n",
       " ('meaningful information', 9),\n",
       " ('recommendation list', 9),\n",
       " ('datum collection', 9),\n",
       " ('topic-based representation', 9),\n",
       " ('promising performance', 9),\n",
       " ('cosine similarity', 9),\n",
       " ('structured datum', 9),\n",
       " ('knowledge discovery', 9),\n",
       " ('variable model', 9),\n",
       " ('current study', 9),\n",
       " ('experimental study', 9),\n",
       " ('other word', 9),\n",
       " ('various domain', 9),\n",
       " ('different kind', 9),\n",
       " ('key factor', 9),\n",
       " ('qualitative analysis', 9),\n",
       " ('predictive model', 9),\n",
       " ('information gain', 9),\n",
       " ('textual feature', 9),\n",
       " ('social media platform', 9),\n",
       " ('generative topic model', 9),\n",
       " ('semantic relatedness', 9),\n",
       " ('corresponding topic', 9),\n",
       " ('prediction performance', 9),\n",
       " ('classification problem', 9),\n",
       " ('clinical report', 9),\n",
       " ('standard topic model', 9),\n",
       " ('latent Dirichlet Allocation Model', 9),\n",
       " ('long time', 9),\n",
       " ('promising approach', 9),\n",
       " ('comparative analysis', 9),\n",
       " ('weighting scheme', 9),\n",
       " ('semantic concept', 9),\n",
       " ('other type', 9),\n",
       " ('state-of-the-art performance', 9),\n",
       " ('recommendation approach', 9),\n",
       " ('Wikipedia article', 9),\n",
       " ('patent datum', 9),\n",
       " ('different number', 9),\n",
       " ('vector representation', 9),\n",
       " ('different dataset', 9),\n",
       " ('topic network', 9),\n",
       " ('training corpus', 9),\n",
       " ('multimodal information', 9),\n",
       " ('time complexity', 9),\n",
       " ('latent factor model', 9),\n",
       " ('document modeling', 9),\n",
       " ('semantic topic', 9),\n",
       " ('training dataset', 9),\n",
       " ('Vector Space Model', 9),\n",
       " ('Gibbs sampler', 9),\n",
       " ('efficient algorithm', 9),\n",
       " ('% accuracy', 9),\n",
       " ('vocabulary size', 9),\n",
       " ('proper name', 9),\n",
       " ('John Wiley', 9),\n",
       " ('variational method', 9),\n",
       " ('domain expert', 9),\n",
       " ('web document', 9),\n",
       " ('preliminary result', 9),\n",
       " ('second method', 9),\n",
       " ('scene recognition', 9),\n",
       " ('hsr imagery', 9),\n",
       " ('previous approach', 9),\n",
       " ('training image', 9),\n",
       " ('image feature', 9),\n",
       " ('traffic incident', 9),\n",
       " ('current research', 8),\n",
       " ('large quantity', 8),\n",
       " ('related word', 8),\n",
       " ('key topic', 8),\n",
       " ('regression model', 8),\n",
       " ('news topic', 8),\n",
       " ('explosive growth', 8),\n",
       " ('key feature', 8),\n",
       " ('recall rate', 8),\n",
       " ('sentiment word', 8),\n",
       " ('overall rating', 8),\n",
       " ('non-negative matrix factorization', 8),\n",
       " ('many study', 8),\n",
       " ('logistic regression', 8),\n",
       " ('service quality', 8),\n",
       " ('word cloud', 8),\n",
       " ('data-driven approach', 8),\n",
       " ('deep learning method', 8),\n",
       " ('software artifact', 8),\n",
       " ('personalized recommendation', 8),\n",
       " ('different region', 8),\n",
       " ('other topic', 8),\n",
       " ('core topic', 8),\n",
       " ('semantic structure', 8),\n",
       " ('text mining approach', 8),\n",
       " ('semantic coherence', 8),\n",
       " ('high level', 8),\n",
       " ('recent study', 8),\n",
       " ('further research', 8),\n",
       " ('quantitative analysis', 8),\n",
       " ('classification model', 8),\n",
       " ('new way', 8),\n",
       " ('clustering approach', 8),\n",
       " ('feature vector', 8),\n",
       " ('latent aspect', 8),\n",
       " ('feature set', 8),\n",
       " ('large datum set', 8),\n",
       " ('first study', 8),\n",
       " ('sparse coding', 8),\n",
       " ('document analysis', 8),\n",
       " ('topic-based approach', 8),\n",
       " ('Dirichlet Allocation model', 8),\n",
       " ('topic-based model', 8),\n",
       " ('difficult task', 8),\n",
       " ('important source', 8),\n",
       " ('several study', 8),\n",
       " ('text clustering', 8),\n",
       " ('different weight', 8),\n",
       " ('low-dimensional representation', 8),\n",
       " ('machine learning algorithm', 8),\n",
       " ('satisfactory result', 8),\n",
       " ('major topic', 8),\n",
       " ('input datum', 8),\n",
       " ('unsupervised machine', 8),\n",
       " ('weak hypothesis', 8),\n",
       " ('diabetes mellitus', 8),\n",
       " ('encouraging result', 8),\n",
       " ('different perspective', 8),\n",
       " ('topic classification', 8),\n",
       " ('new type', 8),\n",
       " ('huge number', 8),\n",
       " ('optimal number', 8),\n",
       " ('Dirichlet Allocation', 8),\n",
       " ('unseen datum', 8),\n",
       " ('Hidden Markov Model', 8),\n",
       " ('probabilistic distribution', 8),\n",
       " ('classification result', 8),\n",
       " ('human topic ranking', 8),\n",
       " ('Dirichlet prior', 8),\n",
       " ('challenging problem', 8),\n",
       " ('Markov model', 8),\n",
       " ('citation network', 8),\n",
       " ('program comprehension', 8),\n",
       " ('unstructured text', 8),\n",
       " ('community detection', 8),\n",
       " ('state-of-the-art baseline', 8),\n",
       " ('newspaper article', 8),\n",
       " ('clustering technique', 8),\n",
       " ('input image', 8),\n",
       " ('potential topic', 8),\n",
       " ('latent topic structure', 8),\n",
       " ('unsupervised topic model', 8),\n",
       " ('alternative method', 8),\n",
       " ('Dirichlet process', 8),\n",
       " ('international student', 8),\n",
       " ('semantic indexing', 8),\n",
       " ('different user', 8),\n",
       " ('self-driving car', 8),\n",
       " ('recommendation performance', 8),\n",
       " ('software maintenance', 8),\n",
       " ('topic modelling method', 8),\n",
       " ('valuable source', 8),\n",
       " ('latent factor', 8),\n",
       " ('Twitter user', 8),\n",
       " ('social tag', 8),\n",
       " ('web image', 8),\n",
       " ('Information Retrieval', 8),\n",
       " ('important part', 8),\n",
       " ('higher precision', 8),\n",
       " ('topic change', 8),\n",
       " ('Markov chain', 8),\n",
       " ('user context', 8),\n",
       " ('first method', 8),\n",
       " ('alternative approach', 8),\n",
       " ('active user', 8),\n",
       " ('variational baye', 8),\n",
       " ('common crawl', 8),\n",
       " ('classification approach', 8),\n",
       " ('similar interest', 8),\n",
       " ('software repository', 8),\n",
       " ('treatment pattern', 8),\n",
       " ('lda-based technique', 8),\n",
       " ('time point', 8),\n",
       " ('probability model', 8),\n",
       " ('natural language text', 8),\n",
       " ('document retrieval', 8),\n",
       " ('tv user', 8),\n",
       " ('datum sparseness', 8),\n",
       " ('optical flow', 8),\n",
       " ('Great East Japan Earthquake', 8),\n",
       " ('life style', 8),\n",
       " ('scene class', 8),\n",
       " ('activity routine', 8),\n",
       " ('reliable user', 8),\n",
       " ('clinical trial protocol', 8),\n",
       " ('tag-topic model', 8),\n",
       " ('cluster ensemble', 8),\n",
       " ('visual analytic', 7),\n",
       " ('tourist attraction', 7),\n",
       " ('common topic', 7),\n",
       " ('time period', 7),\n",
       " ('second step', 7),\n",
       " ('thematic structure', 7),\n",
       " ('future study', 7),\n",
       " ('user study', 7),\n",
       " ('information propagation', 7),\n",
       " ('co-occurrence pattern', 7),\n",
       " ('novel technique', 7),\n",
       " ('pattern recognition', 7),\n",
       " ('simulation study', 7),\n",
       " ('retrieval result', 7),\n",
       " ('urban planning', 7),\n",
       " ('machine translation', 7),\n",
       " ('software project', 7),\n",
       " ('significant challenge', 7),\n",
       " ('state-of-the-art model', 7),\n",
       " ('document frequency', 7),\n",
       " ('Linguistic Inquiry', 7),\n",
       " ('dominant topic', 7),\n",
       " ('cold topic', 7),\n",
       " ('research gap', 7),\n",
       " ('latent interest', 7),\n",
       " ('online user', 7),\n",
       " ('empirical analysis', 7),\n",
       " ('current state', 7),\n",
       " ('mixture distribution', 7),\n",
       " ('available online', 7),\n",
       " ('hidden knowledge', 7),\n",
       " ('human perception', 7),\n",
       " ('trend analysis', 7),\n",
       " ('mental health', 7),\n",
       " ('computational efficiency', 7),\n",
       " ('transfer learning', 7),\n",
       " ('relevant study', 7),\n",
       " ('practical implication', 7),\n",
       " ('current approach', 7),\n",
       " ('effective tool', 7),\n",
       " ('various method', 7),\n",
       " ('urgent care center', 7),\n",
       " ('Dirichlet allocation model', 7),\n",
       " ('predictive power', 7),\n",
       " ('service description', 7),\n",
       " ('problem list', 7),\n",
       " ('great deal', 7),\n",
       " ('patent document', 7),\n",
       " ('naive Bayes', 7),\n",
       " ('same event', 7),\n",
       " ('allocation topic model', 7),\n",
       " ('target user', 7),\n",
       " ('mining process', 7),\n",
       " ('many method', 7),\n",
       " ('visual information', 7),\n",
       " ('semantic level', 7),\n",
       " ('embedding vector', 7),\n",
       " ('spectral clustering', 7),\n",
       " ('multiple feature', 7),\n",
       " ('recent work', 7),\n",
       " ('last decade', 7),\n",
       " ('fraud detection', 7),\n",
       " ('sentiment polarity', 7),\n",
       " ('significant role', 7),\n",
       " ('input document', 7),\n",
       " ('limited number', 7),\n",
       " ('probabilistic modeling', 7),\n",
       " ('potential value', 7),\n",
       " ('text message', 7),\n",
       " ('supervised learning', 7),\n",
       " ('unsupervised fashion', 7),\n",
       " ('linguistic feature', 7),\n",
       " ('fake review', 7),\n",
       " ('other disease', 7),\n",
       " ('ground truth', 7),\n",
       " ('such method', 7),\n",
       " ('proposed approach', 7),\n",
       " ('supervised information', 7),\n",
       " ('useful insight', 7),\n",
       " ('many case', 7),\n",
       " ('literature review', 7),\n",
       " ('heterogeneous network', 7),\n",
       " ('document-topic distribution', 7),\n",
       " ('semantic representation', 7),\n",
       " ('overall accuracy', 7),\n",
       " ('new perspective', 7),\n",
       " ('full-text datum', 7),\n",
       " ('different size', 7),\n",
       " ('bow representation', 7),\n",
       " ('computational linguistics', 7),\n",
       " ('social image', 7),\n",
       " ('many researcher', 7),\n",
       " ('content-based method', 7),\n",
       " ('word level', 7),\n",
       " ('real world datum', 7),\n",
       " ('further analysis', 7),\n",
       " ('training sample', 7),\n",
       " ('new challenge', 7),\n",
       " ('receiver operating', 7),\n",
       " ('language modeling', 7),\n",
       " ('image patch', 7),\n",
       " ('research area', 7),\n",
       " ('different feature', 7),\n",
       " ('Big Data', 7),\n",
       " ('complementary information', 7),\n",
       " ('topic segmentation', 7),\n",
       " ('open problem', 7),\n",
       " ('new algorithm', 7),\n",
       " ('recent research', 7),\n",
       " ('further improvement', 7),\n",
       " ('space complexity', 7),\n",
       " ('software developer', 7),\n",
       " ('software evolution', 7),\n",
       " ('service composition', 7),\n",
       " ('available datum', 7),\n",
       " ('traditional LDA model', 7),\n",
       " ('topic clustering', 7),\n",
       " ('indonesian text', 7),\n",
       " ('web content', 7),\n",
       " ('Wall Street Journal', 7),\n",
       " ('various source', 7),\n",
       " ('textual review', 7),\n",
       " ('computational time', 7),\n",
       " ('hsr image', 7),\n",
       " ('computational model', 7),\n",
       " ('spatial pattern', 7),\n",
       " ('object concept', 7),\n",
       " ('recent advance', 7),\n",
       " ('sentiment lexicon', 7),\n",
       " ('activity recognition', 7),\n",
       " ('particular topic', 7),\n",
       " ('time dimension', 7),\n",
       " ('multi-document summarization', 7),\n",
       " ('expert finding system', 7),\n",
       " ('other domain', 7),\n",
       " ('keyword extraction', 7),\n",
       " ('automatic speech recognition', 7),\n",
       " ('mixture weight', 7),\n",
       " ('new question', 7),\n",
       " ('novel algorithm', 7),\n",
       " ('latent Dirichlet', 7),\n",
       " ('risk stratification', 7),\n",
       " ('mobile application', 7),\n",
       " ('eligibility criterion', 7),\n",
       " ('temporal feature', 7),\n",
       " ('wide variety', 7),\n",
       " ('posterior probability', 7),\n",
       " ('optimization problem', 7),\n",
       " ('automatic transcription', 7),\n",
       " ('mesh term', 7),\n",
       " ('patient trace', 7),\n",
       " ('lda-based method', 7),\n",
       " ('document cluster', 7),\n",
       " ('patient record', 7),\n",
       " ('risk type', 7),\n",
       " ('datum point', 7),\n",
       " ('web api', 7),\n",
       " ('same topic', 7),\n",
       " ('feature descriptor', 7),\n",
       " ('dimension reduction', 7),\n",
       " ('curriculum learning', 7),\n",
       " ('service ecosystem', 7),\n",
       " ('social circle', 7),\n",
       " ('target domain', 7),\n",
       " ('social tagging system', 7),\n",
       " ('human motion', 7),\n",
       " ('unigram scaling', 7),\n",
       " ('co-occurrence event', 7),\n",
       " ('drive topic', 7),\n",
       " ('Kullback-Leibler divergence', 7),\n",
       " ('unsupervised topic modeling', 6),\n",
       " ('research domain', 6),\n",
       " ('various application', 6),\n",
       " ('destination image', 6),\n",
       " ('overall performance', 6),\n",
       " ('local context', 6),\n",
       " ('real datum set', 6),\n",
       " ('CNN model', 6),\n",
       " ('news text', 6),\n",
       " ('precision rate', 6),\n",
       " ('digital document', 6),\n",
       " ('benchmark datum set', 6),\n",
       " ('unsupervised way', 6),\n",
       " ('learning model', 6),\n",
       " ('last year', 6),\n",
       " ('time frame', 6),\n",
       " ('research question', 6),\n",
       " ('higher performance', 6),\n",
       " ('temporal dynamics', 6),\n",
       " ('large-scale dataset', 6),\n",
       " ('k-means clustering algorithm', 6),\n",
       " ('Word Count', 6),\n",
       " ('supervised lda', 6),\n",
       " ('compact representation', 6),\n",
       " ('annotation performance', 6),\n",
       " ('intelligent system', 6),\n",
       " ('many people', 6),\n",
       " ('topic modeling process', 6),\n",
       " ('integral part', 6),\n",
       " ('% ci', 6),\n",
       " ('text representation', 6),\n",
       " ('acoustic feature', 6),\n",
       " ('sample size', 6),\n",
       " ('datum size', 6),\n",
       " ('patent analysis', 6),\n",
       " ('software engineering', 6),\n",
       " ('network analysis', 6),\n",
       " ('distinct topic', 6),\n",
       " ('similarity metric', 6),\n",
       " ('popular topic model', 6),\n",
       " ('public sentiment', 6),\n",
       " ('spatial distribution', 6),\n",
       " ('dynamic topic model', 6),\n",
       " ('latent topic distribution', 6),\n",
       " ('correlation analysis', 6),\n",
       " ('domain adaptation', 6),\n",
       " ('main challenge', 6),\n",
       " ('point cluster', 6),\n",
       " ('decision making', 6),\n",
       " ('word model', 6),\n",
       " ('significant change', 6),\n",
       " ('high quality', 6),\n",
       " ('latent pattern', 6),\n",
       " ('comprehensive experiment', 6),\n",
       " ('quantitative evaluation', 6),\n",
       " ('specific aspect', 6),\n",
       " ('service discovery', 6),\n",
       " ('relevant feature', 6),\n",
       " ('main goal', 6),\n",
       " ('keyword search', 6),\n",
       " ('important tool', 6),\n",
       " ('cryptocurrency investment', 6),\n",
       " ('nonnegative matrix factorization', 6),\n",
       " ('privacy policy', 6),\n",
       " ('feature representation', 6),\n",
       " ('recommendation model', 6),\n",
       " ('Gibbs sampling method', 6),\n",
       " ('news item', 6),\n",
       " ('great significance', 6),\n",
       " ('important component', 6),\n",
       " ('semantic dependency', 6),\n",
       " ('online course', 6),\n",
       " ('paragraph vector', 6),\n",
       " ('latent theme', 6),\n",
       " ('real-life dataset', 6),\n",
       " ('biomedical researcher', 6),\n",
       " ('research direction', 6),\n",
       " ('main research area', 6),\n",
       " ('New York City', 6),\n",
       " ('significant increase', 6),\n",
       " ('active learning', 6),\n",
       " ('online news website', 6),\n",
       " ('key component', 6),\n",
       " ('bag-of-word model', 6),\n",
       " ('information sharing', 6),\n",
       " ('background knowledge', 6),\n",
       " ('lda approach', 6),\n",
       " ('smart city', 6),\n",
       " ('different time period', 6),\n",
       " ('ad video', 6),\n",
       " ('synthetic aperture radar', 6),\n",
       " ('asymmetrical prior', 6),\n",
       " ('supervised learning method', 6),\n",
       " ('online product review', 6),\n",
       " ('content similarity', 6),\n",
       " ('spatial relation', 6),\n",
       " ('probabilistic approach', 6),\n",
       " ('full use', 6),\n",
       " ('complex network', 6),\n",
       " ('ranking function', 6),\n",
       " ('search space', 6),\n",
       " ('web service description', 6),\n",
       " ('high probability', 6),\n",
       " ('hidden structure', 6),\n",
       " ('multiple source', 6),\n",
       " ('topic stability', 6),\n",
       " ('small amount', 6),\n",
       " ('topic layer', 6),\n",
       " ('different scale', 6),\n",
       " ('new framework', 6),\n",
       " ('cold start problem', 6),\n",
       " ('great success', 6),\n",
       " ('open source project', 6),\n",
       " ('response variable', 6),\n",
       " ('variational approximation', 6),\n",
       " ('important factor', 6),\n",
       " ('sensor datum', 6),\n",
       " ('continuous datum', 6),\n",
       " ('hierarchical topic model', 6),\n",
       " ('model perplexity', 6),\n",
       " ('count matrix', 6),\n",
       " ('training process', 6),\n",
       " ('little attention', 6),\n",
       " ('sentiment-aware topic', 6),\n",
       " ('pseudo document', 6),\n",
       " ('automatic method', 6),\n",
       " ('target word', 6),\n",
       " ('support Vector Machines', 6),\n",
       " ('business process', 6),\n",
       " ('concurrency topic', 6),\n",
       " ('computation cost', 6),\n",
       " ('novel way', 6),\n",
       " ('single topic', 6),\n",
       " ('high-dimensional datum', 6),\n",
       " ('financial news', 6),\n",
       " ('popular method', 6),\n",
       " ('contextual feature', 6),\n",
       " ('single document', 6),\n",
       " ('other language', 6),\n",
       " ('user activity', 6),\n",
       " ('lda-based approach', 6),\n",
       " ('supervised method', 6),\n",
       " ('average accuracy', 6),\n",
       " ('crowded scene', 6),\n",
       " ('many domain', 6),\n",
       " ('high performance', 6),\n",
       " ('large archive', 6),\n",
       " ('media platform', 6),\n",
       " ('movie review', 6),\n",
       " ('co-occurrence relation', 6),\n",
       " ('many field', 6),\n",
       " ('difficult problem', 6),\n",
       " ...]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ps = relation.get_phrases_by_pattern(parse)\n",
    "sorted(dict(Counter([\" \".join(x) for x in ps])).items(), key=lambda x:x[1], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lda362",
   "language": "python",
   "name": "lda362"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
