{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not use stanford CoreNLP client!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading sentiwordnet: <urlopen error [Errno 111]\n",
      "[nltk_data]     Connection refused>\n"
     ]
    }
   ],
   "source": [
    "import preprocess as pp\n",
    "import configs\n",
    "import persister\n",
    "import relation\n",
    "import lda\n",
    "import vis\n",
    "import evaluate\n",
    "from utils import *\n",
    "\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "from functools import reduce\n",
    "\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk.tree import Tree\n",
    "from nltk import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load abs\n"
     ]
    }
   ],
   "source": [
    "# is_news = 1\n",
    "\n",
    "# l = 7\n",
    "# t = 4\n",
    "# m = \"c_v\"\n",
    "# i = 200\n",
    "# min_df = 1\n",
    "\n",
    "is_news = 0\n",
    "l = 6\n",
    "t = 14\n",
    "m = \"c_v\"\n",
    "i = 200\n",
    "min_df = 1\n",
    "\n",
    "# load\n",
    "if is_news:\n",
    "    _raw = persister.load_json(configs.RAWNEWS)\n",
    "    parse = persister.read_parse()\n",
    "    _input = persister.read_input(configs.NEWSINPUT)\n",
    "    model_name = configs.NEWSMODEL.format(l, t, m, i, min_df)\n",
    "    terms, doc_topic, topic_word = persister.read_lda(configs.NEWSLDA.format(model_name))\n",
    "    model = persister.load_model(model_name)\n",
    "    vec = persister.load_model(configs.NEWSVEC.format(min_df))\n",
    "    print(\"load news\")\n",
    "else:\n",
    "    _raw = persister.load_json(configs.RAWABSTRACT)\n",
    "    _input = persister.read_input(configs.ABSTRACTINPUT)\n",
    "    model_name = configs.ABSTRACTMODEL.format(l, t, m, i, min_df)\n",
    "    terms, doc_topic, topic_word = persister.read_lda(configs.ABSTRACTLDA.format(model_name))\n",
    "    parse = persister.read_parse(configs.ABSTRACTPARSE)\n",
    "    model = persister.load_model(model_name)\n",
    "    vec = persister.load_model(configs.ABSVEC.format(min_df))\n",
    "    print(\"load abs\")\n",
    "tf = vec.fit_transform(_input)\n",
    "top_terms, top_docs = lda.get_topics(topic_word, terms, doc_topic)\n",
    "df_top_words, df_top_docs = lda.pd_topics_vis(top_terms, top_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10453"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(terms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "docs distribution: {9: 152, 5: 33, 11: 969, 12: 1330, 1: 7, 13: 3, 10: 76, 3: 3, 0: 8, 7: 14, 6: 1}\n"
     ]
    }
   ],
   "source": [
    "distr = lda.get_dominant_topic(doc_topic)\n",
    "print(\"docs distribution:\",dict(Counter(distr)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top terms info:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Word 0</th>\n",
       "      <th>Word 1</th>\n",
       "      <th>Word 2</th>\n",
       "      <th>Word 3</th>\n",
       "      <th>Word 4</th>\n",
       "      <th>Word 5</th>\n",
       "      <th>Word 6</th>\n",
       "      <th>Word 7</th>\n",
       "      <th>Word 8</th>\n",
       "      <th>Word 9</th>\n",
       "      <th>Word 10</th>\n",
       "      <th>Word 11</th>\n",
       "      <th>Word 12</th>\n",
       "      <th>Word 13</th>\n",
       "      <th>Word 14</th>\n",
       "      <th>Word 15</th>\n",
       "      <th>Word 16</th>\n",
       "      <th>Word 17</th>\n",
       "      <th>Word 18</th>\n",
       "      <th>Word 19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>(3730, 123.1)</td>\n",
       "      <td>(2699, 96.87)</td>\n",
       "      <td>(3253, 78.19)</td>\n",
       "      <td>(2522, 77.49)</td>\n",
       "      <td>(7395, 75.45)</td>\n",
       "      <td>(895, 64.39)</td>\n",
       "      <td>(6752, 51.34)</td>\n",
       "      <td>(1245, 51.19)</td>\n",
       "      <td>(4333, 48.51)</td>\n",
       "      <td>(888, 47.03)</td>\n",
       "      <td>(1071, 40.99)</td>\n",
       "      <td>(107, 40.76)</td>\n",
       "      <td>(8958, 39.19)</td>\n",
       "      <td>(466, 37.74)</td>\n",
       "      <td>(5491, 31.56)</td>\n",
       "      <td>(5967, 29.42)</td>\n",
       "      <td>(7771, 29.31)</td>\n",
       "      <td>(1696, 28.59)</td>\n",
       "      <td>(9501, 26.52)</td>\n",
       "      <td>(7766, 25.93)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>(6755, 223.06)</td>\n",
       "      <td>(5621, 143.69)</td>\n",
       "      <td>(4041, 143.5)</td>\n",
       "      <td>(1426, 139.72)</td>\n",
       "      <td>(9624, 99.44)</td>\n",
       "      <td>(2398, 86.78)</td>\n",
       "      <td>(2522, 81.07)</td>\n",
       "      <td>(8012, 79.0)</td>\n",
       "      <td>(7666, 78.67)</td>\n",
       "      <td>(1171, 68.06)</td>\n",
       "      <td>(3624, 47.64)</td>\n",
       "      <td>(4042, 42.31)</td>\n",
       "      <td>(1146, 39.36)</td>\n",
       "      <td>(1490, 36.28)</td>\n",
       "      <td>(2161, 32.74)</td>\n",
       "      <td>(1696, 30.77)</td>\n",
       "      <td>(6926, 29.99)</td>\n",
       "      <td>(2935, 25.48)</td>\n",
       "      <td>(2836, 25.13)</td>\n",
       "      <td>(2633, 24.57)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>(5788, 42.64)</td>\n",
       "      <td>(6839, 25.17)</td>\n",
       "      <td>(7529, 13.82)</td>\n",
       "      <td>(4168, 9.76)</td>\n",
       "      <td>(256, 8.0)</td>\n",
       "      <td>(7408, 7.74)</td>\n",
       "      <td>(5407, 7.74)</td>\n",
       "      <td>(5085, 7.05)</td>\n",
       "      <td>(599, 6.96)</td>\n",
       "      <td>(1303, 6.65)</td>\n",
       "      <td>(10434, 6.65)</td>\n",
       "      <td>(5137, 6.22)</td>\n",
       "      <td>(7265, 5.86)</td>\n",
       "      <td>(5844, 5.86)</td>\n",
       "      <td>(4352, 5.13)</td>\n",
       "      <td>(1730, 5.01)</td>\n",
       "      <td>(9584, 5.0)</td>\n",
       "      <td>(6774, 4.14)</td>\n",
       "      <td>(9263, 4.12)</td>\n",
       "      <td>(8352, 4.12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>(2920, 181.73)</td>\n",
       "      <td>(2689, 87.04)</td>\n",
       "      <td>(8961, 86.19)</td>\n",
       "      <td>(8739, 55.34)</td>\n",
       "      <td>(3757, 39.85)</td>\n",
       "      <td>(7653, 38.09)</td>\n",
       "      <td>(8708, 34.84)</td>\n",
       "      <td>(8847, 33.33)</td>\n",
       "      <td>(10381, 31.62)</td>\n",
       "      <td>(5081, 30.16)</td>\n",
       "      <td>(84, 30.1)</td>\n",
       "      <td>(2693, 29.08)</td>\n",
       "      <td>(2691, 27.25)</td>\n",
       "      <td>(3082, 26.57)</td>\n",
       "      <td>(8560, 24.96)</td>\n",
       "      <td>(10078, 24.49)</td>\n",
       "      <td>(684, 22.67)</td>\n",
       "      <td>(6250, 22.09)</td>\n",
       "      <td>(674, 21.59)</td>\n",
       "      <td>(2533, 21.36)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>(8129, 79.5)</td>\n",
       "      <td>(6679, 73.15)</td>\n",
       "      <td>(3803, 67.68)</td>\n",
       "      <td>(8271, 57.19)</td>\n",
       "      <td>(2579, 52.59)</td>\n",
       "      <td>(1527, 42.46)</td>\n",
       "      <td>(1455, 33.26)</td>\n",
       "      <td>(1003, 32.4)</td>\n",
       "      <td>(1021, 31.0)</td>\n",
       "      <td>(5090, 30.54)</td>\n",
       "      <td>(2018, 29.71)</td>\n",
       "      <td>(8165, 29.68)</td>\n",
       "      <td>(9489, 29.34)</td>\n",
       "      <td>(481, 28.55)</td>\n",
       "      <td>(5646, 27.44)</td>\n",
       "      <td>(3858, 27.32)</td>\n",
       "      <td>(8128, 26.77)</td>\n",
       "      <td>(5551, 23.32)</td>\n",
       "      <td>(620, 23.12)</td>\n",
       "      <td>(5719, 22.88)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 5</th>\n",
       "      <td>(7969, 537.88)</td>\n",
       "      <td>(8339, 474.31)</td>\n",
       "      <td>(6502, 265.32)</td>\n",
       "      <td>(9716, 235.55)</td>\n",
       "      <td>(7297, 228.45)</td>\n",
       "      <td>(7103, 206.48)</td>\n",
       "      <td>(6471, 196.94)</td>\n",
       "      <td>(9508, 194.3)</td>\n",
       "      <td>(350, 180.66)</td>\n",
       "      <td>(9724, 174.52)</td>\n",
       "      <td>(5615, 171.56)</td>\n",
       "      <td>(559, 166.01)</td>\n",
       "      <td>(2109, 159.65)</td>\n",
       "      <td>(7581, 142.65)</td>\n",
       "      <td>(1812, 111.75)</td>\n",
       "      <td>(8626, 101.7)</td>\n",
       "      <td>(9991, 101.33)</td>\n",
       "      <td>(6138, 88.63)</td>\n",
       "      <td>(7096, 85.19)</td>\n",
       "      <td>(8958, 81.34)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 6</th>\n",
       "      <td>(8659, 81.12)</td>\n",
       "      <td>(651, 74.02)</td>\n",
       "      <td>(6058, 57.13)</td>\n",
       "      <td>(7010, 35.01)</td>\n",
       "      <td>(1051, 28.77)</td>\n",
       "      <td>(6479, 24.22)</td>\n",
       "      <td>(4526, 19.81)</td>\n",
       "      <td>(6059, 18.49)</td>\n",
       "      <td>(5409, 17.49)</td>\n",
       "      <td>(3092, 16.39)</td>\n",
       "      <td>(10382, 14.97)</td>\n",
       "      <td>(1578, 14.53)</td>\n",
       "      <td>(8986, 13.59)</td>\n",
       "      <td>(4285, 11.69)</td>\n",
       "      <td>(2814, 10.71)</td>\n",
       "      <td>(5957, 10.39)</td>\n",
       "      <td>(4869, 9.79)</td>\n",
       "      <td>(7954, 9.73)</td>\n",
       "      <td>(7831, 9.62)</td>\n",
       "      <td>(9448, 8.87)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 7</th>\n",
       "      <td>(8639, 268.18)</td>\n",
       "      <td>(7518, 203.92)</td>\n",
       "      <td>(1490, 149.7)</td>\n",
       "      <td>(2376, 143.61)</td>\n",
       "      <td>(8669, 143.52)</td>\n",
       "      <td>(1076, 139.43)</td>\n",
       "      <td>(7834, 120.26)</td>\n",
       "      <td>(7336, 87.55)</td>\n",
       "      <td>(7322, 68.98)</td>\n",
       "      <td>(411, 65.81)</td>\n",
       "      <td>(7840, 53.22)</td>\n",
       "      <td>(7865, 49.86)</td>\n",
       "      <td>(3137, 49.8)</td>\n",
       "      <td>(9280, 48.97)</td>\n",
       "      <td>(6483, 44.34)</td>\n",
       "      <td>(8958, 43.24)</td>\n",
       "      <td>(5388, 40.78)</td>\n",
       "      <td>(8810, 33.35)</td>\n",
       "      <td>(1296, 30.56)</td>\n",
       "      <td>(6211, 30.05)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 8</th>\n",
       "      <td>(5310, 75.09)</td>\n",
       "      <td>(3592, 55.04)</td>\n",
       "      <td>(5228, 31.56)</td>\n",
       "      <td>(562, 28.23)</td>\n",
       "      <td>(8928, 22.76)</td>\n",
       "      <td>(5151, 18.42)</td>\n",
       "      <td>(158, 18.41)</td>\n",
       "      <td>(8022, 17.93)</td>\n",
       "      <td>(5, 15.75)</td>\n",
       "      <td>(172, 15.54)</td>\n",
       "      <td>(113, 15.29)</td>\n",
       "      <td>(5389, 13.09)</td>\n",
       "      <td>(2882, 13.06)</td>\n",
       "      <td>(4503, 12.3)</td>\n",
       "      <td>(641, 11.6)</td>\n",
       "      <td>(8121, 11.02)</td>\n",
       "      <td>(10215, 10.98)</td>\n",
       "      <td>(3877, 10.91)</td>\n",
       "      <td>(4946, 10.27)</td>\n",
       "      <td>(2625, 10.12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 9</th>\n",
       "      <td>(9508, 893.39)</td>\n",
       "      <td>(7874, 662.45)</td>\n",
       "      <td>(8958, 393.56)</td>\n",
       "      <td>(350, 331.47)</td>\n",
       "      <td>(9991, 254.09)</td>\n",
       "      <td>(533, 239.38)</td>\n",
       "      <td>(4333, 225.84)</td>\n",
       "      <td>(9286, 215.99)</td>\n",
       "      <td>(9632, 201.75)</td>\n",
       "      <td>(5296, 154.3)</td>\n",
       "      <td>(3409, 150.61)</td>\n",
       "      <td>(9344, 137.1)</td>\n",
       "      <td>(6669, 135.72)</td>\n",
       "      <td>(5108, 131.54)</td>\n",
       "      <td>(6739, 131.04)</td>\n",
       "      <td>(7876, 130.56)</td>\n",
       "      <td>(8204, 129.8)</td>\n",
       "      <td>(505, 125.64)</td>\n",
       "      <td>(2468, 123.53)</td>\n",
       "      <td>(292, 122.98)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 10</th>\n",
       "      <td>(4387, 923.95)</td>\n",
       "      <td>(3374, 404.79)</td>\n",
       "      <td>(6375, 384.84)</td>\n",
       "      <td>(10120, 298.9)</td>\n",
       "      <td>(10151, 296.7)</td>\n",
       "      <td>(8179, 291.52)</td>\n",
       "      <td>(7374, 268.69)</td>\n",
       "      <td>(5694, 208.2)</td>\n",
       "      <td>(5200, 190.16)</td>\n",
       "      <td>(5862, 189.69)</td>\n",
       "      <td>(8701, 171.52)</td>\n",
       "      <td>(7730, 157.06)</td>\n",
       "      <td>(397, 151.42)</td>\n",
       "      <td>(1403, 148.49)</td>\n",
       "      <td>(4249, 143.97)</td>\n",
       "      <td>(7653, 138.61)</td>\n",
       "      <td>(7935, 128.93)</td>\n",
       "      <td>(3593, 127.79)</td>\n",
       "      <td>(8282, 122.98)</td>\n",
       "      <td>(5934, 117.66)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 11</th>\n",
       "      <td>(9997, 1748.72)</td>\n",
       "      <td>(9991, 1591.63)</td>\n",
       "      <td>(2172, 1381.07)</td>\n",
       "      <td>(9508, 1103.26)</td>\n",
       "      <td>(4579, 1036.7)</td>\n",
       "      <td>(5694, 971.08)</td>\n",
       "      <td>(5108, 941.61)</td>\n",
       "      <td>(7374, 858.59)</td>\n",
       "      <td>(292, 801.15)</td>\n",
       "      <td>(8626, 798.08)</td>\n",
       "      <td>(472, 753.49)</td>\n",
       "      <td>(2468, 750.28)</td>\n",
       "      <td>(5862, 704.64)</td>\n",
       "      <td>(7935, 701.73)</td>\n",
       "      <td>(762, 692.76)</td>\n",
       "      <td>(6169, 607.05)</td>\n",
       "      <td>(5134, 589.42)</td>\n",
       "      <td>(8369, 569.67)</td>\n",
       "      <td>(6668, 561.59)</td>\n",
       "      <td>(9449, 553.23)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 12</th>\n",
       "      <td>(9508, 5025.38)</td>\n",
       "      <td>(5862, 4893.76)</td>\n",
       "      <td>(5134, 2687.1)</td>\n",
       "      <td>(5108, 2152.96)</td>\n",
       "      <td>(2636, 1992.37)</td>\n",
       "      <td>(9991, 1943.68)</td>\n",
       "      <td>(5694, 1839.26)</td>\n",
       "      <td>(10349, 1773.7)</td>\n",
       "      <td>(2468, 1564.05)</td>\n",
       "      <td>(7374, 1468.27)</td>\n",
       "      <td>(9344, 1357.69)</td>\n",
       "      <td>(292, 1355.55)</td>\n",
       "      <td>(7935, 1055.09)</td>\n",
       "      <td>(762, 997.56)</td>\n",
       "      <td>(472, 918.45)</td>\n",
       "      <td>(6668, 905.81)</td>\n",
       "      <td>(278, 897.52)</td>\n",
       "      <td>(2172, 891.62)</td>\n",
       "      <td>(3374, 814.18)</td>\n",
       "      <td>(4579, 704.53)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 13</th>\n",
       "      <td>(4017, 94.97)</td>\n",
       "      <td>(5643, 54.98)</td>\n",
       "      <td>(1288, 51.44)</td>\n",
       "      <td>(8066, 49.23)</td>\n",
       "      <td>(3664, 38.29)</td>\n",
       "      <td>(1179, 35.55)</td>\n",
       "      <td>(5909, 35.55)</td>\n",
       "      <td>(7079, 30.26)</td>\n",
       "      <td>(4459, 26.4)</td>\n",
       "      <td>(9573, 24.96)</td>\n",
       "      <td>(8476, 23.26)</td>\n",
       "      <td>(7021, 23.24)</td>\n",
       "      <td>(2787, 22.99)</td>\n",
       "      <td>(6852, 22.4)</td>\n",
       "      <td>(9467, 21.34)</td>\n",
       "      <td>(4282, 21.08)</td>\n",
       "      <td>(1645, 20.08)</td>\n",
       "      <td>(10445, 19.08)</td>\n",
       "      <td>(8523, 18.45)</td>\n",
       "      <td>(9731, 17.93)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Word 0           Word 1           Word 2           Word 3  \\\n",
       "Topic 0     (3730, 123.1)    (2699, 96.87)    (3253, 78.19)    (2522, 77.49)   \n",
       "Topic 1    (6755, 223.06)   (5621, 143.69)    (4041, 143.5)   (1426, 139.72)   \n",
       "Topic 2     (5788, 42.64)    (6839, 25.17)    (7529, 13.82)     (4168, 9.76)   \n",
       "Topic 3    (2920, 181.73)    (2689, 87.04)    (8961, 86.19)    (8739, 55.34)   \n",
       "Topic 4      (8129, 79.5)    (6679, 73.15)    (3803, 67.68)    (8271, 57.19)   \n",
       "Topic 5    (7969, 537.88)   (8339, 474.31)   (6502, 265.32)   (9716, 235.55)   \n",
       "Topic 6     (8659, 81.12)     (651, 74.02)    (6058, 57.13)    (7010, 35.01)   \n",
       "Topic 7    (8639, 268.18)   (7518, 203.92)    (1490, 149.7)   (2376, 143.61)   \n",
       "Topic 8     (5310, 75.09)    (3592, 55.04)    (5228, 31.56)     (562, 28.23)   \n",
       "Topic 9    (9508, 893.39)   (7874, 662.45)   (8958, 393.56)    (350, 331.47)   \n",
       "Topic 10   (4387, 923.95)   (3374, 404.79)   (6375, 384.84)   (10120, 298.9)   \n",
       "Topic 11  (9997, 1748.72)  (9991, 1591.63)  (2172, 1381.07)  (9508, 1103.26)   \n",
       "Topic 12  (9508, 5025.38)  (5862, 4893.76)   (5134, 2687.1)  (5108, 2152.96)   \n",
       "Topic 13    (4017, 94.97)    (5643, 54.98)    (1288, 51.44)    (8066, 49.23)   \n",
       "\n",
       "                   Word 4           Word 5           Word 6           Word 7  \\\n",
       "Topic 0     (7395, 75.45)     (895, 64.39)    (6752, 51.34)    (1245, 51.19)   \n",
       "Topic 1     (9624, 99.44)    (2398, 86.78)    (2522, 81.07)     (8012, 79.0)   \n",
       "Topic 2        (256, 8.0)     (7408, 7.74)     (5407, 7.74)     (5085, 7.05)   \n",
       "Topic 3     (3757, 39.85)    (7653, 38.09)    (8708, 34.84)    (8847, 33.33)   \n",
       "Topic 4     (2579, 52.59)    (1527, 42.46)    (1455, 33.26)     (1003, 32.4)   \n",
       "Topic 5    (7297, 228.45)   (7103, 206.48)   (6471, 196.94)    (9508, 194.3)   \n",
       "Topic 6     (1051, 28.77)    (6479, 24.22)    (4526, 19.81)    (6059, 18.49)   \n",
       "Topic 7    (8669, 143.52)   (1076, 139.43)   (7834, 120.26)    (7336, 87.55)   \n",
       "Topic 8     (8928, 22.76)    (5151, 18.42)     (158, 18.41)    (8022, 17.93)   \n",
       "Topic 9    (9991, 254.09)    (533, 239.38)   (4333, 225.84)   (9286, 215.99)   \n",
       "Topic 10   (10151, 296.7)   (8179, 291.52)   (7374, 268.69)    (5694, 208.2)   \n",
       "Topic 11   (4579, 1036.7)   (5694, 971.08)   (5108, 941.61)   (7374, 858.59)   \n",
       "Topic 12  (2636, 1992.37)  (9991, 1943.68)  (5694, 1839.26)  (10349, 1773.7)   \n",
       "Topic 13    (3664, 38.29)    (1179, 35.55)    (5909, 35.55)    (7079, 30.26)   \n",
       "\n",
       "                   Word 8           Word 9          Word 10         Word 11  \\\n",
       "Topic 0     (4333, 48.51)     (888, 47.03)    (1071, 40.99)    (107, 40.76)   \n",
       "Topic 1     (7666, 78.67)    (1171, 68.06)    (3624, 47.64)   (4042, 42.31)   \n",
       "Topic 2       (599, 6.96)     (1303, 6.65)    (10434, 6.65)    (5137, 6.22)   \n",
       "Topic 3    (10381, 31.62)    (5081, 30.16)       (84, 30.1)   (2693, 29.08)   \n",
       "Topic 4      (1021, 31.0)    (5090, 30.54)    (2018, 29.71)   (8165, 29.68)   \n",
       "Topic 5     (350, 180.66)   (9724, 174.52)   (5615, 171.56)   (559, 166.01)   \n",
       "Topic 6     (5409, 17.49)    (3092, 16.39)   (10382, 14.97)   (1578, 14.53)   \n",
       "Topic 7     (7322, 68.98)     (411, 65.81)    (7840, 53.22)   (7865, 49.86)   \n",
       "Topic 8        (5, 15.75)     (172, 15.54)     (113, 15.29)   (5389, 13.09)   \n",
       "Topic 9    (9632, 201.75)    (5296, 154.3)   (3409, 150.61)   (9344, 137.1)   \n",
       "Topic 10   (5200, 190.16)   (5862, 189.69)   (8701, 171.52)  (7730, 157.06)   \n",
       "Topic 11    (292, 801.15)   (8626, 798.08)    (472, 753.49)  (2468, 750.28)   \n",
       "Topic 12  (2468, 1564.05)  (7374, 1468.27)  (9344, 1357.69)  (292, 1355.55)   \n",
       "Topic 13     (4459, 26.4)    (9573, 24.96)    (8476, 23.26)   (7021, 23.24)   \n",
       "\n",
       "                  Word 12         Word 13         Word 14         Word 15  \\\n",
       "Topic 0     (8958, 39.19)    (466, 37.74)   (5491, 31.56)   (5967, 29.42)   \n",
       "Topic 1     (1146, 39.36)   (1490, 36.28)   (2161, 32.74)   (1696, 30.77)   \n",
       "Topic 2      (7265, 5.86)    (5844, 5.86)    (4352, 5.13)    (1730, 5.01)   \n",
       "Topic 3     (2691, 27.25)   (3082, 26.57)   (8560, 24.96)  (10078, 24.49)   \n",
       "Topic 4     (9489, 29.34)    (481, 28.55)   (5646, 27.44)   (3858, 27.32)   \n",
       "Topic 5    (2109, 159.65)  (7581, 142.65)  (1812, 111.75)   (8626, 101.7)   \n",
       "Topic 6     (8986, 13.59)   (4285, 11.69)   (2814, 10.71)   (5957, 10.39)   \n",
       "Topic 7      (3137, 49.8)   (9280, 48.97)   (6483, 44.34)   (8958, 43.24)   \n",
       "Topic 8     (2882, 13.06)    (4503, 12.3)     (641, 11.6)   (8121, 11.02)   \n",
       "Topic 9    (6669, 135.72)  (5108, 131.54)  (6739, 131.04)  (7876, 130.56)   \n",
       "Topic 10    (397, 151.42)  (1403, 148.49)  (4249, 143.97)  (7653, 138.61)   \n",
       "Topic 11   (5862, 704.64)  (7935, 701.73)   (762, 692.76)  (6169, 607.05)   \n",
       "Topic 12  (7935, 1055.09)   (762, 997.56)   (472, 918.45)  (6668, 905.81)   \n",
       "Topic 13    (2787, 22.99)    (6852, 22.4)   (9467, 21.34)   (4282, 21.08)   \n",
       "\n",
       "                 Word 16         Word 17         Word 18         Word 19  \n",
       "Topic 0    (7771, 29.31)   (1696, 28.59)   (9501, 26.52)   (7766, 25.93)  \n",
       "Topic 1    (6926, 29.99)   (2935, 25.48)   (2836, 25.13)   (2633, 24.57)  \n",
       "Topic 2      (9584, 5.0)    (6774, 4.14)    (9263, 4.12)    (8352, 4.12)  \n",
       "Topic 3     (684, 22.67)   (6250, 22.09)    (674, 21.59)   (2533, 21.36)  \n",
       "Topic 4    (8128, 26.77)   (5551, 23.32)    (620, 23.12)   (5719, 22.88)  \n",
       "Topic 5   (9991, 101.33)   (6138, 88.63)   (7096, 85.19)   (8958, 81.34)  \n",
       "Topic 6     (4869, 9.79)    (7954, 9.73)    (7831, 9.62)    (9448, 8.87)  \n",
       "Topic 7    (5388, 40.78)   (8810, 33.35)   (1296, 30.56)   (6211, 30.05)  \n",
       "Topic 8   (10215, 10.98)   (3877, 10.91)   (4946, 10.27)   (2625, 10.12)  \n",
       "Topic 9    (8204, 129.8)   (505, 125.64)  (2468, 123.53)   (292, 122.98)  \n",
       "Topic 10  (7935, 128.93)  (3593, 127.79)  (8282, 122.98)  (5934, 117.66)  \n",
       "Topic 11  (5134, 589.42)  (8369, 569.67)  (6668, 561.59)  (9449, 553.23)  \n",
       "Topic 12   (278, 897.52)  (2172, 891.62)  (3374, 814.18)  (4579, 704.53)  \n",
       "Topic 13   (1645, 20.08)  (10445, 19.08)   (8523, 18.45)   (9731, 17.93)  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"top terms info:\")\n",
    "df_top_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top docs info:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Topic 0</th>\n",
       "      <td>(198, 0.548)</td>\n",
       "      <td>(263, 0.493)</td>\n",
       "      <td>(107, 0.407)</td>\n",
       "      <td>(1764, 0.386)</td>\n",
       "      <td>(2009, 0.383)</td>\n",
       "      <td>(1770, 0.371)</td>\n",
       "      <td>(1740, 0.361)</td>\n",
       "      <td>(894, 0.346)</td>\n",
       "      <td>(896, 0.334)</td>\n",
       "      <td>(65, 0.32)</td>\n",
       "      <td>(751, 0.318)</td>\n",
       "      <td>(725, 0.318)</td>\n",
       "      <td>(122, 0.314)</td>\n",
       "      <td>(52, 0.285)</td>\n",
       "      <td>(2577, 0.271)</td>\n",
       "      <td>(562, 0.264)</td>\n",
       "      <td>(526, 0.263)</td>\n",
       "      <td>(50, 0.257)</td>\n",
       "      <td>(685, 0.247)</td>\n",
       "      <td>(2248, 0.245)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 1</th>\n",
       "      <td>(489, 0.751)</td>\n",
       "      <td>(142, 0.651)</td>\n",
       "      <td>(115, 0.587)</td>\n",
       "      <td>(722, 0.42)</td>\n",
       "      <td>(1566, 0.37)</td>\n",
       "      <td>(1101, 0.364)</td>\n",
       "      <td>(1112, 0.346)</td>\n",
       "      <td>(80, 0.346)</td>\n",
       "      <td>(32, 0.345)</td>\n",
       "      <td>(627, 0.339)</td>\n",
       "      <td>(415, 0.319)</td>\n",
       "      <td>(2304, 0.319)</td>\n",
       "      <td>(1037, 0.313)</td>\n",
       "      <td>(1446, 0.291)</td>\n",
       "      <td>(1402, 0.291)</td>\n",
       "      <td>(1129, 0.281)</td>\n",
       "      <td>(174, 0.277)</td>\n",
       "      <td>(64, 0.276)</td>\n",
       "      <td>(338, 0.275)</td>\n",
       "      <td>(1624, 0.266)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 2</th>\n",
       "      <td>(1855, 0.23)</td>\n",
       "      <td>(1319, 0.208)</td>\n",
       "      <td>(1400, 0.199)</td>\n",
       "      <td>(872, 0.198)</td>\n",
       "      <td>(1309, 0.196)</td>\n",
       "      <td>(1079, 0.187)</td>\n",
       "      <td>(1340, 0.151)</td>\n",
       "      <td>(1310, 0.146)</td>\n",
       "      <td>(1164, 0.133)</td>\n",
       "      <td>(1350, 0.131)</td>\n",
       "      <td>(2158, 0.115)</td>\n",
       "      <td>(2386, 0.114)</td>\n",
       "      <td>(2188, 0.112)</td>\n",
       "      <td>(1242, 0.11)</td>\n",
       "      <td>(2220, 0.107)</td>\n",
       "      <td>(1868, 0.105)</td>\n",
       "      <td>(1932, 0.104)</td>\n",
       "      <td>(2422, 0.102)</td>\n",
       "      <td>(2457, 0.075)</td>\n",
       "      <td>(1876, 0.074)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 3</th>\n",
       "      <td>(81, 0.685)</td>\n",
       "      <td>(874, 0.517)</td>\n",
       "      <td>(109, 0.374)</td>\n",
       "      <td>(771, 0.349)</td>\n",
       "      <td>(18, 0.338)</td>\n",
       "      <td>(195, 0.293)</td>\n",
       "      <td>(1178, 0.287)</td>\n",
       "      <td>(344, 0.285)</td>\n",
       "      <td>(1281, 0.265)</td>\n",
       "      <td>(1813, 0.255)</td>\n",
       "      <td>(171, 0.254)</td>\n",
       "      <td>(1422, 0.254)</td>\n",
       "      <td>(1761, 0.244)</td>\n",
       "      <td>(1306, 0.24)</td>\n",
       "      <td>(1266, 0.238)</td>\n",
       "      <td>(737, 0.233)</td>\n",
       "      <td>(1278, 0.228)</td>\n",
       "      <td>(113, 0.222)</td>\n",
       "      <td>(1175, 0.22)</td>\n",
       "      <td>(365, 0.216)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 4</th>\n",
       "      <td>(508, 0.371)</td>\n",
       "      <td>(461, 0.346)</td>\n",
       "      <td>(413, 0.333)</td>\n",
       "      <td>(98, 0.282)</td>\n",
       "      <td>(1076, 0.27)</td>\n",
       "      <td>(720, 0.268)</td>\n",
       "      <td>(740, 0.268)</td>\n",
       "      <td>(741, 0.268)</td>\n",
       "      <td>(1666, 0.252)</td>\n",
       "      <td>(606, 0.248)</td>\n",
       "      <td>(693, 0.247)</td>\n",
       "      <td>(2027, 0.247)</td>\n",
       "      <td>(1379, 0.246)</td>\n",
       "      <td>(82, 0.241)</td>\n",
       "      <td>(2447, 0.231)</td>\n",
       "      <td>(886, 0.226)</td>\n",
       "      <td>(89, 0.225)</td>\n",
       "      <td>(1273, 0.217)</td>\n",
       "      <td>(450, 0.215)</td>\n",
       "      <td>(56, 0.206)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 5</th>\n",
       "      <td>(85, 0.951)</td>\n",
       "      <td>(57, 0.727)</td>\n",
       "      <td>(386, 0.669)</td>\n",
       "      <td>(111, 0.651)</td>\n",
       "      <td>(1, 0.631)</td>\n",
       "      <td>(575, 0.57)</td>\n",
       "      <td>(434, 0.518)</td>\n",
       "      <td>(698, 0.513)</td>\n",
       "      <td>(1577, 0.493)</td>\n",
       "      <td>(786, 0.486)</td>\n",
       "      <td>(103, 0.48)</td>\n",
       "      <td>(59, 0.471)</td>\n",
       "      <td>(6, 0.466)</td>\n",
       "      <td>(270, 0.455)</td>\n",
       "      <td>(957, 0.454)</td>\n",
       "      <td>(2272, 0.452)</td>\n",
       "      <td>(67, 0.452)</td>\n",
       "      <td>(973, 0.451)</td>\n",
       "      <td>(410, 0.435)</td>\n",
       "      <td>(1936, 0.42)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 6</th>\n",
       "      <td>(1570, 0.465)</td>\n",
       "      <td>(1117, 0.415)</td>\n",
       "      <td>(1606, 0.337)</td>\n",
       "      <td>(477, 0.302)</td>\n",
       "      <td>(757, 0.276)</td>\n",
       "      <td>(970, 0.254)</td>\n",
       "      <td>(1874, 0.252)</td>\n",
       "      <td>(1167, 0.169)</td>\n",
       "      <td>(1520, 0.166)</td>\n",
       "      <td>(684, 0.162)</td>\n",
       "      <td>(1668, 0.15)</td>\n",
       "      <td>(854, 0.147)</td>\n",
       "      <td>(2526, 0.144)</td>\n",
       "      <td>(702, 0.141)</td>\n",
       "      <td>(581, 0.141)</td>\n",
       "      <td>(1021, 0.131)</td>\n",
       "      <td>(2325, 0.126)</td>\n",
       "      <td>(2396, 0.125)</td>\n",
       "      <td>(1720, 0.124)</td>\n",
       "      <td>(995, 0.12)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 7</th>\n",
       "      <td>(1586, 0.575)</td>\n",
       "      <td>(747, 0.526)</td>\n",
       "      <td>(224, 0.503)</td>\n",
       "      <td>(831, 0.447)</td>\n",
       "      <td>(453, 0.428)</td>\n",
       "      <td>(898, 0.427)</td>\n",
       "      <td>(20, 0.397)</td>\n",
       "      <td>(1594, 0.396)</td>\n",
       "      <td>(731, 0.392)</td>\n",
       "      <td>(2283, 0.386)</td>\n",
       "      <td>(380, 0.385)</td>\n",
       "      <td>(1473, 0.382)</td>\n",
       "      <td>(891, 0.379)</td>\n",
       "      <td>(1181, 0.377)</td>\n",
       "      <td>(2312, 0.376)</td>\n",
       "      <td>(2533, 0.375)</td>\n",
       "      <td>(945, 0.371)</td>\n",
       "      <td>(2467, 0.368)</td>\n",
       "      <td>(752, 0.357)</td>\n",
       "      <td>(141, 0.357)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 8</th>\n",
       "      <td>(441, 0.215)</td>\n",
       "      <td>(1958, 0.173)</td>\n",
       "      <td>(890, 0.172)</td>\n",
       "      <td>(626, 0.172)</td>\n",
       "      <td>(2121, 0.167)</td>\n",
       "      <td>(2184, 0.165)</td>\n",
       "      <td>(70, 0.161)</td>\n",
       "      <td>(802, 0.16)</td>\n",
       "      <td>(1442, 0.157)</td>\n",
       "      <td>(273, 0.152)</td>\n",
       "      <td>(2561, 0.148)</td>\n",
       "      <td>(1343, 0.147)</td>\n",
       "      <td>(1440, 0.137)</td>\n",
       "      <td>(576, 0.136)</td>\n",
       "      <td>(2505, 0.129)</td>\n",
       "      <td>(2233, 0.125)</td>\n",
       "      <td>(1882, 0.123)</td>\n",
       "      <td>(849, 0.114)</td>\n",
       "      <td>(2522, 0.113)</td>\n",
       "      <td>(621, 0.108)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 9</th>\n",
       "      <td>(2, 0.994)</td>\n",
       "      <td>(41, 0.9)</td>\n",
       "      <td>(1044, 0.871)</td>\n",
       "      <td>(317, 0.861)</td>\n",
       "      <td>(1013, 0.836)</td>\n",
       "      <td>(97, 0.813)</td>\n",
       "      <td>(996, 0.8)</td>\n",
       "      <td>(47, 0.798)</td>\n",
       "      <td>(1475, 0.772)</td>\n",
       "      <td>(335, 0.772)</td>\n",
       "      <td>(1305, 0.772)</td>\n",
       "      <td>(388, 0.765)</td>\n",
       "      <td>(308, 0.755)</td>\n",
       "      <td>(120, 0.752)</td>\n",
       "      <td>(546, 0.741)</td>\n",
       "      <td>(51, 0.737)</td>\n",
       "      <td>(8, 0.727)</td>\n",
       "      <td>(617, 0.724)</td>\n",
       "      <td>(264, 0.716)</td>\n",
       "      <td>(564, 0.712)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 10</th>\n",
       "      <td>(745, 0.69)</td>\n",
       "      <td>(399, 0.664)</td>\n",
       "      <td>(291, 0.662)</td>\n",
       "      <td>(2580, 0.654)</td>\n",
       "      <td>(2520, 0.652)</td>\n",
       "      <td>(1111, 0.645)</td>\n",
       "      <td>(1224, 0.645)</td>\n",
       "      <td>(284, 0.642)</td>\n",
       "      <td>(760, 0.638)</td>\n",
       "      <td>(695, 0.633)</td>\n",
       "      <td>(1694, 0.626)</td>\n",
       "      <td>(1410, 0.624)</td>\n",
       "      <td>(1222, 0.621)</td>\n",
       "      <td>(1900, 0.618)</td>\n",
       "      <td>(374, 0.612)</td>\n",
       "      <td>(2339, 0.605)</td>\n",
       "      <td>(662, 0.604)</td>\n",
       "      <td>(2363, 0.598)</td>\n",
       "      <td>(1550, 0.589)</td>\n",
       "      <td>(538, 0.582)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 11</th>\n",
       "      <td>(14, 0.994)</td>\n",
       "      <td>(149, 0.993)</td>\n",
       "      <td>(529, 0.993)</td>\n",
       "      <td>(619, 0.992)</td>\n",
       "      <td>(392, 0.991)</td>\n",
       "      <td>(2018, 0.989)</td>\n",
       "      <td>(1526, 0.989)</td>\n",
       "      <td>(1997, 0.988)</td>\n",
       "      <td>(162, 0.988)</td>\n",
       "      <td>(1549, 0.987)</td>\n",
       "      <td>(1180, 0.987)</td>\n",
       "      <td>(2326, 0.987)</td>\n",
       "      <td>(974, 0.987)</td>\n",
       "      <td>(2503, 0.986)</td>\n",
       "      <td>(206, 0.985)</td>\n",
       "      <td>(589, 0.985)</td>\n",
       "      <td>(485, 0.983)</td>\n",
       "      <td>(1741, 0.982)</td>\n",
       "      <td>(1862, 0.982)</td>\n",
       "      <td>(1703, 0.98)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 12</th>\n",
       "      <td>(966, 0.992)</td>\n",
       "      <td>(739, 0.991)</td>\n",
       "      <td>(1033, 0.991)</td>\n",
       "      <td>(1075, 0.991)</td>\n",
       "      <td>(2306, 0.991)</td>\n",
       "      <td>(354, 0.991)</td>\n",
       "      <td>(211, 0.991)</td>\n",
       "      <td>(870, 0.991)</td>\n",
       "      <td>(1071, 0.991)</td>\n",
       "      <td>(2168, 0.991)</td>\n",
       "      <td>(1408, 0.99)</td>\n",
       "      <td>(173, 0.99)</td>\n",
       "      <td>(1788, 0.99)</td>\n",
       "      <td>(1704, 0.99)</td>\n",
       "      <td>(1828, 0.99)</td>\n",
       "      <td>(2543, 0.99)</td>\n",
       "      <td>(422, 0.99)</td>\n",
       "      <td>(824, 0.99)</td>\n",
       "      <td>(390, 0.989)</td>\n",
       "      <td>(2477, 0.989)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Topic 13</th>\n",
       "      <td>(53, 0.618)</td>\n",
       "      <td>(377, 0.343)</td>\n",
       "      <td>(158, 0.312)</td>\n",
       "      <td>(1762, 0.288)</td>\n",
       "      <td>(55, 0.285)</td>\n",
       "      <td>(356, 0.284)</td>\n",
       "      <td>(1744, 0.278)</td>\n",
       "      <td>(1499, 0.255)</td>\n",
       "      <td>(288, 0.225)</td>\n",
       "      <td>(342, 0.222)</td>\n",
       "      <td>(665, 0.221)</td>\n",
       "      <td>(75, 0.206)</td>\n",
       "      <td>(1045, 0.199)</td>\n",
       "      <td>(68, 0.194)</td>\n",
       "      <td>(1003, 0.189)</td>\n",
       "      <td>(777, 0.189)</td>\n",
       "      <td>(269, 0.186)</td>\n",
       "      <td>(1657, 0.164)</td>\n",
       "      <td>(601, 0.164)</td>\n",
       "      <td>(562, 0.162)</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      0              1              2              3  \\\n",
       "Topic 0    (198, 0.548)   (263, 0.493)   (107, 0.407)  (1764, 0.386)   \n",
       "Topic 1    (489, 0.751)   (142, 0.651)   (115, 0.587)    (722, 0.42)   \n",
       "Topic 2    (1855, 0.23)  (1319, 0.208)  (1400, 0.199)   (872, 0.198)   \n",
       "Topic 3     (81, 0.685)   (874, 0.517)   (109, 0.374)   (771, 0.349)   \n",
       "Topic 4    (508, 0.371)   (461, 0.346)   (413, 0.333)    (98, 0.282)   \n",
       "Topic 5     (85, 0.951)    (57, 0.727)   (386, 0.669)   (111, 0.651)   \n",
       "Topic 6   (1570, 0.465)  (1117, 0.415)  (1606, 0.337)   (477, 0.302)   \n",
       "Topic 7   (1586, 0.575)   (747, 0.526)   (224, 0.503)   (831, 0.447)   \n",
       "Topic 8    (441, 0.215)  (1958, 0.173)   (890, 0.172)   (626, 0.172)   \n",
       "Topic 9      (2, 0.994)      (41, 0.9)  (1044, 0.871)   (317, 0.861)   \n",
       "Topic 10    (745, 0.69)   (399, 0.664)   (291, 0.662)  (2580, 0.654)   \n",
       "Topic 11    (14, 0.994)   (149, 0.993)   (529, 0.993)   (619, 0.992)   \n",
       "Topic 12   (966, 0.992)   (739, 0.991)  (1033, 0.991)  (1075, 0.991)   \n",
       "Topic 13    (53, 0.618)   (377, 0.343)   (158, 0.312)  (1762, 0.288)   \n",
       "\n",
       "                      4              5              6              7  \\\n",
       "Topic 0   (2009, 0.383)  (1770, 0.371)  (1740, 0.361)   (894, 0.346)   \n",
       "Topic 1    (1566, 0.37)  (1101, 0.364)  (1112, 0.346)    (80, 0.346)   \n",
       "Topic 2   (1309, 0.196)  (1079, 0.187)  (1340, 0.151)  (1310, 0.146)   \n",
       "Topic 3     (18, 0.338)   (195, 0.293)  (1178, 0.287)   (344, 0.285)   \n",
       "Topic 4    (1076, 0.27)   (720, 0.268)   (740, 0.268)   (741, 0.268)   \n",
       "Topic 5      (1, 0.631)    (575, 0.57)   (434, 0.518)   (698, 0.513)   \n",
       "Topic 6    (757, 0.276)   (970, 0.254)  (1874, 0.252)  (1167, 0.169)   \n",
       "Topic 7    (453, 0.428)   (898, 0.427)    (20, 0.397)  (1594, 0.396)   \n",
       "Topic 8   (2121, 0.167)  (2184, 0.165)    (70, 0.161)    (802, 0.16)   \n",
       "Topic 9   (1013, 0.836)    (97, 0.813)     (996, 0.8)    (47, 0.798)   \n",
       "Topic 10  (2520, 0.652)  (1111, 0.645)  (1224, 0.645)   (284, 0.642)   \n",
       "Topic 11   (392, 0.991)  (2018, 0.989)  (1526, 0.989)  (1997, 0.988)   \n",
       "Topic 12  (2306, 0.991)   (354, 0.991)   (211, 0.991)   (870, 0.991)   \n",
       "Topic 13    (55, 0.285)   (356, 0.284)  (1744, 0.278)  (1499, 0.255)   \n",
       "\n",
       "                      8              9             10             11  \\\n",
       "Topic 0    (896, 0.334)     (65, 0.32)   (751, 0.318)   (725, 0.318)   \n",
       "Topic 1     (32, 0.345)   (627, 0.339)   (415, 0.319)  (2304, 0.319)   \n",
       "Topic 2   (1164, 0.133)  (1350, 0.131)  (2158, 0.115)  (2386, 0.114)   \n",
       "Topic 3   (1281, 0.265)  (1813, 0.255)   (171, 0.254)  (1422, 0.254)   \n",
       "Topic 4   (1666, 0.252)   (606, 0.248)   (693, 0.247)  (2027, 0.247)   \n",
       "Topic 5   (1577, 0.493)   (786, 0.486)    (103, 0.48)    (59, 0.471)   \n",
       "Topic 6   (1520, 0.166)   (684, 0.162)   (1668, 0.15)   (854, 0.147)   \n",
       "Topic 7    (731, 0.392)  (2283, 0.386)   (380, 0.385)  (1473, 0.382)   \n",
       "Topic 8   (1442, 0.157)   (273, 0.152)  (2561, 0.148)  (1343, 0.147)   \n",
       "Topic 9   (1475, 0.772)   (335, 0.772)  (1305, 0.772)   (388, 0.765)   \n",
       "Topic 10   (760, 0.638)   (695, 0.633)  (1694, 0.626)  (1410, 0.624)   \n",
       "Topic 11   (162, 0.988)  (1549, 0.987)  (1180, 0.987)  (2326, 0.987)   \n",
       "Topic 12  (1071, 0.991)  (2168, 0.991)   (1408, 0.99)    (173, 0.99)   \n",
       "Topic 13   (288, 0.225)   (342, 0.222)   (665, 0.221)    (75, 0.206)   \n",
       "\n",
       "                     12             13             14             15  \\\n",
       "Topic 0    (122, 0.314)    (52, 0.285)  (2577, 0.271)   (562, 0.264)   \n",
       "Topic 1   (1037, 0.313)  (1446, 0.291)  (1402, 0.291)  (1129, 0.281)   \n",
       "Topic 2   (2188, 0.112)   (1242, 0.11)  (2220, 0.107)  (1868, 0.105)   \n",
       "Topic 3   (1761, 0.244)   (1306, 0.24)  (1266, 0.238)   (737, 0.233)   \n",
       "Topic 4   (1379, 0.246)    (82, 0.241)  (2447, 0.231)   (886, 0.226)   \n",
       "Topic 5      (6, 0.466)   (270, 0.455)   (957, 0.454)  (2272, 0.452)   \n",
       "Topic 6   (2526, 0.144)   (702, 0.141)   (581, 0.141)  (1021, 0.131)   \n",
       "Topic 7    (891, 0.379)  (1181, 0.377)  (2312, 0.376)  (2533, 0.375)   \n",
       "Topic 8   (1440, 0.137)   (576, 0.136)  (2505, 0.129)  (2233, 0.125)   \n",
       "Topic 9    (308, 0.755)   (120, 0.752)   (546, 0.741)    (51, 0.737)   \n",
       "Topic 10  (1222, 0.621)  (1900, 0.618)   (374, 0.612)  (2339, 0.605)   \n",
       "Topic 11   (974, 0.987)  (2503, 0.986)   (206, 0.985)   (589, 0.985)   \n",
       "Topic 12   (1788, 0.99)   (1704, 0.99)   (1828, 0.99)   (2543, 0.99)   \n",
       "Topic 13  (1045, 0.199)    (68, 0.194)  (1003, 0.189)   (777, 0.189)   \n",
       "\n",
       "                     16             17             18             19  \n",
       "Topic 0    (526, 0.263)    (50, 0.257)   (685, 0.247)  (2248, 0.245)  \n",
       "Topic 1    (174, 0.277)    (64, 0.276)   (338, 0.275)  (1624, 0.266)  \n",
       "Topic 2   (1932, 0.104)  (2422, 0.102)  (2457, 0.075)  (1876, 0.074)  \n",
       "Topic 3   (1278, 0.228)   (113, 0.222)   (1175, 0.22)   (365, 0.216)  \n",
       "Topic 4     (89, 0.225)  (1273, 0.217)   (450, 0.215)    (56, 0.206)  \n",
       "Topic 5     (67, 0.452)   (973, 0.451)   (410, 0.435)   (1936, 0.42)  \n",
       "Topic 6   (2325, 0.126)  (2396, 0.125)  (1720, 0.124)    (995, 0.12)  \n",
       "Topic 7    (945, 0.371)  (2467, 0.368)   (752, 0.357)   (141, 0.357)  \n",
       "Topic 8   (1882, 0.123)   (849, 0.114)  (2522, 0.113)   (621, 0.108)  \n",
       "Topic 9      (8, 0.727)   (617, 0.724)   (264, 0.716)   (564, 0.712)  \n",
       "Topic 10   (662, 0.604)  (2363, 0.598)  (1550, 0.589)   (538, 0.582)  \n",
       "Topic 11   (485, 0.983)  (1741, 0.982)  (1862, 0.982)   (1703, 0.98)  \n",
       "Topic 12    (422, 0.99)    (824, 0.99)   (390, 0.989)  (2477, 0.989)  \n",
       "Topic 13   (269, 0.186)  (1657, 0.164)   (601, 0.164)   (562, 0.162)  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"top docs info:\")\n",
    "df_top_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vis.pyLDA(topic_word, doc_topic, [len(s) for s in [word_tokenize(corp) for corp in _input]], vec.get_feature_names(), np.array(sum(tf).todense())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.07 2 489 5\n",
      "lda show that , prior to surgery , patient ' priority be primarily in cancer surgery and recovery .\n",
      "LDA showed that, prior to surgery, patients' priorities were primarily in cancer surgery and recovery.\n",
      "[(0,), (1,), None]\n",
      "['priorities']-['were']-['cancer', 'surgery', 'and', 'recovery']\n",
      "[(0,), (17,), None]\n",
      "========================================\n",
      "1.07 2 489 4\n",
      "we aim to apply LDA to interview datum collect as part of a prospective , longitudinal study of qol in patient undergo radical cystectomy and urinary diversion for bladder cancer .\n",
      "We aim to apply LDA to interview data collected as part of a prospective, longitudinal study of QOL in patients undergoing radical cystectomy and urinary diversion for bladder cancer.\n",
      "[(0,), (1,), None]\n",
      "['We']-['apply']-['LDA']\n",
      "['We']-['interview']-['data']\n",
      "========================================\n",
      "1.0 1 489 3\n",
      "latent Dirichlet allocation -lrb- lda -rrb- may offer statistical rigor in summarize patient ' concern and cope strategy in a life-threatening illness .\n",
      "Latent Dirichlet Allocation (LDA) may offer statistical rigor in summarizing patients' concerns and coping strategies in a life-threatening illness.\n",
      "['Latent', 'Dirichlet', 'Allocation']-['offer']-['statistical', 'rigor']\n",
      "['Latent', 'Dirichlet', 'Allocation']-['summarizing']-['concerns']\n",
      "['Latent', 'Dirichlet', 'Allocation']-['coping']-['life-threatening', 'illness']\n",
      "========================================\n",
      "0.82 2 489 0\n",
      "as we begin to leverage Big Data in health care setting and particularly in assess patient-reported outcome , there be a need for novel analytic to address unique challenge .\n",
      "As we begin to leverage Big Data in health care settings and particularly in assessing patient-reported outcomes, there is a need for novel analytics to address unique challenges.\n",
      "[(21, 22, 24), (19,), None]\n",
      "[(1,), (2,), None]\n",
      "['we']-['leverage']-['Big', 'Data']\n",
      "['we']-['leverage']-['health', 'care', 'settings', 'and', 'assessing']\n",
      "========================================\n",
      "0.06 1 489 7\n",
      "novel analytic such as lda offer the possibility of summarize personal goal in real time without the need for conventional fixed-length measure and qualitative datum code .\n",
      "Novel analytics such as LDA offer the possibility of summarizing personal goals in real time without the need for conventional fixed-length measures and qualitative data coding.\n",
      "['Novel', 'analytics']-['offer']-['possibility']\n",
      "========================================\n",
      "0.06 1 489 1\n",
      "one such challenge be in code transcribe interview datum , typically free-text entry of statement make by interviewee during face-to-face interview .\n",
      "One such challenge is in coding transcribed interview data, typically free-text entries of statements made by interviewees during face-to-face interviews.\n",
      "['such', 'challenge']-['is']-['transcribed', 'interview', 'data']\n",
      "========================================\n",
      "1.07 2 115 7\n",
      "prior to surgery , patient ' priority be primarily in cancer surgery and recovery .\n",
      "Prior to surgery, patients' priorities were primarily in cancer surgery and recovery.\n",
      "['priorities']-['were']-['cancer', 'surgery', 'and', 'recovery']\n",
      "========================================\n",
      "1.07 2 115 3\n",
      "method LDA be apply to interview datum collect as part of a prospective , longitudinal study of qol in n = 211 patient undergo radical cystectomy and urinary diversion for bladder cancer .\n",
      "Methods LDA was applied to interview data collected as part of a prospective, longitudinal study of QOL in N = 211 patients undergoing radical cystectomy and urinary diversion for bladder cancer.\n",
      "[(0, 1), (3,), None]\n",
      "['Methods', 'LDA']-['interview']-['data']\n",
      "========================================\n",
      "1.0 1 115 4\n",
      "lda analyze personal goal statement to extract the latent topic and theme , stratify by time , and on thing patient want to accomplish and prevent .\n",
      "LDA analyzed personal goal statements to extract the latent topics and themes, stratified by time, and on things patients wanted to accomplish and prevent.\n",
      "['LDA']-['analyzed']-['personal', 'goal', 'statements', 'and', 'patients']\n",
      "['LDA']-['analyzed']-['things', 'patients']\n",
      "========================================\n",
      "1.0 1 115 2\n",
      "latent Dirichlet allocation -lrb- lda -rrb- offer statistical rigor and consistency in automate the interpretation of patient ' express concern and cope strategy .\n",
      "Latent Dirichlet Allocation (LDA) offers statistical rigor and consistency in automating the interpretation of patients' expressed concerns and coping strategies.\n",
      "['Latent', 'Dirichlet', 'Allocation']-['offers']-['statistical', 'rigor', 'and', 'consistency']\n",
      "['Latent', 'Dirichlet', 'Allocation']-['automating']-['interpretation']\n",
      "['Latent', 'Dirichlet', 'Allocation']-['coping']-['strategies']\n",
      "========================================\n",
      "0.82 2 115 0\n",
      "purpose as we begin to leverage Big Data in health care setting and particularly in assess patient-reported outcome , there be a need for novel analytic to address unique challenge .\n",
      "Purpose As we begin to leverage Big Data in health care settings and particularly in assessing patient-reported outcomes, there is a need for novel analytics to address unique challenges.\n",
      "[(22, 23, 25), (20,), None]\n",
      "[(22, 23, 25), (0,), None]\n",
      "[(2,), (3,), None]\n",
      "['we']-['leverage']-['Big', 'Data']\n",
      "['we']-['leverage']-['health', 'care', 'settings', 'and', 'assessing']\n",
      "========================================\n",
      "0.12 1 115 8\n",
      "six month after the surgery , they be replace by goal on regain a sense of normalcy , to resume work , to enjoy life more fully , and to appreciate friend and family more .\n",
      "Six months after the surgery, they were replaced by goals on regaining a sense of normalcy, to resume work, to enjoy life more fully, and to appreciate friends and family more.\n",
      "[(6,), (8,), None]\n",
      "['they']-['resume']-['work']\n",
      "['they']-['enjoy']-['life']\n",
      "['they']-['appreciate']-['friends', 'and', 'family']\n",
      "========================================\n",
      "0.09 2 115 10\n",
      "conclusion novel Big Data analytic such as lda offer the possibility of summarize personal goal without the need for conventional fixed-length measure and resource-intensive qualitative datum code .\n",
      "Conclusions Novel Big Data analytics such as LDA offer the possibility of summarizing personal goals without the need for conventional fixed-length measures and resource-intensive qualitative data coding.\n",
      "['Conclusions', 'Novel', 'Big', 'Data', 'analytics']-['offer']-['possibility']\n",
      "========================================\n",
      "0.07 1 115 9\n",
      "LDA model parameter show change priority , e.g. , immediate concern on surgery and resume employment decrease post-surgery and be replace by concern over cancer recurrence and a desire to remain healthy and strong .\n",
      "LDA model parameters showed changing priorities, e.g., immediate concerns on surgery and resuming employment decreased post-surgery and were replaced by concerns over cancer recurrence and a desire to remain healthy and strong.\n",
      "[(0, 1, 2), (3,), None]\n",
      "['LDA', 'model', 'parameters']-['decreased']-['post-surgery']\n",
      "[(0, 1, 2), (20,), None]\n",
      "========================================\n",
      "0.06 1 115 1\n",
      "one such challenge be in code transcribe interview datum , typically free-text entry of statement make during a face-to-face interview .\n",
      "One such challenge is in coding transcribed interview data, typically free-text entries of statements made during a face-to-face interview.\n",
      "['such', 'challenge']-['is']-['transcribed', 'interview', 'data']\n",
      "========================================\n",
      "1.58 2 722 6\n",
      "a total of 33.3 % -lrb- 12/36 -rrb- patient positive for hpv-16 have cervical intraepithelial neoplasia -lrb- cin -rrb- 2 or a worse result , which be significantly higher than the prevalence of cin2 of 1.8 % -lrb- 8/455 -rrb- in patient negative for hpv-16 -lrb- p < 0.001 -rrb- , while no significant association be identify for other genotype in term of genotype and clinical progress .\n",
      "A total of 33.3% (12/36) patients positive for HPV-16 had cervical intraepithelial neoplasia (CIN) 2 or a worse result, which was significantly higher than the prevalence of CIN2 of 1.8% (8/455) in patients negative for HPV-16 (P< 0.001), while no significant association was identified for other genotypes in terms of genotype and clinical progress.\n",
      "['%']-['had']-['cervical', 'intraepithelial', 'neoplasia', 'or', 'result']\n",
      "['%']-['had']-['worse', 'result']\n",
      "['significant', 'association']-['identified', 'for']-['other', 'genotypes']\n",
      "['significant', 'association']-['identified', 'for']-['terms', 'of', 'genotype']\n",
      "========================================\n",
      "1.0 1 722 12\n",
      "therefore , lda result may be present as explanatory evidence during time-constrained patient-doctor consultation in order to deliver information regard the patient 's status .\n",
      "Therefore, LDA results may be presented as explanatory evidence during time-constrained patient-doctor consultations in order to deliver information regarding the patient's status.\n",
      "['LDA', 'results']-['presented', 'as']-['explanatory', 'evidence']\n",
      "========================================\n",
      "1.0 1 722 8\n",
      "persistent infection be higher in patient aged > = 51 year -lrb- 38.7 % -rrb- than in those aged < = 50 year -lrb- 20.4 % ; p = 0.036 -rrb- .\n",
      "Persistent infection was higher in patients aged >= 51 years (38.7%) than in those aged <= 50 years (20.4%; P= 0.036).\n",
      "['Persistent', 'infection']-['was']-['higher', 'in', 'patients']\n",
      "========================================\n",
      "1.0 1 722 3\n",
      "patient undergo PAP and HPV DNA chip test between January 2006 and January 2009 .\n",
      "Patients underwent PAP and HPV DNA chip tests between January 2006 and January 2009.\n",
      "['Patients']-['underwent']-['PAP', 'HPV', 'DNA', 'chip', 'tests']\n",
      "========================================\n",
      "1.0 1 722 2\n",
      "the present study assess 491 patient -lrb- 139 hpv-positive and 352 hpv-negative case -rrb- with a PAP test result of ascus with a follow-up period > = 2 year .\n",
      "The present study assessed 491 patients (139 HPV-positive and 352 HPV-negative cases) with a PAP test result of ASCUS with a follow-up period >= 2 years.\n",
      "['present', 'study']-['assessed']-['patients', 'with', 'result']\n",
      "========================================\n",
      "1.0 1 722 0\n",
      "the present study aim to investigate difference in prognosis base on human papillomavirus -lrb- hpv -rrb- infection , persistent infection and genotype variation for patient exhibit atypical squamous cell of undetermined significance -lrb- ascus -rrb- in they initial Papanicolaou -lrb- PAP -rrb- test result .\n",
      "The present study aimed to investigate differences in prognosis based on human papillomavirus (HPV) infection, persistent infection and genotype variations for patients exhibiting atypical squamous cells of undetermined significance (ASCUS) in their initial Papanicolaou (PAP) test results.\n",
      "[(1, 2), (43,), None]\n",
      "========================================\n",
      "0.58 1 722 11\n",
      "statistical and lda analysis produce consistent result regard the association between persistent infection of hpv-16 , old age and long infection period with a clinical progression of cin2 or worse .\n",
      "Statistical and LDA analyses produced consistent results regarding the association between persistent infection of HPV-16, old age and long infection period with a clinical progression of CIN2 or worse.\n",
      "['Statistical', 'LDA', 'analyses']-['produced']-['consistent', 'results']\n",
      "========================================\n",
      "2.77 6 1566 1\n",
      "by examine the clinical observation -lrb- such as diagnosis , risk factor , and medication -rrb- mention in longitudinal emr , we can use patient ' medical chronology to automatically predict the progression of they pathology .\n",
      "By examining the clinical observations (such as diagnoses, risk factors, and medications) mentioned in longitudinal EMRs, we can use patients' medical chronologies to automatically predict the progression of their pathologies.\n",
      "['we']-['use']-['medical', 'chronologies']\n",
      "['we']-['examining']-['clinical', 'observations']\n",
      "['we']-['predict']-['progression', 'of', 'pathologies']\n",
      "========================================\n",
      "2.27 5 1566 0\n",
      "the expand clinical information provide by the advent of electronic medical record offer a exciting opportunity to substantially improve the quality of health care .\n",
      "The expanding clinical information provided by the advent of electronic medical records offers an exciting opportunity to substantially improve the quality of health care.\n",
      "['expanding', 'clinical', 'information']-['offers']-['exciting', 'opportunity']\n",
      "========================================\n",
      "1.8 3 1566 4\n",
      "in addition , we model have the potential to improve the quality of over-all patient care in practice by predict the most likely set of clinical observation at a arbitrary point in the future .\n",
      "In addition, our model has the potential to improve the quality of over-all patient care in practice by predicting the most likely set of clinical observations at an arbitrary point in the future.\n",
      "['model']-['has']-['addition']\n",
      "['model']-['has']-['potential']\n",
      "========================================\n",
      "1.58 2 1566 3\n",
      "we show that we model can be use to not only track how a patient 's clinical finding might change over time , but to also identify which patient be due for preventative visit .\n",
      "We show that our model can be used to not only track how a patient's clinical findings might change over time, but to also identify which patients are due for preventative visits.\n",
      "[(0,), (1,), None]\n",
      "[(4,), (7,), None]\n",
      "[(4,), (26,), None]\n",
      "[(4,), (11,), None]\n",
      "['patients']-['are']-['due', 'for', 'visits']\n",
      "['clinical', 'findings']-['change', 'over']-['time']\n",
      "========================================\n",
      "1.58 2 1566 2\n",
      "in this paper , we present a novel probabilistic model which jointly learn how to -lrb- 1 -rrb- group patient base on the similarity between they clinical observation as well as how to -lrb- 2 -rrb- predict the way a new patient 's clinical observation might evolve in the future .\n",
      "In this paper, we present a novel probabilistic model which jointly learns how to (1) group patients based on the similarities between their clinical observations as well as how to (2) predict the way a new patient's clinical observations might evolve in the future.\n",
      "['we']-['present']-['novel', 'probabilistic', 'model']\n",
      "========================================\n",
      "1.88 4 1101 1\n",
      "a hospital emr dataset typically consist of medical record of hospitalize patient .\n",
      "A hospital EMR dataset typically consists of medical records of hospitalized patients.\n",
      "['hospital', 'EMR', 'dataset']-['consists', 'of']-['medical', 'records', 'of', 'patients']\n",
      "========================================\n",
      "1.66 3 1101 4\n",
      "this topic modeling help to understand the constitution of patient disease and offer a tool for better planning of treatment .\n",
      "This topic modeling helps to understand the constitution of patient diseases and offers a tool for better planning of treatment.\n",
      "[(1, 2), (3,), None]\n",
      "['topic', 'modeling']-['understand']-['constitution', 'of', 'diseases']\n",
      "========================================\n",
      "1.66 5 1101 3\n",
      "traditional topic model , such as latent dirichlet allocation -lrb- lda -rrb- and hierarchical dirichlet process -lrb- hdp -rrb- , can be employ to discover disease topic from emr datum by treat patient as document and diagnosis code as word .\n",
      "Traditional topic models, such as latent Dirichlet allocation (LDA) and hierarchical Dirichlet process (HDP), can be employed to discover disease topics from EMR data by treating patients as documents and diagnosis codes as words.\n",
      "[(0, 1, 2), (22,), None]\n",
      "['Traditional', 'topic', 'models']-['discover']-['disease', 'topics']\n",
      "['Traditional', 'topic', 'models']-['discover']-['EMR', 'data']\n",
      "['Traditional', 'topic', 'models']-['treating']-['documents', 'and', 'codes', 'as', 'words']\n",
      "========================================\n",
      "1.25 4 1101 2\n",
      "a medical record contain diagnostic information -lrb- diagnosis code -rrb- , procedure perform -lrb- procedure code -rrb- and admission detail .\n",
      "A medical record contains diagnostic information (diagnosis codes), procedures performed (procedure codes) and admission details.\n",
      "['medical', 'record']-['contains']-['diagnostic', 'information']\n",
      "========================================\n",
      "0.88 2 1101 11\n",
      "we evaluate the propose model on two real-world medical dataset - polyvascular disease and acute myocardial infarction disease .\n",
      "We evaluate the proposed models on two real-world medical datasets - PolyVascular disease and Acute Myocardial Infarction disease.\n",
      "['We']-['evaluate']-['proposed', 'models', 'on', 'datasets']\n",
      "========================================\n",
      "0.66 3 1101 9\n",
      "furthermore , since procedure code be often correlate with diagnosis code , we develop the correspondence wddcrf -lrb- corr-wddcrf -rrb- to explore conditional relationship of procedure code for a give disease pattern .\n",
      "Furthermore, since procedure codes are often correlated with diagnosis codes, we develop the correspondence wddCRF (Corr-wddCRF) to explore conditional relationships of procedure codes for a given disease pattern.\n",
      "['we']-['develop']-['correspondence', 'wddCRF']\n",
      "['procedure', 'codes']-['correlated', 'with']-['diagnosis', 'codes']\n",
      "['we']-['explore']-['conditional', 'relationships', 'of', 'codes']\n",
      "['we']-['explore']-['given', 'disease', 'pattern']\n",
      "========================================\n",
      "0.6 1 1101 0\n",
      "Electronic Medical Record -lrb- EMR -rrb- have establish itself as a valuable resource for large scale analysis of health datum .\n",
      "Electronic Medical Record (EMR) has established itself as a valuable resource for large scale analysis of health data.\n",
      "['Electronic', 'Medical', 'Record']-['established']-['valuable', 'resource', 'for', 'analysis']\n",
      "========================================\n",
      "0.37 2 1101 6\n",
      "we be motivate by the fact that diagnosis code be connect in the form of icd-10 tree structure which present semantic relationship between code .\n",
      "We are motivated by the fact that diagnosis codes are connected in the form of ICD-10 tree structure which presents semantic relationships between codes.\n",
      "[(0,), (2,), None]\n",
      "========================================\n",
      "0.28 1 1101 13\n",
      "we also use disease topic proportion as new feature and show that use feature from the corr-wddcrf outperform the baseline on 14-day readmission prediction .\n",
      "We also use disease topic proportions as new features and show that using features from the Corr-wddCRF outperforms the baselines on 14-days readmission prediction.\n",
      "['We']-['use']-['disease', 'topic', 'proportions']\n",
      "['We']-['use']-['new', 'features']\n",
      "========================================\n",
      "0.28 1 1101 5\n",
      "in this paper , we propose a novel and flexible hierarchical bayesian nonparametric model , the word distance dependent chinese restaurant franchise -lrb- wddcrf -rrb- , which incorporate word-to-word distance to discover semantically-coherent disease topic .\n",
      "In this paper, we propose a novel and flexible hierarchical Bayesian nonparametric model, the word distance dependent Chinese restaurant franchise (wddCRF), which incorporates word-to-word distances to discover semantically-coherent disease topics.\n",
      "['we']-['propose']-['novel', 'flexible', 'hierarchical', 'Bayesian', 'nonparametric', 'model']\n",
      "========================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.06 1 1101 14\n",
      "beside these , the prediction for procedure code base on the corr-wddcrf also show considerable accuracy .\n",
      "Beside these, the prediction for procedure codes based on the Corr-wddCRF also shows considerable accuracy.\n",
      "['prediction', 'for', 'codes']-['shows']-['considerable', 'accuracy']\n",
      "========================================\n",
      "1.4 3 1112 0\n",
      "information and communication technology have enable healthcare institution to accumulate large amount of healthcare datum that include diagnosis , medication , and additional contextual information such as patient demographic .\n",
      "Information and communications technologies have enabled healthcare institutions to accumulate large amounts of healthcare data that include diagnoses, medications, and additional contextual information such as patient demographics.\n",
      "['Information', 'communications', 'technologies']-['enabled']-['healthcare', 'institutions']\n",
      "['healthcare', 'institutions']-['accumulate']-['large', 'amounts', 'of', 'data']\n",
      "========================================\n",
      "0.98 3 1112 1\n",
      "to gain a better understanding of big healthcare datum and to develop better data-driven clinical decision support system , we propose a novel multiple - channel latent dirichlet allocation -lrb- mclda -rrb- approach for modeling diagnosis , medication , and contextual information in healthcare datum .\n",
      "To gain a better understanding of big healthcare data and to develop better data-driven clinical decision support systems, we propose a novel multiple -channel latent Dirichlet allocation (MCLDA) approach for modeling diagnoses, medications, and contextual information in healthcare data.\n",
      "['we']-['propose']-['allocation', 'approach', 'for', 'diagnoses']\n",
      "['we']-['gain']-['better', 'understanding', 'of', 'data']\n",
      "['we']-['develop']-['better', 'data-driven', 'clinical', 'decision', 'support', 'systems']\n",
      "========================================\n",
      "0.91 2 1112 2\n",
      "the propose MCLDA model assume that a latent health status group structure be responsible for the observe co-occurrence among diagnosis , medication , and contextual information .\n",
      "The proposed MCLDA model assumes that a latent health status group structure is responsible for the observed co-occurrences among diagnoses, medications, and contextual information.\n",
      "[(1, 2, 3), (4,), None]\n",
      "['latent', 'health', 'status', 'group', 'structure']-['is']-['responsible', 'for', 'co-occurrences']\n",
      "========================================\n",
      "0.67 2 1112 8\n",
      "thus , mclda represent a promising approach to modeling healthcare datum for clinical decision support .\n",
      "Thus, MCLDA represents a promising approach to modeling healthcare data for clinical decision support.\n",
      "['MCLDA']-['represents']-['promising', 'approach', 'to', 'data', 'for', 'support']\n",
      "========================================\n",
      "0.59 2 1112 6\n",
      "MCLDA can also be employ to predict miss medication or diagnosis give partial record .\n",
      "MCLDA can also be employed to predict missing medications or diagnoses given partial records.\n",
      "[(0,), (4,), None]\n",
      "['MCLDA']-['predict']-['missing', 'medications', 'or', 'diagnoses']\n",
      "========================================\n",
      "0.59 2 1112 5\n",
      "moreover , MCLDA be able to identify the pairing between diagnosis and medication in a record base on the assign latent group .\n",
      "Moreover, MCLDA is able to identify the pairing between diagnoses and medications in a record based on the assigned latent groups.\n",
      "['MCLDA']-['is']-['able']\n",
      "['MCLDA']-['identify']-['pairing']\n",
      "['MCLDA']-['identify']-['record']\n",
      "========================================\n",
      "0.36 2 1112 3\n",
      "use a real-world research testb that include one million healthcare insurance claim record , we investigate the utility of MCLDA .\n",
      "Using a real-world research testbed that includes one million healthcare insurance claim records, we investigate the utility of MCLDA.\n",
      "['we']-['investigate']-['utility', 'of', 'MCLDA']\n",
      "[(14,), (4,), None]\n",
      "['that']-['includes']-['healthcare', 'insurance', 'claim', 'records']\n",
      "========================================\n",
      "0.31 1 1112 7\n",
      "we evaluation result also show that , in most case , MCLDA outperform alternative method such as logistic regression and the k-nearest-neighbor -lrb- knn -rrb- model for two prediction task , i.e. , medication and diagnosis prediction .\n",
      "Our evaluation results also show that, in most cases, MCLDA outperforms alternative methods such as logistic regressions and the k-nearest-neighbor (KNN) model for two prediction tasks, i.e., medication and diagnosis prediction.\n",
      "[(1, 2), (4,), None]\n",
      "['MCLDA']-['outperforms']-['most', 'cases']\n",
      "['MCLDA']-['outperforms']-['alternative', 'methods']\n",
      "========================================\n",
      "1.34 4 80 9\n",
      "however , this study suggest that imperfect datum -lrb- e.g. , icd code in combination with other ehr feature -rrb- can serve as a silver standard to develop a risk model , apply that model to patient without dementia code , and then select a case-detection threshold .\n",
      "The study is one of the first to utilize both structured and unstructured EHRs to develop risk scores for the diagnosis of dementia.\n",
      "['study']-['suggests']-['that']\n",
      "========================================\n",
      "0.87 3 80 3\n",
      "this study seek to identify case of undiagnosed dementia by develop and validate a weakly supervise machine-learning approach that incorporate the analysis of both structured and unstructured electronic health record -lrb- ehr -rrb- data.methodsa topic modeling approach that include latent Dirichlet allocation , stable topic extraction , and random sampling be apply to VHA ehr .\n",
      "This study seeks to identify cases of undiagnosed dementia by developing and validating a weakly supervised machine-learning approach that incorporates the analysis of both structured and unstructured electronic health record (EHR) data.MethodsA topic modeling approach that included latent Dirichlet allocation, stable topic extraction, and random sampling was applied to VHA EHRs.\n",
      "========================================\n",
      "0.59 3 80 10\n",
      "the study be one of the first to utilize both structured and unstructured ehr to develop risk score for the diagnosis of dementia .\n",
      "sent token differ\n",
      "['study']-['is']-['one', 'of', 'first']\n",
      "========================================\n",
      "0.37 2 80 8\n",
      "ConclusionsDementia be underdiagnosed , and thus , icd code alone can not serve as a gold standard for diagnosis .\n",
      "However, this study suggests that imperfect data (e.g., ICD codes in combination with other EHR features) can serve as a silver standard to develop a risk model, apply that model to patients without dementia codes, and then select a case-detection threshold.\n",
      "['ConclusionsDementia']-['is']-['underdiagnosed', 'and']\n",
      "========================================\n",
      "0.33 2 80 6\n",
      "these score be validate in a subset of veteran without icd-9 dementia code -lrb- n = 120 -rrb- by expert in dementia who perform manual record review and achieve a high level of inter-rater agreement .\n",
      "These scores were validated in a subset of Veterans without ICD-9 dementia codes (n=120) by experts in dementia who performed manual record reviews and achieved a high level of inter-rater agreement.\n",
      "['scores']-['validated', 'in']-['subset', 'of', 'Veterans']\n",
      "['scores']-['validated', 'in']-['ICD-9', 'dementia', 'codes']\n",
      "========================================\n",
      "0.06 1 80 5\n",
      "a logistic regression model be use to develop dementia prediction score , and manual review be conduct to validate the machine-learning results.resultsa total of 853 feature be identify -lrb- 290 topic , 174 non-dementia icd code , 159 CPT code , 59 medication , and 171 note type -rrb- for the development of logistic regression prediction score .\n",
      "A logistic regression model was used to develop dementia prediction scores, and manual reviews were conducted to validate the machine-learning results.ResultsA total of 853 features were identified (290 topics, 174 non-dementia ICD codes, 159 CPT codes, 59 medications, and 171 note types) for the development of logistic regression prediction scores.\n",
      "[(1, 2, 3), (5,), None]\n",
      "['logistic', 'regression', 'model']-['develop']-['dementia', 'prediction', 'scores']\n",
      "========================================\n",
      "0.06 1 80 4\n",
      "topic feature from unstructured datum and feature from structured datum be compare between veteran with -lrb- n = 1861 -rrb- and without -lrb- n = 9305 -rrb- icd-9 dementia code .\n",
      "Topic features from unstructured data and features from structured data were compared between Veterans with (n=1861) and without (n=9305) ICD-9 dementia codes.\n",
      "[(0, 1, 2, 4), (11,), None]\n",
      "========================================\n",
      "1.0 1 32 1\n",
      "predict diabetic complication be regard as a highly effective technique for increase the survival rate of diabetic patient .\n",
      "Predicting diabetic complications is regarded as a highly effective technique for increasing the survival rate of diabetic patients.\n",
      "========================================\n",
      "0.87 2 32 5\n",
      "specifically , we first estimate the similarity between textual medical record after datum preprocessing , and then we perform selda-based diabetic complication topic mining base on similarity constraint .\n",
      "Specifically, we first estimate the similarity between textual medical records after data preprocessing, and then we perform seLDA-based diabetic complication topic mining based on similarity constraints.\n",
      "['we']-['estimate']-['similarity']\n",
      "========================================\n",
      "0.87 2 32 3\n",
      "moreover , the similarity among medical record that be overlook by exist approach could potentially improve the accuracy of prediction model .\n",
      "Moreover, the similarities among medical records that are overlooked by existing approaches could potentially improve the accuracy of prediction models.\n",
      "['similarities']-['improve']-['accuracy', 'of', 'models']\n",
      "========================================\n",
      "0.87 2 32 2\n",
      "while many study currently use medical image and structured medical record , very limited effort have be dedicate to apply datum mining technique for unstructured textual medical record , such as admission and discharge record .\n",
      "While many studies currently use medical images and structured medical records, very limited efforts have been dedicated to applying data mining techniques for unstructured textual medical records, such as admission and discharge records.\n",
      "[(13, 14), (17,), None]\n",
      "['many', 'studies']-['use']-['medical', 'images', 'and', 'records']\n",
      "['many', 'studies']-['use']-['structured', 'medical', 'records']\n",
      "['limited', 'efforts']-['applying']-['data', 'mining', 'techniques']\n",
      "['limited', 'efforts']-['applying']-['unstructured', 'textual', 'medical', 'records']\n",
      "========================================\n",
      "0.6 1 32 0\n",
      "Diabetes and its complication have be recognize worldwide as a major public health threat .\n",
      "Diabetes and its complications have been recognized worldwide as a major public health threat.\n",
      "['Diabetes', 'and', 'complications']-['recognized']-['major', 'public', 'health', 'threat']\n",
      "========================================\n",
      "1.48 4 627 0\n",
      "obstetric electronic medical record -lrb- emr -rrb- contain massive amount of medical datum and health information .\n",
      "Obstetric electronic medical records (EMRs) contain massive amounts of medical data and health information.\n",
      "['Obstetric', 'electronic', 'medical', 'records']-['contain']-['massive', 'amounts', 'of', 'data']\n",
      "========================================\n",
      "0.91 2 627 6\n",
      "the result of the diagnosis assistant can be introduce as a supplementary learning method for medical student .\n",
      "The result of the diagnosis assistant can be introduced as a supplementary learning method for medical students.\n",
      "['result', 'of', 'assistant']-['introduced', 'as']-['supplementary', 'learning', 'method', 'for', 'students']\n",
      "========================================\n",
      "0.88 3 627 7\n",
      "additionally , the method can be use not only for obstetric emr but also for other medical record .\n",
      "Additionally, the method can be used not only for obstetric EMRs but also for other medical records.\n",
      "['method']-['used']-['obstetric', 'EMRs']\n",
      "['method']-['used']-['other', 'medical', 'records']\n",
      "['method']-['used']-['obstetric', 'EMRs']\n",
      "['method']-['used']-['other', 'medical', 'records']\n",
      "========================================\n",
      "0.59 3 627 2\n",
      "the admit diagnosis in the first course record of the emr be reason from various source , such as chief complaint , auxiliary examination , and physical examination .\n",
      "The admitting diagnosis in the first course record of the EMR is reasoned from various sources, such as chief complaints, auxiliary examinations, and physical examinations.\n",
      "['admitting', 'diagnosis', 'in', 'record']-['reasoned', 'from']-['various', 'sources']\n",
      "========================================\n",
      "0.32 2 627 3\n",
      "this paper treat the diagnosis assistant as a multilabel classification task base on the analysis of obstetric emr .\n",
      "This paper treats the diagnosis assistant as a multilabel classification task based on the analyses of obstetric EMRs.\n",
      "['paper']-['treats']-['diagnosis', 'assistant']\n",
      "['paper']-['treats']-['multilabel', 'classification', 'task']\n",
      "========================================\n",
      "0.32 2 627 1\n",
      "the information extraction and diagnosis assistant of obstetric emr be of great significance in improve the fertility level of the population .\n",
      "The information extraction and diagnosis assistants of obstetric EMRs are of great significance in improving the fertility level of the population.\n",
      "['information', 'extraction', 'and', 'assistants', 'of', 'EMRs']-['are']-['great', 'significance']\n",
      "========================================\n",
      "0.31 1 627 4\n",
      "the latent Dirichlet allocation -lrb- lda -rrb- topic and the word vector be use as feature and the four multilabel classification method , bp-mll -lrb- backpropagation multilabel learning -rrb- , rakel -lrb- random k labelset -rrb- , mlknn -lrb- multilabel k-nearest neighbor -rrb- , and cc -lrb- chain classifier -rrb- , be utilize to build the diagnosis assistant model .\n",
      "The latent Dirichlet allocation (LDA) topic and the word vector are used as features and the four multilabel classification methods, BP-MLL (backpropagation multilabel learning), RAkEL (RAndom k labELsets), MLkNN (multilabel k-nearest neighbor), and CC (chain classifier), are utilized to build the diagnosis assistant models.\n",
      "['allocation', 'topic', 'and', 'vector']-['used', 'as']-['features', 'and', 'methods']\n",
      "['allocation', 'topic', 'and', 'vector']-['used', 'as']-['multilabel', 'classification', 'methods']\n",
      "['word', 'vector']-['used', 'as']-['features', 'and', 'methods']\n",
      "['word', 'vector']-['used', 'as']-['multilabel', 'classification', 'methods']\n",
      "========================================\n",
      "1.28 2 415 3\n",
      "the result obtain from the mkl method be give to the ANFIS classifier to classify the heart disease and healthy patient .\n",
      "The result obtained from the MKL method is given to the ANFIS classifier to classify the heart disease and healthy patients.\n",
      "['result']-['given', 'to']-['ANFIS', 'classifier']\n",
      "['result']-['classify']-['heart', 'disease', 'and', 'patients']\n",
      "['result']-['classify']-['healthy', 'patients']\n",
      "========================================\n",
      "1.28 2 415 2\n",
      "mkl method be use to divide parameter between heart disease patient and normal individual .\n",
      "MKL method is used to divide parameters between heart disease patients and normal individuals.\n",
      "[(0, 1), (3,), None]\n",
      "['MKL', 'method']-['divide']-['parameters']\n",
      "========================================\n",
      "0.6 2 415 0\n",
      "multiple Kernel Learning with adaptive Neuro-Fuzzy Inference System -lrb- mkl with anfis -rrb- base deep learning method be propose in this paper for heart disease diagnosis .\n",
      "Multiple Kernel Learning with Adaptive Neuro-Fuzzy Inference System (MKL with ANFIS) based deep learning method is proposed in this paper for heart disease diagnosis.\n",
      "['Learning', 'based', 'deep', 'learning', 'method']-['proposed', 'in']-['heart', 'disease', 'diagnosis']\n",
      "========================================\n",
      "1.58 4 2304 4\n",
      "we obtain meaningful diagnosis and treatment topic -lrb- cluster -rrb- from the datum , which clinically indicate some important medical group correspond to comorbidity disease -lrb- e.g. , heart disease and diabetic kidney disease in t2dm inpatient -rrb- .\n",
      "We obtained meaningful diagnosis and treatment topics (clusters) from the data, which clinically indicated some important medical groups corresponding to comorbidity diseases (e.g., heart disease and diabetic kidney diseases in T2DM inpatients).\n",
      "['We']-['obtained']-['meaningful', 'diagnosis', 'and', 'topics']\n",
      "['We']-['obtained']-['treatment', 'topics']\n",
      "========================================\n",
      "1.0 1 2304 5\n",
      "the result show that manifestation sub-category actually exist in t2dm patient that need specific , individualised cm therapy .\n",
      "The results show that manifestation sub-categories actually exist in T2DM patients that need specific, individualised CM therapies.\n",
      "[(1,), (2,), None]\n",
      "['manifestation', 'sub-categories']-['exist', 'in']-['T2DM', 'patients']\n",
      "========================================\n",
      "0.93 3 2304 1\n",
      "in this paper , we propose a data mining method , call the Symptom-Herb-Diagnosis topic -lrb- shot -rrb- model , to automatically extract the common relationship among symptom , herb combination and diagnosis from large-scale cm clinical datum .\n",
      "In this paper, we propose a data mining method, called the Symptom-Herb-Diagnosis topic (SHOT) model, to automatically extract the common relationships among symptoms, herb combinations and diagnoses from large-scale CM clinical data.\n",
      "['we']-['propose']-['data', 'mining', 'method']\n",
      "========================================\n",
      "0.69 2 2304 3\n",
      "we apply the SHDT model to discover the common cm diagnosis and treatment knowledge for type 2 diabetes mellitus -lrb- t2dm -rrb- use 3 238 inpatient case .\n",
      "We applied the SHDT model to discover the common CM diagnosis and treatment knowledge for type 2 diabetes mellitus (T2DM) using 3 238 inpatient cases.\n",
      "['We']-['applied']-['SHDT', 'model']\n",
      "['We']-['discover']-['common', 'CM', 'diagnosis', 'and', 'knowledge', 'for', 'mellitus']\n",
      "========================================\n",
      "0.58 1 2304 6\n",
      "furthermore , the result demonstrate that this method be helpful for generate cm clinical guideline for t2dm base on structured collect clinical datum .\n",
      "Furthermore, the results demonstrate that this method is helpful for generating CM clinical guidelines for T2DM based on structured collected clinical data.\n",
      "[(3,), (4,), None]\n",
      "['method']-['is']-['helpful']\n",
      "['method']-['generating']-['CM', 'clinical', 'guidelines', 'for', 'T2DM']\n",
      "========================================\n",
      "0.58 1 2304 0\n",
      "induction of common knowledge or regularity from large-scale clinical datum be a vital task for chinese medicine -lrb- cm -rrb- .\n",
      "Induction of common knowledge or regularities from large-scale clinical data is a vital task for Chinese medicine (CM).\n",
      "['Induction', 'of', 'knowledge', 'from', 'data']-['is']-['vital', 'task', 'for', 'medicine']\n",
      "========================================\n",
      "1.0 1 1037 5\n",
      "of 4687 patient with inpatient discharge summary , 470 be readmit within 30 day .\n",
      "Of 4687 patients with inpatient discharge summaries, 470 were readmitted within 30 days.\n",
      "['470']-['readmitted']-['patients', 'with', 'summaries']\n",
      "========================================\n",
      "0.93 4 1037 1\n",
      "the symptom or characteristic of illness course necessary to develop reliable predictor be not available in code billing datum , but may be present in narrative electronic health record -lrb- ehr -rrb- discharge summary .\n",
      "The symptoms or characteristics of illness course necessary to develop reliable predictors are not available in coded billing data, but may be present in narrative electronic health record (EHR) discharge summaries.\n",
      "========================================\n",
      "0.62 2 1037 4\n",
      "the cohort be randomly split to derive a training -lrb- 70 % -rrb- and testing -lrb- 30 % -rrb- data set , and we train separate support vector machine model for baseline clinical feature alone , baseline feature plus common individual word and the above plus topic identify from the 75-topic LDA model .\n",
      "The cohort was randomly split to derive a training (70%) and testing (30%) data set, and we trained separate support vector machine models for baseline clinical features alone, baseline features plus common individual words and the above plus topics identified from the 75-topic LDA model.\n",
      "[(1,), (4,), None]\n",
      "['cohort']-['derive']-['training', 'and', 'set']\n",
      "['cohort']-['derive']-['testing', 'data', 'set']\n",
      "========================================\n",
      "0.58 2 1037 9\n",
      "topic modeling and related approach offer the potential to improve prediction use ehr , if generalizability can be establish in other clinical cohort .\n",
      "Topic modeling and related approaches offer the potential to improve prediction using EHRs, if generalizability can be established in other clinical cohorts.\n",
      "['Topic', 'modeling', 'and', 'approaches']-['offer']-['potential']\n",
      "['related', 'approaches']-['offer']-['potential']\n",
      "['generalizability']-['established', 'in']-['other', 'clinical', 'cohorts']\n",
      "========================================\n",
      "0.31 1 1037 2\n",
      "we identify a cohort of individual admit to a psychiatric inpatient unit between 1994 and 2012 with a principal diagnosis of major depressive disorder , and extract inpatient psychiatric discharge narrative note .\n",
      "We identified a cohort of individuals admitted to a psychiatric inpatient unit between 1994 and 2012 with a principal diagnosis of major depressive disorder, and extracted inpatient psychiatric discharge narrative notes.\n",
      "['We']-['identified']-['cohort', 'of', 'individuals', 'and', 'notes']\n",
      "['We']-['identified']-['extracted', 'inpatient', 'psychiatric', 'discharge', 'narrative', 'notes']\n",
      "========================================\n",
      "0.28 1 1037 6\n",
      "the 75-topic LDA model include topic link to psychiatric symptom -lrb- suicide , severe depression , anxiety , trauma , eating/weight and panic -rrb- and major depressive disorder comorbidity -lrb- infection , postpartum , brain tumor , diarrhea and pulmonary disease -rrb- .\n",
      "The 75-topic LDA model included topics linked to psychiatric symptoms (suicide, severe depression, anxiety, trauma, eating/weight and panic) and major depressive disorder comorbidities (infection, postpartum, brain tumor, diarrhea and pulmonary disease).\n",
      "['75-topic', 'LDA', 'model']-['included']-['topics']\n",
      "========================================\n",
      "0.27 1 1037 8\n",
      "inclusion of topic derive from narrative note allow more accurate discrimination of individual at high risk for psychiatric readmission in this cohort .\n",
      "Inclusion of topics derived from narrative notes allows more accurate discrimination of individuals at high risk for psychiatric readmission in this cohort.\n",
      "['Inclusion', 'of', 'topics']-['allows']-['accurate', 'discrimination', 'of', 'individuals', 'at', 'risk']\n",
      "========================================\n",
      "0.27 1 1037 0\n",
      "the ability to predict psychiatric readmission would facilitate the development of intervention to reduce this risk , a major driver of psychiatric health-care cost .\n",
      "The ability to predict psychiatric readmission would facilitate the development of interventions to reduce this risk, a major driver of psychiatric health-care costs.\n",
      "['ability']-['facilitate']-['development', 'of', 'interventions']\n",
      "========================================\n",
      "0.04 1 1037 7\n",
      "by include LDA topic , prediction of readmission , as measure by area under receiver-operating characteristic curve in the testing data set , be improve from baseline -lrb- area under the curve 0.618 -rrb- to baseline + 1000 word -lrb- 0.682 -rrb- to baseline +75 topic -lrb- 0.784 -rrb- .\n",
      "By including LDA topics, prediction of readmission, as measured by area under receiver-operating characteristic curves in the testing data set, was improved from baseline (area under the curve 0.618) to baseline + 1000 words (0.682) to baseline+75 topics (0.784).\n",
      "['prediction', 'of', 'readmission']-['improved', 'from']-['baseline', '+', 'words', 'to', 'topics']\n",
      "========================================\n",
      "1.58 2 1446 0\n",
      "clinical pathway -lrb- cp -rrb- analysis play a important role in health-care management in ensure specialize , standardized , normalize and sophisticated therapy procedure for individual patient .\n",
      "Clinical pathway (CP) analysis plays an important role in health-care management in ensuring specialized, standardized, normalized and sophisticated therapy procedures for individual patients.\n",
      "['pathway', 'analysis']-['plays']-['important', 'role']\n",
      "['pathway', 'analysis']-['plays']-['health-care', 'management']\n",
      "['pathway', 'analysis']-['ensuring']-['specialized', 'standardized', 'normalized', 'sophisticated', 'therapy', 'procedures', 'for', 'patients']\n",
      "========================================\n",
      "1.4 3 1446 4\n",
      "discover treatment pattern , as actionable knowledge represent the best practice for most patient in most time of they treatment process , form the backbone of cp , and can be exploit to help physician better understand they specialty and learn from previous experience for cp analysis and improvement .\n",
      "Discovered treatment patterns, as actionable knowledge representing the best practice for most patients in most time of their treatment processes, form the backbone of CPs, and can be exploited to help physicians better understand their specialty and learn from previous experiences for CP analysis and improvement.\n",
      "[(0, 1, 2), (31,), None]\n",
      "['actionable', 'knowledge', 'in', 'time']-['form']-['backbone', 'of', 'CPs']\n",
      "['Discovered', 'treatment', 'patterns']-['help']-['physicians']\n",
      "========================================\n",
      "1.38 3 1446 3\n",
      "more specifically , we develop a probabilistic topic model to link patient feature and treatment behavior together to mine treatment pattern hide in emr .\n",
      "More specifically, we develop a probabilistic topic model to link patient features and treatment behaviors together to mine treatment patterns hidden in EMRs.\n",
      "['we']-['develop']-['probabilistic', 'topic', 'model']\n",
      "========================================\n",
      "0.88 3 1446 1\n",
      "recently , with the rapid development of hospital information system , a large volume of electronic medical record -lrb- emr -rrb- have be produce , which provide a comprehensive source for cp analysis .\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recently, with the rapid development of hospital information systems, a large volume of electronic medical records (EMRs) has been produced, which provides a comprehensive source for CP analysis.\n",
      "['large', 'volume', 'of', 'records']-['produced']-['rapid', 'development', 'of', 'systems']\n",
      "========================================\n",
      "0.38 2 1446 5\n",
      "experimental result on a real collection of 985 emr collect from a chinese hospital show that the propose approach can effectively identify meaningful treatment pattern from emr .\n",
      "Experimental results on a real collection of 985 EMRs collected from a Chinese hospital show that the proposed approach can effectively identify meaningful treatment patterns from EMRs.\n",
      "[(0, 1, 2, 5), (14,), None]\n",
      "['proposed', 'approach']-['identify']-['meaningful', 'treatment', 'patterns', 'from', 'EMRs']\n",
      "========================================\n",
      "0.0 1 1446 2\n",
      "in this paper , we be concern with the problem of utilize the heterogeneous emr to assist cp analysis and improvement .\n",
      "In this paper, we are concerned with the problem of utilizing the heterogeneous EMRs to assist CP analysis and improvement.\n",
      "['we']-['concerned', 'with']-['paper']\n",
      "['we']-['concerned', 'with']-['problem']\n",
      "========================================\n",
      "1.88 4 1402 0\n",
      "background and objective : risk stratification aim to provide physician with the accurate assessment of a patient 's clinical risk such that a individualized prevention or management strategy can be develop and deliver .\n",
      "Background and objective: Risk stratification aims to provide physicians with the accurate assessment of a patient's clinical risk such that an individualized prevention or management strategy can be developed and delivered.\n",
      "========================================\n",
      "1.86 3 1402 7\n",
      "result : we verify the effectiveness of the propose approach on a clinical dataset contain 3463 coronary heart disease -lrb- chd -rrb- patient instance .\n",
      "Results: We verify the effectiveness of the proposed approach on a clinical dataset containing 3463 coronary heart disease (CHD) patient instances.\n",
      "========================================\n",
      "1.86 4 1402 4\n",
      "the propose PRSM recognize a patient clinical state as a probabilistic combination of latent sub-profile , and generate sub-profile-specific risk tier of patient from they ehr in a fully unsupervised fashion .\n",
      "The proposed PRSM recognizes a patient clinical state as a probabilistic combination of latent sub-profiles, and generates sub-profile-specific risk tiers of patients from their EHRs in a fully unsupervised fashion.\n",
      "['proposed', 'PRSM']-['recognizes']-['patient', 'clinical', 'state']\n",
      "['proposed', 'PRSM']-['recognizes']-['probabilistic', 'combination', 'of', 'sub-profiles']\n",
      "========================================\n",
      "1.85 3 1402 1\n",
      "exist risk stratification technique mainly focus on predict the overall risk of a individual patient in a supervised manner , and , at the cohort level , often offer little insight beyond a flat score-based segmentation from the label clinical dataset .\n",
      "Existing risk stratification techniques mainly focus on predicting the overall risk of an individual patient in a supervised manner, and, at the cohort level, often offer little insight beyond a flat score-based segmentation from the labeled clinical dataset.\n",
      "[(0, 1, 2, 3), (5,), None]\n",
      "['Existing', 'risk', 'stratification', 'techniques']-['predicting']-['overall', 'risk', 'of', 'patient']\n",
      "========================================\n",
      "1.27 2 1402 12\n",
      "moreover , patient sub-profile and sub-profile-specific risk tier generate by we model be coherent and informative , and provide significant potential to be explore for the further task , such as patient cohort analysis .\n",
      "Moreover, patient sub-profiles and sub-profile-specific risk tiers generated by our models are coherent and informative, and provide significant potential to be explored for the further tasks, such as patient cohort analysis.\n",
      "['patient', 'sub-profiles', 'and', 'tiers']-['are']-['coherent', 'and', 'informative', 'and', 'provide']\n",
      "['sub-profile-specific', 'risk', 'tiers']-['are']-['coherent', 'and', 'informative', 'and', 'provide']\n",
      "========================================\n",
      "1.15 4 1402 2\n",
      "to this end , in this paper , we propose a new approach for risk stratification by explore a large volume of electronic health record -lrb- ehr -rrb- in a unsupervised fashion .\n",
      "To this end, in this paper, we propose a new approach for risk stratification by exploring a large volume of electronic health records (EHRs) in an unsupervised fashion.\n",
      "['we']-['propose']-['new', 'approach', 'for', 'stratification']\n",
      "['we']-['exploring']-['large', 'volume', 'of', 'records']\n",
      "['we']-['exploring']-['unsupervised', 'fashion']\n",
      "========================================\n",
      "0.56 2 1402 11\n",
      "in addition , the unsupervised nature of we model make they highly portable to the risk stratification task of various disease .\n",
      "In addition, the unsupervised nature of our models makes them highly portable to the risk stratification tasks of various diseases.\n",
      "['unsupervised', 'nature', 'of', 'models']-['makes']-['addition']\n",
      "['them']-['portable', 'to']-['risk', 'stratification', 'tasks', 'of', 'diseases']\n",
      "========================================\n",
      "0.56 2 1402 6\n",
      "in addition , we present a extension of PRSM , call weakly supervise prsm -lrb- ws-prsm -rrb- by incorporate minimum prior information into the model , in order to improve the risk stratification accuracy , and to make we model highly portable to risk stratification task of various disease .\n",
      "In addition, we present an extension of PRSM, called weakly supervised PRSM (WS-PRSM) by incorporating minimum prior information into the model, in order to improve the risk stratification accuracy, and to make our models highly portable to risk stratification tasks of various diseases.\n",
      "['we']-['present']-['extension', 'of', 'PRSM']\n",
      "========================================\n",
      "0.31 2 1402 10\n",
      "conclusion : experimental result reveal that we model achieve competitive performance in risk stratification in comparison with exist supervised approach .\n",
      "Conclusions: Experimental results reveal that our models achieve competitive performance in risk stratification in comparison with existing supervised approaches.\n",
      "========================================\n",
      "0.28 2 1402 13\n",
      "we hypothesize that the propose framework can readily meet the demand for risk stratification from a large volume of ehr in a open-ended fashion .\n",
      "We hypothesize that the proposed framework can readily meet the demand for risk stratification from a large volume of EHRs in an open-ended fashion.\n",
      "[(0,), (1,), None]\n",
      "['proposed', 'framework']-['meet']-['demand', 'for', 'stratification', 'from', 'volume']\n",
      "========================================\n",
      "0.27 1 1402 9\n",
      "as well , in comparison with prsm , ws-prsm have over 2 % performance gain , on the experimental dataset , demonstrate that incorporate risk score knowledge as prior information can improve the performance in risk stratification .\n",
      "As well, in comparison with PRSM, WS-PRSM has over 2% performance gain, on the experimental dataset, demonstrating that incorporating risk scoring knowledge as prior information can improve the performance in risk stratification.\n",
      "['WS-PRSM']-['has']-['comparison', 'with', 'PRSM']\n",
      "['WS-PRSM']-['has']-['performance', 'gain']\n",
      "['WS-PRSM']-['has']-['experimental', 'dataset']\n",
      "['WS-PRSM']-['demonstrating']-['that']\n",
      "========================================\n",
      "0.27 1 1402 8\n",
      "both prsm and ws-prsm be compare with two established supervised risk stratification algorithm , i.e. , logistic regression and support vector machine , and show the effectiveness of we model in risk stratification of chd in term of the Area under the receiver operating characteristic curve -lrb- auc -rrb- analysis .\n",
      "Both PRSM and WS-PRSM were compared with two established supervised risk stratification algorithms, i.e., logistic regression and support vector machine, and showed the effectiveness of our models in risk stratification of CHD in terms of the Area Under the receiver operating characteristic Curve (AUC) analysis.\n",
      "['PRSM', 'and', 'WS-PRSM']-['compared', 'with']-['established', 'supervised', 'risk', 'stratification', 'algorithms']\n",
      "========================================\n",
      "0.27 1 1402 3\n",
      "method : along this line , this paper propose a novel probabilistic topic modeling framework call probabilistic risk stratification model -lrb- prsm -rrb- base on latent Dirichlet allocation -lrb- lda -rrb- .\n",
      "Methods: Along this line, this paper proposes a novel probabilistic topic modeling framework called probabilistic risk stratification model (PRSM) based on Latent Dirichlet Allocation (LDA).\n",
      "========================================\n",
      "1.67 3 1129 0\n",
      "in healthcare organizational setting , the design of a clinical pathway -lrb- cp -rrb- be challenge since patient follow a particular pathway may have not only one single first-diagnosis but also several typical comorbidity , and thus it require different discipline involve to put together they partial knowledge about the overall pathway .\n",
      "In healthcare organizational settings, the design of a clinical pathway (CP) is challenging since patients following a particular pathway may have not only one single first-diagnosis but also several typical comorbidities, and thus it requires different disciplines involved to put together their partial knowledge about the overall pathway.\n",
      "['design', 'of', 'pathway']-['challenging']-['healthcare', 'organizational', 'settings']\n",
      "========================================\n",
      "1.58 2 1129 7\n",
      "we verify the effectiveness of the propose model on a real clinical dataset contain 12,120 patient trace , which pertain to the unstable angina cp .\n",
      "We verify the effectiveness of the proposed model on a real clinical dataset containing 12,120 patient traces, which pertain to the unstable angina CP.\n",
      "['We']-['verify']-['effectiveness', 'of', 'model']\n",
      "['We']-['verify']-['real', 'clinical', 'dataset']\n",
      "========================================\n",
      "0.98 2 1129 9\n",
      "in addition , a possible medical application in term of treatment recommendation be provide to illustrate the potential of the propose model .\n",
      "In addition, a possible medical application in terms of treatment recommendation is provided to illustrate the potential of the proposed model.\n",
      "['possible', 'medical', 'application', 'in', 'terms']-['provided']-['addition']\n",
      "['possible', 'medical', 'application', 'in', 'terms']-['illustrate']-['potential', 'of', 'model']\n",
      "========================================\n",
      "0.96 2 1129 1\n",
      "although many datum mining technique have be propose to discover latent treatment information for cp analysis and reconstruction from a large volume of clinical datum , they be specific to extract nontrivial information about the therapy and treatment of the first-diagnosis .\n",
      "Although many data mining techniques have been proposed to discover latent treatment information for CP analysis and reconstruction from a large volume of clinical data, they are specific to extract nontrivial information about the therapy and treatment of the first-diagnosis.\n",
      "['they']-['are']-['specific']\n",
      "[(1, 2, 3, 4), (7,), None]\n",
      "['they']-['extract']-['nontrivial', 'information', 'about', 'therapy']\n",
      "['many', 'data', 'mining', 'techniques']-['discover']-['latent', 'treatment', 'information', 'for', 'analysis']\n",
      "['many', 'data', 'mining', 'techniques']-['discover']-['large', 'volume', 'of', 'data']\n",
      "========================================\n",
      "0.69 2 1129 6\n",
      "it first generate a set of latent treatment pattern from diagnosis label , follow by sampling treatment from each pattern .\n",
      "It first generates a set of latent treatment patterns from diagnosis labels, followed by sampling treatments from each pattern.\n",
      "['It']-['generates']-['set', 'of', 'patterns']\n",
      "========================================\n",
      "0.69 2 1129 4\n",
      "in particular , we propose a generative statistical model to extract underlie treatment pattern , unveil the latent association between diagnosis label -lrb- include both first-diagnosis and comorbidity -rrb- and treatment , and compute the contribution of comorbidity in these pattem .\n",
      "In particular, we propose a generative statistical model to extract underlying treatment patterns, unveil the latent associations between diagnosis labels (including both first-diagnosis and comorbidities) and treatments, and compute the contribution of comorbidities in these pattems.\n",
      "['we']-['propose']-['generative', 'statistical', 'model']\n",
      "========================================\n",
      "0.38 1 1129 10\n",
      "experimental result indicate that we approach can discover not only meaningful latent treatment pattern exhibit comorbidity focus , but also implicit change of treatment of first-diagnosis due to the incorporation of typical comorbidity potentially .\n",
      "Experimental results indicate that our approach can discover not only meaningful latent treatment patterns exhibiting comorbidity focus, but also implicit changes of treatments of first-diagnosis due to the incorporation of typical comorbidities potentially.\n",
      "[(0, 1), (2,), None]\n",
      "['approach']-['discover']-['meaningful', 'latent', 'treatment', 'patterns', 'but', 'changes']\n",
      "['approach']-['discover']-['implicit', 'changes', 'of', 'treatments']\n",
      "========================================\n",
      "0.38 1 1129 8\n",
      "three treatment pattern be discover from datum , indicate latent correlation between comorbidity and treatment in the pathway .\n",
      "Three treatment patterns are discovered from data, indicating latent correlations between comorbidities and treatments in the pathway.\n",
      "['treatment', 'patterns']-['discovered', 'from']-['data']\n",
      "['treatment', 'patterns']-['indicating']-['latent', 'correlations', 'in', 'pathway']\n",
      "========================================\n",
      "0.38 1 1129 3\n",
      "this study propose to extract latent treatment pattem that characterize essential treatment for both first-diagnosis and typical comorbidity from the execution datum of a pathway .\n",
      "This study proposes to extract latent treatment pattems that characterize essential treatments for both first-diagnosis and typical comorbidities from the execution data of a pathway.\n",
      "[(1,), (2,), None]\n",
      "['study']-['extract']-['latent', 'treatment', 'pattems']\n",
      "========================================\n",
      "0.38 1 1129 2\n",
      "the influence of comorbidity on adopt essential treatment be crucial for a pathway but have seldom be explore .\n",
      "The influence of comorbidities on adopting essential treatments is crucial for a pathway but has seldom been explored.\n",
      "['influence', 'of', 'comorbidities']-['is']-['crucial', 'for', 'pathway', 'but', 'explored']\n",
      "========================================\n",
      "0.31 1 1129 5\n",
      "the propose model extend latent Dirichlet allocation with a additional layer for diagnosis modeling .\n",
      "The proposed model extends latent Dirichlet allocation with an additional layer for diagnosis modeling.\n",
      "['proposed', 'model']-['extends']-['latent', 'Dirichlet', 'allocation']\n",
      "['proposed', 'model']-['extends']-['additional', 'layer', 'for', 'modeling']\n",
      "========================================\n",
      "1.04 2 174 1\n",
      "in this study , we use social media data to examine the knowledge , attitude , and belief of as patient regard biologic therapy .\n",
      "In this study, we used social media data to examine the knowledge, attitudes, and beliefs of AS patients regarding biologic therapies.\n",
      "['we']-['used']-['social', 'media', 'data']\n",
      "['we']-['examine']-['knowledge', 'attitudes', 'and', 'beliefs']\n",
      "['we']-['examine']-['beliefs', 'of', 'patients']\n",
      "========================================\n",
      "1.03 2 174 12\n",
      "conclusion Social media reveal a dynamic range of theme govern as patient ' experience with and choice of biologic agent .\n",
      "Conclusion Social media revealed a dynamic range of themes governing AS patients' experience with and choice of biologic agents.\n",
      "['Conclusion', 'Social', 'media']-['revealed']-['dynamic', 'range', 'of', 'themes']\n",
      "========================================\n",
      "1.0 1 174 13\n",
      "the complexity of select biologic from among many such agent and navigate they risk/benefit profile suggest the merit of create online tool tailor to support patient ' decision-making with regard to biologic therapy for as .\n",
      "The complexity of selecting biologics from among many such agents and navigating their risk/benefit profiles suggests the merit of creating online tools tailored to support patients' decision-making with regard to biologic therapies for AS.\n",
      "['complexity']-['suggests']-['merit']\n",
      "========================================\n",
      "1.0 1 174 11\n",
      "additional implicit patient need -lrb- e.g. , support -rrb- be identify use qualitative analysis .\n",
      "Additional implicit patient needs (e.g., support) were identified using qualitative analyses.\n",
      "[(0, 1, 2, 3), (10,), None]\n",
      "['Additional', 'implicit', 'patient', 'needs']-['using']-['qualitative', 'analyses']\n",
      "========================================\n",
      "1.0 1 174 0\n",
      "objective few study have examine ankylose spondylitis -lrb- as -rrb- patient ' concern about and perception of biologic therapy , apart from traditional survey .\n",
      "Objective Few studies have examined ankylosing spondylitis (AS) patients' concerns about and perceptions of biologic therapies, apart from traditional surveys.\n",
      "[(0, 1, 2), (4,), None]\n",
      "['Objective', 'Few', 'studies']-['ankylosing']-['spondylitis']\n",
      "========================================\n",
      "0.88 2 174 9\n",
      "other theme , include the psychological impact of as , reporting of medical literature , and as disease consequence , account for the remain 40 % -lrb- n = 45 -rrb- .\n",
      "Other themes, including the psychological impact of AS, reporting of medical literature, and AS disease consequences, accounted for the remaining 40% (n = 45).\n",
      "========================================\n",
      "0.73 3 174 10\n",
      "in discussion regard as treatment , most topic involve biologic , and most subtheme involve side effect -lrb- e.g. , fatigue , allergic reaction -rrb- , biologic treatment attribute -lrb- e.g. , dosing , frequency -rrb- , and concern about use of biologic -lrb- e.g. , increase cancer risk -rrb- .\n",
      "In discussions regarding AS treatment, most topics involved biologics, and most subthemes involved side effects (e.g., fatigue, allergic reactions), biologic treatment attributes (e.g., dosing, frequency), and concerns about use of biologics (e.g., increased cancer risk).\n",
      "['most', 'topics']-['involved']-['discussions']\n",
      "['most', 'topics']-['involved']-['biologics']\n",
      "========================================\n",
      "0.38 1 174 8\n",
      "the majority of theme -lrb- n = 67 -lsb- 60 % -rsb- -rrb- focus on discussion relate to as treatment .\n",
      "The majority of themes (n = 67 [60%]) focused on discussions related to AS treatment.\n",
      "['majority', 'of', 'themes']-['focused', 'on']-['discussions']\n",
      "========================================\n",
      "0.04 1 174 6\n",
      "the topic be manually review to identify theme , which be confirm use thematic data analysis .\n",
      "The topics were manually reviewed to identify themes, which were confirmed using thematic data analysis.\n",
      "[(1,), (4,), None]\n",
      "['topics']-['identify']-['themes']\n",
      "========================================\n",
      "0.04 1 174 4\n",
      "to explore theme within the collection of post in a unsupervised manner , a latent Dirichlet allocation topic model be fit to the data set .\n",
      "To explore themes within the collection of posts in an unsupervised manner, a latent Dirichlet allocation topic model was fit to the data set.\n",
      "['latent', 'Dirichlet', 'allocation', 'topic', 'model']-['fit', 'to']-['data', 'set']\n",
      "['latent', 'Dirichlet', 'allocation', 'topic', 'model']-['explore']-['unsupervised', 'manner']\n",
      "========================================\n",
      "2.2 4 338 14\n",
      "meanwhile , patient with serious disease be more interested in medical competence -lrb- Cohen d = -0.99 , Delta u = -0.165 , t = -32.58 , and p < .001 -rrb- , medical advice and prescription -lrb- Cohen d = -0.65 , Delta u = -0.082 , t = -21.45 , and p < .001 -rrb- , financing -lrb- Cohen d = -0.26 , Delta u = -0.018 , t = -8.45 , and p < .001 -rrb- , and diagnosis and pathogenesis -lrb- Cohen d = -1.55 , Delta u = -0.229 , t = -50.93 , and p < .001 -rrb- .\n",
      "Meanwhile, patients with serious diseases were more interested in medical competence (Cohen d=-0.99, Delta u=-0.165, t=-32.58, and P<.001), medical advice and prescription (Cohen d=-0.65, Delta u=-0.082, t=-21.45, and P<.001), financing (Cohen d=-0.26, Delta u=-0.018, t=-8.45, and P<.001), and diagnosis and pathogenesis (Cohen d=-1.55, Delta u=-0.229, t=-50.93, and P<.001).\n",
      "['patients', 'with', 'diseases']-['were']-['interested', 'in', 'competence']\n",
      "========================================\n",
      "1.88 3 338 13\n",
      "patient with mild disease be more interested in medical ethic -lrb- Cohen d = 0.25 , Delta u 0.039 , t = 8.33 , and p < .001 -rrb- , operation process -lrb- Cohen d = 0.57 , Delta u 0.060 , t = 18.75 , and p < .001 -rrb- , patient profile -lrb- Cohen d = 1.19 , Delta u 0.132 , t = 39.33 , and p < .001 -rrb- , and symptom -lrb- Cohen d = 1.91 , Delta u = 0.274 , t = 62.82 , and p < .001 -rrb- .\n",
      "Patients with mild diseases were more interested in medical ethics (Cohen d=0.25, Delta u 0.039, t=8.33, and P<.001), operation process (Cohen d=0.57, Delta u 0.060, t=18.75, and P<.001), patient profile (Cohen d=1.19, Delta u 0.132, t=39.33, and P<.001), and symptoms (Cohen d=1.91, Delta u=0.274, t=62.82, and P<.001).\n",
      "['Patients', 'with', 'diseases']-['were']-['interested', 'in', 'ethics']\n",
      "========================================\n",
      "1.6 3 338 12\n",
      "symptom -lrb- Cohen d = 1.58 , Delta u = 0.216 , t = 229.75 , and p < .001 -rrb- be more often mention by patient with acute disease , whereas communication skill -lrb- Cohen d = -0.29 , Delta u = -0.038 , t = -42.01 , and p < .001 -rrb- , financing -lrb- Cohen d = -0.68 , Delta u = -0.098 , t = -99.26 , and p < .001 -rrb- , and diagnosis and pathogenesis -lrb- Cohen d = -0.55 , Delta u = -0.078 , t = -80.09 , and p < .001 -rrb- be more often mention by patient with chronic disease .\n",
      "Symptoms (Cohen d=1.58, Delta u=0.216, t=229.75, and P<.001) are more often mentioned by patients with acute diseases, whereas communication skills (Cohen d=-0.29, Delta u=-0.038, t=-42.01, and P<.001), financing (Cohen d=-0.68, Delta u=-0.098, t=-99.26, and P<.001), and diagnosis and pathogenesis (Cohen d=-0.55, Delta u=-0.078, t=-80.09, and P<.001) are more often mentioned by patients with chronic diseases.\n",
      "[(0,), (24,), None]\n",
      "========================================\n",
      "1.31 2 338 9\n",
      "the patient-related domain include the category of the patient profile , symptom , diagnosis , and pathogenesis .\n",
      "The patient-related domain included the categories of the patient profile, symptoms, diagnosis, and pathogenesis.\n",
      "['patient-related', 'domain']-['included']-['categories', 'of', 'profile']\n",
      "========================================\n",
      "1.28 2 338 17\n",
      "furthermore , the mining result reveal marked difference in patient ' interest across different disease type , socioeconomic development level , and hospital level .\n",
      "Furthermore, the mining results reveal marked differences in patients' interests across different disease types, socioeconomic development levels, and hospital levels.\n",
      "['mining']-['results']-['reveal', 'levels', 'and', 'levels']\n",
      "['mining']-['results']-['socioeconomic', 'development', 'levels']\n",
      "['mining']-['results']-['hospital', 'levels']\n",
      "========================================\n",
      "1.03 2 338 4\n",
      "objective : this study aim to develop a hierarchical topic taxonomy to uncover the latent structure of physician review and illustrate its application for mining patient ' interest base on the propose taxonomy and algorithm .\n",
      "Objective: This study aims to develop a hierarchical topic taxonomy to uncover the latent structure of physician reviews and illustrate its application for mining patients' interests based on the proposed taxonomy and algorithm.\n",
      "========================================\n",
      "1.0 1 338 16\n",
      "the propose algorithm base on labeled-latent Dirichlet allocation can achieve impressive classification result for mining patient ' interest .\n",
      "The proposed algorithm based on Labeled-Latent Dirichlet Allocation can achieve impressive classification results for mining patients' interests.\n",
      "[(1, 2), (9,), None]\n",
      "['impressive', 'classification']-['results', 'for']-['interests']\n",
      "========================================\n",
      "0.6 1 338 8\n",
      "the physician-related domain include the category of medical ethic , medical competence , communication skill , medical advice , and prescription .\n",
      "The physician-related domain included the categories of medical ethics, medical competence, communication skills, medical advice, and prescriptions.\n",
      "['physician-related', 'domain']-['included']-['categories', 'of', 'ethics']\n",
      "========================================\n",
      "0.06 2 338 15\n",
      "conclusion : this mixed-methods approach , integrate literature review , data-driven topic discovery , and human annotation , be a effective and rigorous way to develop a physician review topic taxonomy .\n",
      "Conclusions: This mixed-methods approach, integrating literature reviews, data-driven topic discovery, and human annotation, is an effective and rigorous way to develop a physician review topic taxonomy.\n",
      "========================================\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.03 1 338 6\n",
      "mixed method , include a literature review , data-driven-based topic discovery , and human annotation be use to develop the physician review topic taxonomy .\n",
      "Mixed methods, including a literature review, data-driven-based topic discovery, and human annotation were used to develop the physician review topic taxonomy.\n",
      "[(0, 1), (16,), None]\n",
      "['Mixed', 'methods']-['develop']-['physician', 'review', 'topic', 'taxonomy']\n",
      "========================================\n",
      "0.03 2 338 5\n",
      "method : datum comprise 122,716 physician review , include review of 8501 doctor from a lead physician review website in China -lrb- haodf.com -rrb- , collect between 2007 and 2015 .\n",
      "Methods: Data comprised 122,716 physician reviews, including reviews of 8501 doctors from a leading physician review website in China (haodf.com), collected between 2007 and 2015.\n",
      "========================================\n",
      "0.03 1 338 2\n",
      "the first step toward mining physician review be to determine how the natural structure or dimension be embed in review .\n",
      "The first step toward mining physician reviews is to determine how the natural structure or dimensions is embedded in reviews.\n",
      "[(1, 2), (7,), None]\n",
      "[(1, 2), (9,), None]\n",
      "['natural', 'structure', 'or', 'dimensions']-['embedded', 'in']-['reviews']\n",
      "========================================\n",
      "0.03 1 338 1\n",
      "although many study have explore the text information of physician review , very few have focus on develop a systematic topic taxonomy embed in physician review .\n",
      "Although many studies have explored the text information of physician reviews, very few have focused on developing a systematic topic taxonomy embedded in physician reviews.\n",
      "[(13,), (15,), None]\n",
      "['many', 'studies']-['explored']-['text', 'information', 'of', 'reviews']\n",
      "['few']-['developing']-['systematic', 'topic', 'taxonomy']\n",
      "========================================\n",
      "0.03 1 338 0\n",
      "background : web-based physician review be invaluable gold mine that merit further investigation .\n",
      "Background: Web-based physician reviews are invaluable gold mines that merit further investigation.\n",
      "========================================\n",
      "2.56 5 1624 1\n",
      "these patient record contain valuable medical information include patient information , diagnosis , treatment method , and eventual patient outcome .\n",
      "These patient records contain valuable medical information including patient information, diagnosis, treatment methods, and eventual patient outcomes.\n",
      "['patient', 'records']-['contain']-['valuable', 'medical', 'information']\n",
      "========================================\n",
      "1.09 3 1624 0\n",
      "the recent year have see a surge in the implementation of electronic health care record .\n",
      "The recent years have seen a surge in the implementation of electronic health care records.\n",
      "['recent', 'years']-['seen']-['surge', 'in', 'implementation']\n",
      "========================================\n",
      "1.0 1 1624 3\n",
      "in this paper , we present a method for automatically discover underlie theme and pattern within patient datum .\n",
      "In this paper, we present a method for automatically discovering underlying themes and patterns within patient data.\n",
      "['we']-['present']-['paper']\n",
      "['we']-['present']-['method']\n",
      "========================================\n",
      "0.87 2 1624 5\n",
      "in we research , we partition graph from term gather from electronic health record .\n",
      "In our research, we partitioned graphs from terms gathered from electronic health records.\n",
      "['we']-['partitioned']-['research']\n",
      "['we']-['partitioned']-['graphs']\n",
      "['we']-['partitioned']-['terms']\n",
      "========================================\n",
      "0.27 1 1624 2\n",
      "it be important to analyze pattern within these record in order to more efficiently treat individual .\n",
      "It is important to analyze patterns within these records in order to more efficiently treat individuals.\n",
      "['It']-['is']-['important']\n",
      "['It']-['analyze']-['patterns']\n",
      "['It']-['treat']-['individuals']\n",
      "========================================\n"
     ]
    }
   ],
   "source": [
    "t_idx = 1\n",
    "for top_doc in top_docs[t_idx]:\n",
    "    _idx = top_doc[0]\n",
    "    if type(parse[_idx]) is str:\n",
    "        print(_idx, \"parse\", \"err\")\n",
    "        continue\n",
    "    sents = relation.convert_parse2lemma_sents(parse[_idx])\n",
    "    sort_idxs, importance, counts = relation.extract_important_sents(sents, [terms[x[0]] for x in top_terms[t_idx]], [x[1] for x in top_terms[t_idx]])\n",
    "    for i in sort_idxs:\n",
    "        if importance[i]>0:\n",
    "            print(round(importance[i], 2), counts[i], _idx, i)\n",
    "            sent_tokens = parse[_idx][\"sentences\"][i][\"tokens\"]\n",
    "            sent_deps = parse[_idx][\"sentences\"][i][\"enhancedPlusPlusDependencies\"]\n",
    "            print(\" \".join([s[\"lemma\"] for s in sent_tokens]))\n",
    "            sents = sent_tokenize(_raw[_idx])\n",
    "            if len(sents) > i:\n",
    "                print(sents[i])\n",
    "            else:\n",
    "                print(\"sent token differ\")\n",
    "            triples = relation.extract_triples_from_sent(sent_deps, sent_tokens, True)\n",
    "            for triple in triples:\n",
    "                if None in triple:\n",
    "                    print(triple)\n",
    "                    continue\n",
    "                s = [sent_tokens[i][\"originalText\"] for i in triple[0]]\n",
    "                p = [sent_tokens[i][\"originalText\"] for i in triple[1]]\n",
    "                o = [sent_tokens[i][\"originalText\"] for i in triple[2]]\n",
    "                print(\"{}-{}-{}\".format(s,p,o))\n",
    "            print(\"=\"*40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "158 parse err\n",
      "208\n",
      "175 traditional topic model-[discover]-disease topic\n",
      "155 latent Dirichlet allocation topic model-[fit to]-data set\n",
      "148 traditional topic model-[discover]-emr datum\n",
      "148 latent Dirichlet allocation topic model-[explore]-unsupervised manner\n",
      "147 exist risk stratification technique-[predict]-overall risk of patient\n",
      "143 many datum mining technique-[discover]-latent treatment information for analysis\n",
      "141 hospital emr dataset-[consist of]-medical record of patient\n",
      "139 patient record-[contain]-valuable medical information\n",
      "139 Learning base deep learning method-[propose in]-heart disease diagnosis\n",
      "132 traditional topic model-[treat]-document and code as word\n",
      "129 allocation topic and vector-[use as]-multilabel classification method\n",
      "129 allocation topic and vector-[use as]-feature and method\n",
      "126 mixed method-[develop]-physician review topic taxonomy\n",
      "122 result-[classify]-heart disease and patient\n",
      "122 additional implicit patient need-[use]-qualitative analysis\n",
      "121 present study-[assess]-patient with result\n",
      "119 propose PRSM-[recognize]-patient clinical state\n",
      "116 propose approach-[identify]-meaningful treatment pattern from emr\n",
      "112 propose model-[extend]-latent Dirichlet allocation\n",
      "112 we-[develop]-probabilistic topic model\n",
      "108 approach-[discover]-meaningful latent treatment pattern but change\n",
      "107 75-topic LDA model-[include]-topic\n",
      "105 pathway analysis-[ensure]-specialize standardized normalize sophisticated therapy procedure for patient\n",
      "103 lda-[analyze]-personal goal statement and patient\n",
      "101 obstetric electronic medical record-[contain]-massive amount of datum\n",
      "99 we-[use]-disease topic proportion\n",
      "99 many datum mining technique-[discover]-large volume of datum\n",
      "98 logistic regression model-[develop]-dementia prediction score\n",
      "98 we-[propose]-allocation approach for diagnosis\n",
      "97 method-[use]-other medical record\n",
      "97 method-[use]-other medical record\n",
      "96 result-[classify]-healthy patient\n",
      "95 many study-[use]-structured medical record\n",
      "93 prediction of readmission-[improve from]-baseline + word to topic\n",
      "93 we-[obtain]-meaningful diagnosis and topic\n",
      "93 lda-[analyze]-thing patient\n",
      "92 patient with disease-[be]-interested in competence\n",
      "92 topic modeling and approach-[offer]-potential\n",
      "92 patient with disease-[be]-interested in ethic\n",
      "91 many study-[use]-medical image and record\n",
      "87 inclusion of topic-[allow]-accurate discrimination of individual at risk\n",
      "85 procedure code-[correlate with]-diagnosis code\n",
      "85 topic modeling-[understand]-constitution of disease\n",
      "82 possible medical application in term-[illustrate]-potential of model\n",
      "81 treatment pattern-[discover from]-datum\n",
      "80 treatment pattern-[indicate]-latent correlation in pathway\n",
      "80 study-[extract]-latent treatment pattem\n",
      "80 we-[propose]-novel flexible hierarchical bayesian nonparametric model\n",
      "80 propose model-[extend]-additional layer for modeling\n",
      "79 we-[obtain]-treatment topic\n",
      "79 persistent infection-[be]-higher in patient\n",
      "78 statistical lda analysis-[produce]-consistent result\n",
      "77 patient sub-profile and tier-[be]-coherent and informative and provide\n",
      "76 word vector-[use as]-feature and method\n",
      "76 word vector-[use as]-multilabel classification method\n",
      "76 we-[evaluate]-propose model on dataset\n",
      "74 few-[develop]-systematic topic taxonomy\n",
      "74 mclda-[represent]-promising approach to datum for support\n",
      "74 topic-[identify]-theme\n",
      "74 patient-[undergo]-PAP HPV DNA chip test\n",
      "73 medical record-[contain]-diagnostic information\n",
      "71 we-[propose]-generative statistical model\n",
      "71 manifestation sub-category-[exist in]-t2dm patient\n",
      "70 we-[examine]-belief of patient\n",
      "69 admit diagnosis in record-[reason from]-various source\n",
      "69 they-[portable to]-risk stratification task of disease\n",
      "69 we-[propose]-data mining method\n",
      "68 470-[readmit]-patient with summary\n",
      "66 patient-[be]-due for visit\n",
      "66 result of assistant-[introduce as]-supplementary learning method for student\n",
      "65 limited effort-[apply]-unstructured textual medical record\n",
      "65 method-[use]-obstetric emr\n",
      "65 method-[use]-obstetric emr\n",
      "64 lda result-[present as]-explanatory evidence\n",
      "64 approach-[discover]-implicit change of treatment\n",
      "63 we-[discover]-common cm diagnosis and knowledge for mellitus\n",
      "63 discover treatment pattern-[help]-physician\n",
      "63 we-[present]-novel probabilistic model\n",
      "63 we-[propose]-new approach for stratification\n",
      "62 most topic-[involve]-biologic\n",
      "61 latent Dirichlet allocation-[offer]-statistical rigor and consistency\n",
      "61 method LDA-[interview]-datum\n",
      "60 prsm and ws-prsm-[compare with]-established supervised risk stratification algorithm\n",
      "60 latent Dirichlet allocation-[offer]-statistical rigor\n",
      "58 many study-[explore]-text information of review\n",
      "58 latent health status group structure-[be]-responsible for co-occurrence\n",
      "58 similarity-[improve]-accuracy of model\n",
      "58 we-[develop]-better data-driven clinical decision support system\n",
      "57 method-[generate]-cm clinical guideline for t2dm\n",
      "56 latent Dirichlet allocation-[summarize]-concern\n",
      "56 mining-[result]-reveal level and level\n",
      "56 most topic-[involve]-discussion\n",
      "54 limited effort-[apply]-datum mining technique\n",
      "54 unsupervised nature of model-[make]-addition\n",
      "54 expand clinical information-[offer]-exciting opportunity\n",
      "53 MCLDA-[predict]-miss medication or diagnosis\n",
      "53 paper-[treat]-diagnosis assistant\n",
      "52 we-[use]-medical chronology\n",
      "50 mining-[result]-socioeconomic development level\n",
      "50 that-[include]-healthcare insurance claim record\n",
      "50 latent Dirichlet allocation-[cope]-strategy\n",
      "49 healthcare institution-[accumulate]-large amount of datum\n",
      "49 we-[use]-social media data\n",
      "49 latent Dirichlet allocation-[cope]-life-threatening illness\n",
      "48 mining-[result]-hospital level\n",
      "48 propose framework-[meet]-demand for stratification from volume\n",
      "47 latent Dirichlet allocation-[automate]-interpretation\n",
      "47 we-[apply]-SHDT model\n",
      "47 induction of knowledge from datum-[be]-vital task for medicine\n",
      "46 such challenge-[be]-transcribe interview datum\n",
      "46 possible medical application in term-[provide]-addition\n",
      "46 such challenge-[be]-transcribe interview datum\n",
      "46 large volume of record-[produce]-rapid development of system\n",
      "45 we-[verify]-effectiveness of model\n",
      "44 score-[validate in]-icd-9 dementia code\n",
      "44 model-[have]-potential\n",
      "44 model-[have]-addition\n",
      "44 LDA model parameter-[decrease]-post-surgery\n",
      "42 we-[identify]-extract inpatient psychiatric discharge narrative note\n",
      "42 we-[explore]-large volume of record\n",
      "42 we-[explore]-give disease pattern\n",
      "40 information extraction and assistant of emr-[be]-great significance\n",
      "39 we-[use]-new feature\n",
      "39 we-[interview]-datum\n",
      "39 we-[gain]-better understanding of datum\n",
      "39 we-[verify]-real clinical dataset\n",
      "38 prediction for code-[show]-considerable accuracy\n",
      "38 propose PRSM-[recognize]-probabilistic combination of sub-profile\n",
      "37 related approach-[offer]-potential\n",
      "37 clinical finding-[change over]-time\n",
      "37 sub-profile-specific risk tier-[be]-coherent and informative and provide\n",
      "37 MCLDA-[identify]-record\n",
      "34 patient-related domain-[include]-category of profile\n",
      "34 impressive classification-[result for]-interest\n",
      "33 generalizability-[establish in]-other clinical cohort\n",
      "32 we-[leverage]-health care setting and assess\n",
      "32 significant association-[identify for]-term of genotype\n",
      "32 they-[extract]-nontrivial information about therapy\n",
      "32 we-[examine]-clinical observation\n",
      "32 Diabetes and complication-[recognize]-major public health threat\n",
      "32 we-[leverage]-health care setting and assess\n",
      "32 mkl method-[divide]-parameter\n",
      "31 %-[have]-worse result\n",
      "31 we-[present]-method\n",
      "31 cohort-[derive]-testing data set\n",
      "31 we-[explore]-conditional relationship of code\n",
      "31 information communication technology-[enable]-healthcare institution\n",
      "31 %-[have]-cervical intraepithelial neoplasia or result\n",
      "30 result-[give to]-ANFIS classifier\n",
      "30 natural structure or dimension-[embed in]-review\n",
      "29 physician-related domain-[include]-category of ethic\n",
      "29 paper-[treat]-multilabel classification task\n",
      "29 we-[identify]-cohort of individual and note\n",
      "29 pathway analysis-[play]-important role\n",
      "27 conclusion novel Big Data analytic-[offer]-possibility\n",
      "26 MCLDA-[outperform]-alternative method\n",
      "26 influence of comorbidity-[be]-crucial for pathway but explore\n",
      "26 conclusion Social media-[reveal]-dynamic range of theme\n",
      "26 we-[develop]-correspondence wddcrf\n",
      "26 pathway analysis-[play]-health-care management\n",
      "25 actionable knowledge in time-[form]-backbone of cp\n",
      "25 significant association-[identify for]-other genotype\n",
      "25 design of pathway-[challenge]-healthcare organizational setting\n",
      "24 it-[generate]-set of pattern\n",
      "23 novel analytic-[offer]-possibility\n",
      "23 method-[be]-helpful\n",
      "22 Electronic Medical Record-[establish]-valuable resource for analysis\n",
      "20 majority of theme-[focus on]-discussion\n",
      "20 objective few study-[ankylose]-spondylitis\n",
      "19 priority-[be]-cancer surgery and recovery\n",
      "19 priority-[be]-cancer surgery and recovery\n",
      "19 we-[present]-paper\n",
      "18 we-[concern with]-paper\n",
      "18 study-[suggest]-that\n",
      "18 score-[validate in]-subset of veteran\n",
      "17 cohort-[derive]-training and set\n",
      "15 study-[be]-one of first\n",
      "15 it-[analyze]-pattern\n",
      "14 we-[explore]-unsupervised fashion\n",
      "14 MCLDA-[identify]-pairing\n",
      "14 ws-prsm-[have]-experimental dataset\n",
      "13 we-[examine]-knowledge attitude and belief\n",
      "12 ws-prsm-[have]-comparison with prsm\n",
      "11 we-[present]-extension of PRSM\n",
      "11 we-[predict]-progression of pathology\n",
      "11 ability-[facilitate]-development of intervention\n",
      "10 we-[concern with]-problem\n",
      "10 MCLDA-[outperform]-most case\n",
      "10 it-[treat]-individual\n",
      "9 we-[partition]-term\n",
      "7 recent year-[see]-surge in implementation\n",
      "7 we-[estimate]-similarity\n",
      "7 we-[apply]-LDA\n",
      "6 complexity-[suggest]-merit\n",
      "6 ws-prsm-[have]-performance gain\n",
      "6 they-[enjoy]-life\n",
      "5 they-[resume]-work\n",
      "4 we-[partition]-research\n",
      "4 they-[be]-specific\n",
      "4 they-[appreciate]-friend and family\n",
      "3 we-[investigate]-utility of MCLDA\n",
      "3 we-[partition]-graph\n",
      "3 ws-prsm-[demonstrate]-that\n",
      "3 it-[be]-important\n",
      "2 we-[leverage]-Big Data\n",
      "2 we-[leverage]-Big Data\n",
      "1 ConclusionsDementia-[be]-underdiagnosed and\n",
      "1 MCLDA-[be]-able\n"
     ]
    }
   ],
   "source": [
    "extended = relation.extend_lda_results(parse, _input, top_terms, top_docs, terms, \"lemma\", top_n=-1, score_method=\"tf\")\n",
    "print(len(extended[t_idx]))\n",
    "for triple in extended[t_idx]:\n",
    "    print(round(triple[1],2),\"{}-[{}]-{}\".format(\" \".join(triple[0][0]),\" \".join(triple[0][1]),\" \".join(triple[0][2])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = 20\n",
    "measure = \"c_uci\"\n",
    "old = lda.get_coherence(tf, terms, topic_word, _input, measure, window_size=ws)\n",
    "fake_idxs = []\n",
    "min_top = 999\n",
    "for e in extended: # topic\n",
    "    new_word_idxs = []\n",
    "    for t in e:# triples\n",
    "         new_word_idxs.extend(evaluate.filter_triple2term_idx(t[0], vec))\n",
    "    if min_top > len(set(new_word_idxs)):\n",
    "        min_top = len(set(new_word_idxs))\n",
    "    fake_idxs.append(set(new_word_idxs))\n",
    "fake_topic_word = evaluate.gen_fake_topic_word(topic_word.shape, fake_idxs)\n",
    "new = lda.get_coherence(tf, terms, fake_topic_word, _input, measure,min_top,window_size=ws)\n",
    "print(old, new)\n",
    "print(np.mean(old), np.mean(new))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([terms[i] for i in fake_idxs[1]])\n",
    "print([terms[i[0]] for i in top_terms[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = relation.get_phrases_by_pattern(parse)\n",
    "sorted(dict(Counter([\" \".join(x) for x in ps])).items(), key=lambda x:x[1], reverse=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lda362",
   "language": "python",
   "name": "lda362"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
